"""
ì‘ì—…ëŸ‰ ìºì‹œ ê´€ë¦¬ ëª¨ë“ˆ
ì—…ì²´ë³„ 3ì£¼ì¹˜ ì‘ì—…ëŸ‰ ë°ì´í„°ë¥¼ ìºì‹œë¡œ ê´€ë¦¬í•˜ì—¬ ì„±ëŠ¥ ê°œì„ 
"""
import os
import json
import pytz
from datetime import datetime, timedelta, date
from typing import Dict, List, Optional, Any
import logging

logger = logging.getLogger(__name__)

CACHE_FILE = os.getenv("WORKLOAD_CACHE_FILE", "workload_cache.json")


class WorkloadCache:
    """ì‘ì—…ëŸ‰ ìºì‹œ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, cache_file: str = None):
        """ì´ˆê¸°í™”
        Args:
            cache_file: ìºì‹œ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸: workload_cache.json)
        """
        self.cache_file = cache_file or CACHE_FILE
        self.cache_data = self._load_cache()
    
    def _load_cache(self) -> Dict[str, Any]:
        """ìºì‹œ íŒŒì¼ ë¡œë“œ"""
        try:
            if os.path.exists(self.cache_file):
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    logger.info(f"Loaded workload cache: {len(data.get('companies', {}))} companies")
                    return data
        except Exception as e:
            logger.error(f"Failed to load cache: {e}")
        
        return {
            "updated_at": None,
            "cache_expires_at": None,
            "companies": {}
        }
    
    def _save_cache(self) -> bool:
        """ìºì‹œ íŒŒì¼ ì €ì¥"""
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache_data, f, ensure_ascii=False, indent=2)
            logger.info(f"Saved workload cache: {len(self.cache_data.get('companies', {}))} companies")
            return True
        except Exception as e:
            logger.error(f"Failed to save cache: {e}")
            return False
    
    def is_cache_valid(self) -> bool:
        """ìºì‹œê°€ ìœ íš¨í•œì§€ í™•ì¸"""
        if not self.cache_data.get("cache_expires_at"):
            return False
        
        try:
            kst = pytz.timezone('Asia/Seoul')
            expires_at_str = self.cache_data["cache_expires_at"]
            expires_at = datetime.fromisoformat(expires_at_str)
            
            # timezone-awareë¡œ ë³€í™˜
            if expires_at.tzinfo is None:
                expires_at = kst.localize(expires_at)
            
            now_kst = datetime.now(kst)
            
            is_valid = now_kst < expires_at
            logger.info(f"Cache validation: now={now_kst.strftime('%Y-%m-%d %H:%M')}, expires={expires_at.strftime('%Y-%m-%d %H:%M')}, valid={is_valid}")
            return is_valid
        except Exception as e:
            logger.error(f"Cache validation error: {e}")
            return False
    
    def get_company_workload(self, company: str) -> Optional[Dict[str, Any]]:
        """íŠ¹ì • íšŒì‚¬ì˜ ì‘ì—…ëŸ‰ ìŠ¤ì¼€ì¤„ ì¡°íšŒ
        
        Args:
            company: íšŒì‚¬ëª… (ì œì´íˆ¬ë©, ì¼ë¥˜ê¸°íš)
            
        Returns:
            ì‘ì—…ëŸ‰ ìŠ¤ì¼€ì¤„ ë°ì´í„° ë˜ëŠ” None (ìºì‹œ ì—†ìŒ/ë§Œë£Œ)
        """
        if not self.is_cache_valid():
            logger.info("Cache expired or invalid")
            return None
        
        companies = self.cache_data.get("companies", {})
        return companies.get(company)
    
    def get_business_workload(self, company: str, business_name: str) -> Optional[Dict[str, Any]]:
        """íŠ¹ì • ì—…ì²´ì˜ ì‘ì—…ëŸ‰ ìŠ¤ì¼€ì¤„ ì¡°íšŒ
        
        Args:
            company: íšŒì‚¬ëª… (ì œì´íˆ¬ë©, ì¼ë¥˜ê¸°íš)
            business_name: ìƒí˜¸ëª…
            
        Returns:
            ì‘ì—…ëŸ‰ ìŠ¤ì¼€ì¤„ ë°ì´í„° ë˜ëŠ” None (ìºì‹œ ì—†ìŒ/ë§Œë£Œ)
        """
        if not self.is_cache_valid():
            logger.info("Cache expired or invalid")
            return None
        
        companies = self.cache_data.get("companies", {})
        businesses = companies.get("businesses", {})
        business_key = f"{company}:{business_name}"
        return businesses.get(business_key)
    
    def get_all_businesses_workload(self, company: str) -> Dict[str, Dict[str, Any]]:
        """íŠ¹ì • íšŒì‚¬ì˜ ëª¨ë“  ì—…ì²´ë³„ ì‘ì—…ëŸ‰ ì¡°íšŒ
        
        Args:
            company: íšŒì‚¬ëª… (ì œì´íˆ¬ë©, ì¼ë¥˜ê¸°íš)
            
        Returns:
            ì—…ì²´ë³„ ì‘ì—…ëŸ‰ ë°ì´í„° {"ì—…ì²´ëª…": {"weeks": [...]}, ...}
        """
        if not self.is_cache_valid():
            logger.info("Cache expired or invalid")
            return {}
        
        companies = self.cache_data.get("companies", {})
        businesses = companies.get("businesses", {})
        
        # í•´ë‹¹ íšŒì‚¬ì˜ ì—…ì²´ë§Œ í•„í„°ë§
        result = {}
        prefix = f"{company}:"
        for key, data in businesses.items():
            if key.startswith(prefix):
                business_name = key[len(prefix):]
                result[business_name] = data
        
        return result
    
    def update_cache(self, workload_data: Dict[str, Any]) -> bool:
        """ìºì‹œ ì—…ë°ì´íŠ¸
        
        Args:
            workload_data: {
                "ì œì´íˆ¬ë©": {"weeks": [...]},
                "ì¼ë¥˜ê¸°íš": {"weeks": [...]}
            }
            
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            # í•œêµ­ ì‹œê°„ ê¸°ì¤€
            kst = pytz.timezone('Asia/Seoul')
            now = datetime.now(kst)
            
            # ë§Œë£Œ ì‹œê°„ ì„¤ì •: ë‹¤ìŒ 11:30
            if now.hour < 11 or (now.hour == 11 and now.minute < 30):
                # ì˜¤ëŠ˜ 11:30 ì´ì „ì´ë©´ ì˜¤ëŠ˜ 11:30
                expires_at = now.replace(hour=11, minute=30, second=0, microsecond=0)
            else:
                # ì˜¤ëŠ˜ 11:30 ì´í›„ë©´ ë‚´ì¼ 11:30
                tomorrow = now.date() + timedelta(days=1)
                expires_at = datetime(tomorrow.year, tomorrow.month, tomorrow.day, 11, 30, 0, tzinfo=kst)
            
            logger.info(f"Cache expiry set: now={now.strftime('%Y-%m-%d %H:%M')}, expires={expires_at.strftime('%Y-%m-%d %H:%M')}")
            
            self.cache_data = {
                "updated_at": now.isoformat(),
                "cache_expires_at": expires_at.isoformat(),
                "companies": workload_data
            }
            
            return self._save_cache()
        except Exception as e:
            logger.error(f"Cache update error: {e}")
            return False
    
    def clear_cache(self) -> bool:
        """ìºì‹œ ì´ˆê¸°í™”"""
        try:
            self.cache_data = {
                "updated_at": None,
                "cache_expires_at": None,
                "companies": {}
            }
            return self._save_cache()
        except Exception as e:
            logger.error(f"Cache clear error: {e}")
            return False
    
    def get_cache_status(self) -> Dict[str, Any]:
        """ìºì‹œ ìƒíƒœ ì •ë³´ ì¡°íšŒ"""
        is_valid = self.is_cache_valid()
        companies_data = self.cache_data.get("companies", {})
        
        # íšŒì‚¬ ëª©ë¡ (businesses í‚¤ ì œì™¸)
        companies = [k for k in companies_data.keys() if k != "businesses"]
        
        # ì—…ì²´ë³„ ìºì‹œ ì •ë³´
        businesses_dict = companies_data.get("businesses", {})
        business_count = len(businesses_dict)
        
        status = {
            "is_valid": is_valid,
            "updated_at": self.cache_data.get("updated_at"),
            "expires_at": self.cache_data.get("cache_expires_at"),
            "companies": companies,
            "company_count": len(companies),
            "business_count": business_count
        }
        
        # ê° íšŒì‚¬ë³„ ì£¼ì°¨ ìˆ˜ ì •ë³´
        for company in companies:
            company_data = companies_data[company]
            if isinstance(company_data, dict):
                weeks = company_data.get("weeks", [])
                status[f"{company}_weeks"] = len(weeks)
        
        return status




def refresh_all_workload_cache() -> Dict[str, Any]:
    """ëª¨ë“  íšŒì‚¬ì˜ ì‘ì—…ëŸ‰ ìºì‹œë¥¼ ê°±ì‹  (íšŒì‚¬ ì „ì²´ + ì—…ì²´ë³„)
    
    Returns:
        ê°±ì‹  ê²°ê³¼ {
            "success": bool,
            "updated_companies": [],
            "failed_companies": [],
            "message": str
        }
    """
    from internal_manager import fetch_workload_schedule_direct
    from guarantee_manager import GuaranteeManager
    
    logger.info("Starting workload cache refresh for all companies and businesses...")
    
    companies = ["ì œì´íˆ¬ë©", "ì¼ë¥˜ê¸°íš"]
    workload_data = {}
    updated_companies = []
    failed_companies = []
    business_workloads = {}  # ì—…ì²´ë³„ ì‘ì—…ëŸ‰ ì €ì¥
    
    for company in companies:
        try:
            logger.info(f"ğŸš€ Fetching raw workload data for {company}...")
            
            # RAW ë°ì´í„°ë¥¼ í•œ ë²ˆë§Œ ê°€ì ¸ì˜¤ê¸° (ê°€ì¥ íš¨ìœ¨ì )
            from internal_manager import fetch_internal_items_for_company, process_raw_items_to_schedule
            
            raw_items = fetch_internal_items_for_company(company)
            logger.info(f"  âœ… Raw data fetched: {len(raw_items)} items (ë‹¨ 1íšŒ API í˜¸ì¶œ!)")
            
            # íšŒì‚¬ ì „ì²´ ì‘ì—…ëŸ‰ ê³„ì‚°
            schedule = process_raw_items_to_schedule(raw_items, company, None)
            workload_data[company] = schedule
            logger.info(f"  ğŸ“Š {company} ì „ì²´: {len(schedule.get('weeks', []))} weeks")
            
            # í•´ë‹¹ íšŒì‚¬ì˜ ì§„í–‰ì¤‘ì¸ ì—…ì²´ ëª©ë¡
            from datetime import date, timedelta
            gm = GuaranteeManager()
            guarantee_items = gm.get_items({"company": company})
            
            # ì§„í–‰ì¤‘/í›„ë¶ˆ/ì„¸íŒ…ëŒ€ê¸° ìƒíƒœì¸ ì—…ì²´ ì„ íƒ
            active_statuses = ["ì§„í–‰ì¤‘", "í›„ë¶ˆ", "ì„¸íŒ…ëŒ€ê¸°"]
            
            filtered_guarantee_items = []
            for item in guarantee_items:
                status = item.get("status")
                business_name = item.get("business_name")
                
                if not business_name:
                    continue
                
                # ì§„í–‰ì¤‘/í›„ë¶ˆ/ì„¸íŒ…ëŒ€ê¸° ì—…ì²´ë§Œ í¬í•¨
                if status not in active_statuses:
                    continue
                
                filtered_guarantee_items.append(item)
            
            business_names = [item.get("business_name") for item in filtered_guarantee_items]
            
            logger.info(f"  ğŸ“‹ Processing {len(business_names)} active businesses (ë©”ëª¨ë¦¬ í•„í„°ë§, ëª¨ë“  í–‰ í¬í•¨)...")
            
            # ë©”ëª¨ë¦¬ì—ì„œ ì—…ì²´ë³„ë¡œ ë¶„í•  (ì´ˆê³ ì†, API í˜¸ì¶œ ì—†ìŒ!)
            cached_count = 0
            skipped_count = 0
            failed_count = 0
            
            for idx, business_name in enumerate(business_names, 1):
                try:
                    # Raw ë°ì´í„°ì—ì„œ í•´ë‹¹ ì—…ì²´ë§Œ í•„í„°ë§ (ì •ê·œí™”ëœ ë§¤ì¹­ - ëŒ€ì†Œë¬¸ì, ê³µë°± ë¬´ì‹œ)
                    business_name_normalized = business_name.strip().lower().replace(" ", "")
                    business_raw_items = [
                        item for item in raw_items
                        if str(item.get("bizname", "")).strip().lower().replace(" ", "") == business_name_normalized
                    ]

                    if not business_raw_items:
                        logger.info(f"  [{idx}/{len(business_names)}] âŠ˜ {business_name}: no data")
                        skipped_count += 1
                        continue
                    
                    # ë””ë²„ê¹…: ëª¨ë“  ì—…ì²´ì˜ ì‘ì—… ìˆ˜ ë¡œê·¸
                    logger.info(f"  [{idx}/{len(business_names)}] {business_name}: {len(business_raw_items)}ê°œ ì‘ì—… ë°œê²¬")
                    
                    # ìƒì„¸ ë””ë²„ê¹…: ì²˜ìŒ ëª‡ ê°œ ì—…ì²´ë§Œ
                    if idx <= 5:
                        logger.info(f"     â†³ raw ë°ì´í„° ìƒ˜í”Œ (ìµœëŒ€ 15ê°œ):")
                        for sample_idx, sample_item in enumerate(business_raw_items[:15]):
                            start_str = sample_item['start_date'].strftime('%Y-%m-%d') if sample_item['start_date'] else "ì‹œì‘ì¼ ì—†ìŒ"
                            end_str = sample_item['end_date'].strftime('%Y-%m-%d') if sample_item['end_date'] else "ë§ˆê°ì¼ ì—†ìŒ"
                            logger.info(f"        {sample_idx+1}. {sample_item['task_display']}: ì‹œì‘={start_str}, ë§ˆê°={end_str}, ì‘ì—…ëŸ‰={sample_item['workload']}")
                    
                    # ì—…ì²´ë³„ ìŠ¤ì¼€ì¤„ ê³„ì‚° (ëª¨ë“  ì‘ì—… í¬í•¨)
                    business_schedule = process_raw_items_to_schedule(business_raw_items, company, business_name)
                    
                    # ë³´ì¥ê±´ ì •ë³´ ì¶”ê°€ (ìˆœìœ„, ëŒ€í–‰ì‚¬ëª…)
                    guarantee_info = next((item for item in filtered_guarantee_items if item.get("business_name") == business_name), None)
                    if guarantee_info:
                        business_schedule["guarantee_rank"] = guarantee_info.get("guarantee_rank")
                        business_schedule["agency"] = guarantee_info.get("agency")
                        business_schedule["business_name"] = business_name
                    
                    # ì‘ì—…ì´ ìˆëŠ” ì—…ì²´ë§Œ ìºì‹±
                    if business_schedule.get("weeks"):
                        business_key = f"{company}:{business_name}"
                        business_workloads[business_key] = business_schedule
                        logger.info(f"  [{idx}/{len(business_names)}] âœ… {business_name}: {len(business_schedule.get('weeks', []))} weeks (ì´ {len(business_raw_items)}ê°œ ì‘ì—…)")
                        cached_count += 1
                    else:
                        logger.warning(f"  [{idx}/{len(business_names)}] âš ï¸ {business_name}: {len(business_raw_items)}ê°œ ì‘ì—…ì´ ìˆì§€ë§Œ weeks ë°ì´í„° ì—†ìŒ")
                        skipped_count += 1
                except Exception as e:
                    logger.error(f"  [{idx}/{len(business_names)}] âŒ {business_name}: {e}")
                    import traceback
                    logger.error(traceback.format_exc())
                    failed_count += 1
            
            updated_companies.append(company)
            logger.info(f"âœ… {company} complete - Cached: {cached_count}, Skipped: {skipped_count}, Failed: {failed_count}")
        except Exception as e:
            logger.error(f"âŒ Failed to fetch workload for {company}: {e}")
            failed_companies.append(company)
            # ë¹ˆ ë°ì´í„°ë¼ë„ ì¶”ê°€
            workload_data[company] = {"weeks": []}
    
    # ìºì‹œ ì €ì¥ (íšŒì‚¬ ì „ì²´ + ì—…ì²´ë³„)
    cache = WorkloadCache()
    cache_data = {
        **workload_data,
        "businesses": business_workloads  # ì—…ì²´ë³„ ë°ì´í„° ì¶”ê°€
    }
    success = cache.update_cache(cache_data)
    
    if success:
        message = f"âœ… ìºì‹œ ê°±ì‹  ì™„ë£Œ - {', '.join(updated_companies)} ({len(business_workloads)}ê°œ ì—…ì²´ ìºì‹±ë¨)"
        if failed_companies:
            message += f", ì‹¤íŒ¨: {', '.join(failed_companies)}"
    else:
        message = "âŒ ìºì‹œ ì €ì¥ ì‹¤íŒ¨"
    
    logger.info(message)
    logger.info(f"ğŸ“Š Total cached: {len(business_workloads)} businesses")
    
    return {
        "success": success,
        "updated_companies": updated_companies,
        "failed_companies": failed_companies,
        "business_count": len(business_workloads),
        "message": message,
        "cache_status": cache.get_cache_status()
    }

