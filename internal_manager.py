import os
import json
import pytz
from datetime import datetime, timezone, date, timedelta
from typing import Any, Dict, List, Tuple

from sheet_client import (
	load_settings,
	_get_client,
	_find_header_row,
	_build_records,
	_get_value_flexible,
	_normalize_key,
	_collapse_spaces,
	_parse_int_maybe,
	_is_truthy,
)


CACHE_FILE = os.getenv("INTERNAL_CACHE_FILE", "internal_cache.json")


def _is_internal_or_postpaid(value: Any) -> bool:
	"""ë‚´ë¶€ ì§„í–‰ê±´ ë˜ëŠ” í›„ë¶ˆ ê±´ì¸ì§€ í™•ì¸
	
	Args:
		value: 'ì‘ì—… ì—¬ë¶€' ì»¬ëŸ¼ ê°’
		
	Returns:
		True if ê°’ì´ 'ì§„í–‰ì¤‘', 'í›„ë¶ˆ' ë˜ëŠ” truthy ê°’
	"""
	if value is None:
		return False
	
	s = str(value).strip().lower()
	
	# ëª…ì‹œì  ìƒíƒœê°’ ì²´í¬
	if s in ["ì§„í–‰ì¤‘", "í›„ë¶ˆ", "ì§„í–‰ ì¤‘", "í›„ë¶ˆê±´"]:
		return True
	
	# ê¸°ì¡´ truthy ê°’ë„ í—ˆìš© (í•˜ìœ„ í˜¸í™˜ì„±)
	from sheet_client import _is_truthy
	return _is_truthy(value)


def parse_date_flexible(date_str: str):
	"""ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ì„ íŒŒì‹± (ë§¤ìš° ê´€ëŒ€í•˜ê²Œ)"""
	from datetime import date, datetime, timedelta
	import re
	
	if not date_str:
		return None
	
	# datetime ê°ì²´ê°€ ì´ë¯¸ ë“¤ì–´ì˜¨ ê²½ìš°
	if isinstance(date_str, datetime):
		return date_str.date()
	
	# date ê°ì²´ê°€ ì´ë¯¸ ë“¤ì–´ì˜¨ ê²½ìš°
	if isinstance(date_str, date):
		return date_str
	
	date_str = str(date_str).strip()
	
	# ë¹ˆ ë¬¸ìì—´ ì²´í¬
	if not date_str or date_str.lower() in ['none', 'null', 'n/a', '-']:
		return None
	
	try:
		# 1. í•œêµ­ì–´ ë‚ ì§œ í˜•ì‹ ë¨¼ì € ì²˜ë¦¬ (ì˜ˆ: "10ì›” 31ì¼", "08ì›” 04ì¼")
		match = re.match(r"^(\d{1,2})ì›”\s*(\d{1,2})ì¼$", date_str)
		if match:
			month, day = match.groups()
			year = date.today().year
			try:
				return date(year, int(month), int(day))
			except ValueError:
				pass
		
		# 2. Google Sheets ì‹œë¦¬ì–¼ ë„˜ë²„ ì²˜ë¦¬ (1900-01-01 ê¸°ì¤€)
		# ì˜ˆ: 45582 = 2024-10-10
		if re.match(r"^\d{5,6}$", date_str):
			try:
				serial = int(date_str)
				base_date = datetime(1899, 12, 30)
				result_date = base_date + timedelta(days=serial)
				return result_date.date()
			except:
				pass
		
		# 3. YYYY-MM-DD, YYYY.MM.DD, YYYY/MM/DD í˜•ì‹
		match = re.match(r"^(\d{4})[./-](\d{1,2})[./-](\d{1,2})$", date_str)
		if match:
			year, month, day = match.groups()
			try:
				return date(int(year), int(month), int(day))
			except ValueError:
				pass
		
		# 4. YYYY-MM-DD HH:MM:SS í˜•ì‹ (datetime ë¬¸ìì—´)
		match = re.match(r"^(\d{4})[./-](\d{1,2})[./-](\d{1,2})\s+\d{1,2}:\d{1,2}(:\d{1,2})?$", date_str)
		if match:
			year, month, day = match.groups()[:3]
			try:
				return date(int(year), int(month), int(day))
			except ValueError:
				pass
		
		# 5. YYYYMMDD í˜•ì‹ (ì˜ˆ: 20251027)
		match = re.match(r"^(\d{4})(\d{2})(\d{2})$", date_str)
		if match:
			year, month, day = match.groups()
			try:
				return date(int(year), int(month), int(day))
			except ValueError:
				pass
		
		# 6. YY. M. D í˜•ì‹ (ì˜ˆ: 25. 10. 27, 25.10.27, 25-10-27)
		# êµ¬ë¶„ìê°€ 2ê°œ ìˆì–´ì•¼ í•¨ (3ê°œ ë¶€ë¶„)
		match = re.match(r"^(\d{2})[./ -](\d{1,2})[./ -](\d{1,2})$", date_str)
		if match:
			year_short, month, day = match.groups()
			year = 2000 + int(year_short)
			try:
				return date(year, int(month), int(day))
			except ValueError:
				pass
		
		# 7. M/D ë˜ëŠ” MM/DD í˜•ì‹ (ì˜ˆ: 8/1, 10/27) - í˜„ì¬ ì—°ë„ ê¸°ì¤€
		match = re.match(r"^(\d{1,2})/(\d{1,2})$", date_str)
		if match:
			month, day = match.groups()
			month_int, day_int = int(month), int(day)
			if 1 <= month_int <= 12 and 1 <= day_int <= 31:
				year = date.today().year
				try:
					return date(year, month_int, day_int)
				except ValueError:
					pass
		
		# 8. M-D ë˜ëŠ” MM-DD í˜•ì‹ (ì˜ˆ: 8-1, 10-24) - í˜„ì¬ ì—°ë„ ê¸°ì¤€
		match = re.match(r"^(\d{1,2})-(\d{1,2})$", date_str)
		if match:
			month, day = match.groups()
			month_int, day_int = int(month), int(day)
			if 1 <= month_int <= 12 and 1 <= day_int <= 31:
				year = date.today().year
				try:
					return date(year, month_int, day_int)
				except ValueError:
					pass
		
		# 9. M.D ë˜ëŠ” MM.DD í˜•ì‹ (ì˜ˆ: 8.1, 10.27) - í˜„ì¬ ì—°ë„ ê¸°ì¤€
		match = re.match(r"^(\d{1,2})\.(\d{1,2})$", date_str)
		if match:
			month, day = match.groups()
			month_int, day_int = int(month), int(day)
			if 1 <= month_int <= 12 and 1 <= day_int <= 31:
				year = date.today().year
				try:
					return date(year, month_int, day_int)
				except ValueError:
					pass
		
		# 10. dateutil ì‚¬ìš© (ë§¤ìš° ë³µì¡í•œ í˜•ì‹ë§Œ - ìœ„ì—ì„œ ì‹¤íŒ¨í•œ ê²½ìš°)
		try:
			from dateutil import parser as dateutil_parser
			# dayfirst=Falseë¡œ ë¯¸êµ­ì‹ í•´ì„ (MM-DD-YYYY)
			parsed = dateutil_parser.parse(date_str, dayfirst=False)
			return parsed.date()
		except ImportError:
			pass  # dateutil ì—†ìœ¼ë©´ ìŠ¤í‚µ
		except Exception:
			pass  # dateutil íŒŒì‹± ì‹¤íŒ¨ì‹œ ìŠ¤í‚µ
			
	except Exception as e:
		import logging
		logging.getLogger(__name__).warning(f"ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: '{date_str}' - {e}")
	
	return None


def _build_task_display(tab_title: str, product: str, product_name: str) -> str:
	base_task = (tab_title or "").strip()
	is_misc = _collapse_spaces(base_task) == _collapse_spaces("ê¸°íƒ€")
	if is_misc:
		return (product_name or base_task).strip() or base_task
	else:
		return (f"{base_task} {product}".strip() if product else base_task) or base_task


def fetch_internal_items_for_company(company: str) -> List[Dict[str, Any]]:
	"""íŠ¹ì • íšŒì‚¬ì˜ raw ë‚´ë¶€ ì§„í–‰ê±´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ë‹¨ 1íšŒ API í˜¸ì¶œ)
	
	Args:
		company: íšŒì‚¬ëª… (ì œì´íˆ¬ë©, ì¼ë¥˜ê¸°íš)
	
	Returns:
		raw items ë¦¬ìŠ¤íŠ¸: [{agency, bizname, task_display, workload, start_date, end_date, has_real_start_date}]
	"""
	import logging
	logger = logging.getLogger(__name__)
	from datetime import date, timedelta
	
	settings = load_settings()
	if not settings.spreadsheet_id:
		raise RuntimeError("SPREADSHEET_ID í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
	
	# ë³´ì¥ê±´ ë°ì´í„°ì—ì„œ í•´ë‹¹ íšŒì‚¬ì˜ ìƒí˜¸ëª… ë¦¬ìŠ¤íŠ¸ + ì‘ì—… ì‹œì‘ì¼ ë§¤í•‘
	guarantee_data_map = {}
	company_business_names = None
	try:
		from guarantee_manager import GuaranteeManager
		gm = GuaranteeManager()
		items = gm.get_items({"company": company})
		company_business_names = {item.get("business_name") for item in items if item.get("business_name")}
		for item in items:
			biz = item.get("business_name")
			if biz:
				guarantee_data_map[biz] = item
		logger.info(f"ğŸ“‹ {company} ë³´ì¥ê±´: {len(company_business_names)}ê°œ ì—…ì²´")
	except Exception as e:
		logger.warning(f"ë³´ì¥ê±´ ë¡œë“œ ì‹¤íŒ¨: {e}")
		company_business_names = None
	
	client = _get_client()
	ss = client.open_by_key(settings.spreadsheet_id)
	
	# í•œêµ­ ì‹œê°„ ê¸°ì¤€ (KST)
	kst = pytz.timezone('Asia/Seoul')
	today = datetime.now(kst).date()
	logger.info(f"ğŸ“… ì˜¤ëŠ˜ ë‚ ì§œ (KST): {today}")
	
	all_items = []
	ws_list = ss.worksheets()
	tab_titles = [ws.title for ws in ws_list]
	logger.info(f"ğŸ“Š {company} raw ë°ì´í„° ì¡°íšŒ - ì›Œí¬ì‹œíŠ¸: {len(ws_list)}ê°œ")
	logger.info(f"   íƒ­ ëª©ë¡: {', '.join(tab_titles)}")
	
	# ì›Œí¬ì‹œíŠ¸ë³„ ì²˜ë¦¬ í†µê³„
	tab_stats = {}
	
	for idx, ws in enumerate(ws_list, 1):
		tab_title = (ws.title or "").strip()
		logger.info(f"   [{idx}/{len(ws_list)}] ì²˜ë¦¬ ì¤‘: {tab_title}")
		
		try:
			header_row, headers = _find_header_row(ws, settings)
			records = _build_records(ws, header_row, headers)
			logger.info(f"      âœ“ {len(records)}ê°œ í–‰ ì½ìŒ")
			
			tab_stats[tab_title] = {"total_rows": len(records), "internal_count": 0, "company_match": 0}
		except Exception as e:
			logger.error(f"      âŒ íƒ­ ì½ê¸° ì‹¤íŒ¨: {e}")
			tab_stats[tab_title] = {"error": str(e)}
			continue
		
		for row in records:
			row_norm = { _normalize_key(k): v for k, v in row.items() }
			
			# ë‚´ë¶€ ì§„í–‰ê±´ ë˜ëŠ” í›„ë¶ˆ ê±´ í•„í„°
			is_internal = _is_internal_or_postpaid(_get_value_flexible(row_norm, settings.internal_col, "INTERNAL_COLUMN"))
			if not is_internal:
				continue
			
			tab_stats[tab_title]["internal_count"] += 1
			
			# ê¸°ë³¸ ì •ë³´
			bizname = str(_get_value_flexible(row_norm, settings.bizname_col, "BIZNAME_COLUMN") or "").strip()
			if not bizname:
				continue
			
			agency_raw = str(_get_value_flexible(row_norm, settings.agency_col, "AGENCY_COLUMN") or "").strip()
			remain = _parse_int_maybe(_get_value_flexible(row_norm, settings.remaining_days_col, "REMAINING_DAYS_COLUMN"))
			workload = str(_get_value_flexible(row_norm, settings.daily_workload_col, "DAILY_WORKLOAD_COLUMN") or "").strip()
			product = str(_get_value_flexible(row_norm, settings.product_col, "PRODUCT_COLUMN") or "").strip()
			product_name = str(_get_value_flexible(row_norm, settings.product_name_col, "PRODUCT_NAME_COLUMN") or "").strip()
			
			# íšŒì‚¬ í•„í„° (ìƒí˜¸ëª… ê¸°ì¤€)
			if company_business_names is not None:
				if bizname not in company_business_names:
					continue
			
			tab_stats[tab_title]["company_match"] += 1
			
			# ë§ˆê°ì¼ ê³„ì‚°: ì˜¤ëŠ˜ + ë‚¨ì€ì¼ìˆ˜ (remainì´ Noneì´ì–´ë„ ì¼ë‹¨ ìˆ˜ì§‘)
			end_date = today + timedelta(days=remain) if remain is not None else None
			
			# ì‘ì—… ì‹œì‘ì¼: ê° í–‰ì˜ 'ì‘ì—… ì‹œì‘ì¼' ì»¬ëŸ¼ ì½ê¸° (ë‹¤ì–‘í•œ ì»¬ëŸ¼ëª… ì‹œë„)
			start_date_str = None
			start_col_found = None
			for possible_col in ["ì‘ì—… ì‹œì‘ì¼", "ì‘ì—…ì‹œì‘ì¼", "ì‹œì‘ì¼", "ì„¸íŒ…ì¼", "ì‘ì—…ì‹œì‘", "ì‹œì‘"]:
				start_val = _get_value_flexible(row_norm, possible_col, "")
				if start_val:
					start_date_str = str(start_val).strip()
					start_col_found = possible_col
					# ë””ë²„ê¹…: ì›ë³¸ ê°’ í™•ì¸
					if len(all_items) < 10:
						logger.info(f"  ğŸ” {bizname}/{product}: '{possible_col}' ì»¬ëŸ¼ ì›ë³¸ê°’ = '{start_val}' (íƒ€ì…: {type(start_val).__name__})")
					break
			
			start_date = None
			parse_success = False
			if start_date_str:
				start_date = parse_date_flexible(start_date_str)
				if start_date:
					parse_success = True
					# ë””ë²„ê¹…: ì²˜ìŒ 10ê°œë§Œ ìƒì„¸ ë¡œê·¸
					if len(all_items) < 10:
						logger.info(f"  âœ“ íŒŒì‹± ì„±ê³µ: '{start_date_str}' â†’ {start_date}")
						if end_date:
							logger.info(f"     ë§ˆê°ì¼: {end_date.strftime('%Y-%m-%d')} (ì˜¤ëŠ˜={today}, remain={remain}ì¼)")
				else:
					# íŒŒì‹± ì‹¤íŒ¨ - ê²½ê³  ë¡œê·¸
					logger.warning(f"  âš ï¸ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {bizname}/{product} - '{start_date_str}' (ì»¬ëŸ¼: {start_col_found})")
			
			# ì‹œì‘ì¼ì´ ì—†ê±°ë‚˜ íŒŒì‹± ì‹¤íŒ¨í•œ ê²½ìš° ë³´ì¥ê±´ ë°ì´í„°ì—ì„œ ê°€ì ¸ì˜¤ê¸° ì‹œë„
			if not start_date and bizname in guarantee_data_map:
				guarantee_item = guarantee_data_map[bizname]
				work_start = guarantee_item.get("work_start_date")
				if work_start:
					fallback_date = parse_date_flexible(work_start)
					if fallback_date:
						start_date = fallback_date
						parse_success = True
						if len(all_items) < 10:
							logger.info(f"  âœ“ ë³´ì¥ê±´ ë°ì´í„°ì—ì„œ ì‹œì‘ì¼ ê°€ì ¸ì˜´: {bizname} â†’ {work_start} â†’ {start_date}")
			
			# ê·¸ë˜ë„ ì‹œì‘ì¼ì´ ì—†ìœ¼ë©´ Noneìœ¼ë¡œ ìœ ì§€ (í•„í„°ë§í•˜ì§€ ì•Šê³  í¬í•¨)
			if not start_date:
				if len(all_items) < 10 and end_date:
					logger.info(f"  â—‹ {bizname}/{product}: ì‹œì‘ì¼(ì—†ìŒ), ë§ˆê°ì¼={end_date.strftime('%Y-%m-%d')} (remain={remain}ì¼) - ê·¸ëŒ€ë¡œ í¬í•¨")
			
			# ì‘ì—…ëª… ìƒì„±
			is_review_tab = _collapse_spaces(tab_title) == _collapse_spaces("ì˜ìˆ˜ì¦ë¦¬ë·°")
			if is_review_tab:
				item_col_value = None
				for possible_col in ["í•­ëª©", "í•­ëª©ëª…"]:
					val = _get_value_flexible(row_norm, possible_col, "")
					if val:
						item_col_value = str(val).strip()
						break
				task_display = item_col_value if item_col_value else _build_task_display(tab_title, product, product_name)
			else:
				task_display = _build_task_display(tab_title, product, product_name)
			
			all_items.append({
				"agency": agency_raw or "ë‚´ë¶€ ì§„í–‰",
				"bizname": bizname,
				"task_display": task_display,
				"workload": workload,
				"start_date": start_date,
				"end_date": end_date,
				"has_real_start_date": bool(start_date_str),  # ì‹œíŠ¸ì— ì‹œì‘ì¼ ì»¬ëŸ¼ì´ ìˆì—ˆëŠ”ì§€
				"tab_title": tab_title,  # ì—…ì¢… ë¶„ë¥˜ìš©
				"product": product,  # ì—…ì¢… ë¶„ë¥˜ìš©
				"product_name": product_name  # ì—…ì¢… ë¶„ë¥˜ìš©
			})
	
	logger.info(f"âœ… {company} raw ë°ì´í„°: {len(all_items)}ê°œ")
	
	# ì›Œí¬ì‹œíŠ¸ë³„ í†µê³„ ì¶œë ¥
	logger.info(f"ğŸ“‹ ì›Œí¬ì‹œíŠ¸ë³„ ìƒì„¸ í†µê³„:")
	for tab_title, stats in tab_stats.items():
		if "error" in stats:
			logger.error(f"   âŒ {tab_title}: {stats['error']}")
		else:
			logger.info(f"   âœ“ {tab_title}: ì „ì²´ {stats['total_rows']}í–‰ â†’ ë‚´ë¶€ì§„í–‰ {stats['internal_count']}ê±´ â†’ {company} ë§¤ì¹­ {stats['company_match']}ê±´")
	
	# ì¤‘ë³µ ì œê±° ì—†ìŒ! ê° í–‰ì„ ê·¸ëŒ€ë¡œ ìœ ì§€
	# ê°™ì€ ì‘ì—…ì´ë¼ë„ ì‹œì‘ì¼-ë§ˆê°ì¼ì´ ë‹¤ë¥´ë©´ ë³„ë„ë¡œ í‘œì‹œ
	return all_items


def process_raw_items_to_schedule(raw_items: List[Dict[str, Any]], company: str, business_name: str = None) -> Dict[str, Any]:
	"""Raw ë°ì´í„°ë¥¼ ìŠ¤ì¼€ì¤„ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ë©”ëª¨ë¦¬ ì‘ì—…, API í˜¸ì¶œ ì—†ìŒ)
	
	Args:
		raw_items: raw ë°ì´í„° ë¦¬ìŠ¤íŠ¸
		company: íšŒì‚¬ëª…
		business_name: ì—…ì²´ëª… (Noneì´ë©´ ì „ì²´)
	
	Returns:
		{"weeks": [...]}
	"""
	import logging
	logger = logging.getLogger(__name__)
	
	# í•œêµ­ ì‹œê°„ ê¸°ì¤€ (KST)
	kst = pytz.timezone('Asia/Seoul')
	today = datetime.now(kst).date()
	
	if not raw_items:
		logger.debug(f"âŠ˜ {business_name or company}: raw_items ì—†ìŒ")
		return {"weeks": []}
	
	# ëª¨ë“  ë°ì´í„°ë¥¼ ì½ì–´ì˜¨ í›„ 4ì£¼ ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§
	four_weeks_ago = today - timedelta(days=28)
	logger.info(f"  ğŸ“… ì˜¤ëŠ˜: {today}, 4ì£¼ ì „: {four_weeks_ago}")
	
	# ì‹œì‘ì¼ì´ 4ì£¼ ì´ë‚´ì´ê±°ë‚˜, ì‹œì‘ì¼ì´ ì—†ìœ¼ë©´ ë§ˆê°ì¼ì´ ì˜¤ëŠ˜ ì´í›„ì¸ ì‘ì—…ë§Œ í‘œì‹œ
	filtered_items = [
		item for item in raw_items 
		if (item["start_date"] is not None and item["start_date"] >= four_weeks_ago) or
		   (item["start_date"] is None and item["end_date"] is not None and item["end_date"] >= today)
	]
	
	logger.info(f"  ğŸ“Š í•„í„° ê²°ê³¼: {len(raw_items)}ê°œ â†’ {len(filtered_items)}ê°œ (4ì£¼ í•„í„° ì ìš©, ì‹œì‘ì¼ {four_weeks_ago.strftime('%m/%d')} ì´í›„)")
	
	# ê¸°ê°„ë³„ ê·¸ë£¹í•‘ (ê°™ì€ ì‹œì‘ì¼-ë§ˆê°ì¼ì„ ê°€ì§„ ì‘ì—…ë“¤ì„ ë¬¶ìŒ)
	period_groups = {}
	
	# ë””ë²„ê¹…: ê° ê³ ìœ  ê¸°ê°„ í™•ì¸
	if business_name and len(filtered_items) > 0:
		unique_periods = {}
		for item in filtered_items:
			key = (item["start_date"], item["end_date"])
			if key not in unique_periods:
				unique_periods[key] = []
			unique_periods[key].append(f"{item['task_display']}:{item.get('workload', 0)}")
		
		logger.info(f"  ğŸ” {business_name} ê³ ìœ  ê¸°ê°„: {len(unique_periods)}ê°œ")
		# None ì²˜ë¦¬í•˜ì—¬ ì •ë ¬ (Noneì€ ë§¨ ë’¤ë¡œ)
		sorted_periods = sorted(unique_periods.items(), key=lambda x: (x[0][0] is None, x[0][0] if x[0][0] else date.max, x[0][1]))
		for (s, e), tasks in sorted_periods[:10]:
			s_str = s.strftime('%m/%d') if s else "ë¯¸ì •"
			logger.info(f"    [{s_str} ~ {e.strftime('%m/%d')}] {len(tasks)}ê°œ ì‘ì—…")
	
	for item in filtered_items:
		start_dt = item["start_date"]  # None ê°€ëŠ¥
		end_dt = item["end_date"]  # None ê°€ëŠ¥
		
		# end_dtê°€ Noneì´ë©´ ìŠ¤í‚µ
		if end_dt is None:
			continue
		
		# ë‚ ì§œ ê²€ì¦ (ì‹œì‘ì¼ì´ ìˆëŠ” ê²½ìš°ë§Œ)
		if start_dt is not None and start_dt > end_dt:
			logger.error(f"âŒ {item['bizname']}: ì‹œì‘ì¼({start_dt}) > ë§ˆê°ì¼({end_dt}) - ë…¼ë¦¬ ì˜¤ë¥˜!")
			continue  # ì˜ëª»ëœ ë°ì´í„°ëŠ” ì œì™¸
		
		# ê°™ì€ ê¸°ê°„(ì‹œì‘ì¼-ë§ˆê°ì¼)ë¼ë¦¬ ê·¸ë£¹í•‘ (ì‹œì‘ì¼ Noneë„ í—ˆìš©)
		key = (start_dt, end_dt)
		if key not in period_groups:
			period_groups[key] = {}
		
		task_name = item["task_display"]
		
		try:
			wl_num = _parse_int_maybe(item["workload"]) or 0
		except:
			wl_num = 0
		
		# ê°™ì€ ê¸°ê°„ ë‚´ì—ì„œ ê°™ì€ ì‘ì—…ëª…ì€ í•©ì‚°
		if task_name in period_groups[key]:
			period_groups[key][task_name] += wl_num
		else:
			period_groups[key][task_name] = wl_num
	
	# ìŠ¤ì¼€ì¤„ í¬ë§·íŒ… (ì‹œì‘ì¼ ê¸°ì¤€ ì •ë ¬ - ì˜¤ë˜ëœ ê²ƒë¶€í„°, Noneì€ ë§¨ ë’¤)
	weeks = []
	sorted_groups = sorted(period_groups.items(), key=lambda x: (x[0][0] is None, x[0][0] if x[0][0] else date.max, x[0][1]))
	
	for (start_dt, end_dt), tasks in sorted_groups:
		items = []
		# ì‹œíŠ¸ ìˆœì„œ ìœ ì§€ë¥¼ ìœ„í•´ ì •ë ¬í•˜ì§€ ì•Šê³  ì›ë˜ ìˆœì„œëŒ€ë¡œ (ì‚½ì… ìˆœì„œ ìœ ì§€)
		for task_name, total_workload in tasks.items():
			items.append({
				"name": task_name,
				"workload": str(total_workload) if total_workload > 0 else "0"
			})
		
		weeks.append({
			"start_date": start_dt.strftime("%m/%d") if start_dt else "",
			"end_date": end_dt.strftime("%m/%d"),
			"items": items
		})
	
	# ë””ë²„ê¹…: ì²˜ìŒ ëª‡ ê°œë§Œ ë¡œê·¸
	if business_name and len(weeks) > 0:
		logger.info(f"  ğŸ“Š ì´ {len(weeks)}ê°œ ê¸°ê°„:")
		for idx, week in enumerate(weeks[:5]):
			items_str = ", ".join([f"{item['name'][:20]}:{item['workload']}" for item in week['items']])
			logger.info(f"    [{idx+1}] {week['start_date']} ~ {week['end_date']}: {items_str}")
	
	return {"weeks": weeks}


def fetch_internal_items() -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
	"""êµ¬ê¸€ ì‹œíŠ¸ ì „ íƒ­ì„ ìˆœíšŒí•˜ì—¬ ë‚´ë¶€ ì§„í–‰ê±´ë§Œ í‰íƒ„í™”í•˜ì—¬ ë°˜í™˜í•œë‹¤.

	ë°˜í™˜: (items, stats)
	- items: [{tab_title, agency, bizname, task_display, remain_days, daily_workload, checked}]
	- stats: {worksheets: int, items: int}
	"""
	settings = load_settings()
	if not settings.spreadsheet_id:
		raise RuntimeError("SPREADSHEET_ID í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")

	client = _get_client()
	ss = client.open_by_key(settings.spreadsheet_id)

	# ì¤‘ë³µ ìƒí˜¸ ë³‘í•© ë° ì‘ì—…ëŸ‰ í•©ì‚°ì„ ìœ„í•œ ì§‘ê³„: key=(agency, tab_title, task_display, bizname) -> sum(workload)
	aggregator: Dict[Tuple[str, str, str, str], Dict[str, Any]] = {}
	ws_list = ss.worksheets()
	for ws in ws_list:
		tab_title = (ws.title or "").strip()
		header_row, headers = _find_header_row(ws, settings)
		records = _build_records(ws, header_row, headers)
		for row in records:
			row_norm = { _normalize_key(k): v for k, v in row.items() }
			agency_raw = str(_get_value_flexible(row_norm, settings.agency_col, "AGENCY_COLUMN") or "").strip()
			is_checked = _is_truthy(_get_value_flexible(row_norm, settings.checked_col, "CHECKED_COLUMN"))
			is_internal = _is_internal_or_postpaid(_get_value_flexible(row_norm, settings.internal_col, "INTERNAL_COLUMN"))
			remain = _parse_int_maybe(_get_value_flexible(row_norm, settings.remaining_days_col, "REMAINING_DAYS_COLUMN"))
			bizname = str(_get_value_flexible(row_norm, settings.bizname_col, "BIZNAME_COLUMN") or "").strip()
			product = str(_get_value_flexible(row_norm, settings.product_col, "PRODUCT_COLUMN") or "").strip()
			product_name = str(_get_value_flexible(row_norm, settings.product_name_col, "PRODUCT_NAME_COLUMN") or "").strip()
			workload = str(_get_value_flexible(row_norm, settings.daily_workload_col, "DAILY_WORKLOAD_COLUMN") or "").strip()

			if not is_internal:
				continue
			if not bizname:
				continue

			display_task = _build_task_display(tab_title, product, product_name)
			label_agency = agency_raw or "ë‚´ë¶€ ì§„í–‰"
			try:
				wl_num = _parse_int_maybe(workload) or 0
			except Exception:
				wl_num = 0
			key = (label_agency, tab_title, display_task, bizname)
			entry = aggregator.get(key)
			if not entry:
				aggregator[key] = {
					"tab_title": tab_title,
					"agency": label_agency,
					"bizname": bizname,
					"task_display": display_task,
					"remain_days": remain,
					"daily_workload_sum": wl_num,
					"checked": bool(is_checked),
				}
			else:
				entry["daily_workload_sum"] = int(entry.get("daily_workload_sum", 0)) + wl_num

	# ì¶œë ¥ ë¦¬ìŠ¤íŠ¸ êµ¬ì„± (ì •ë ¬ì€ ìƒí˜¸ëª… ê¸°ì¤€)
	items: List[Dict[str, Any]] = []
	for entry in sorted(aggregator.values(), key=lambda e: (e["agency"], e["task_display"], e["bizname"])):
		wl_sum = int(entry.get("daily_workload_sum", 0))
		items.append({
			"tab_title": entry["tab_title"],
			"agency": entry["agency"],
			"bizname": entry["bizname"],
			"task_display": entry["task_display"],
			"remain_days": entry["remain_days"],
			"daily_workload": (str(wl_sum) if wl_sum > 0 else None),
			"checked": entry["checked"],
		})

	return items, {"worksheets": len(ws_list), "items": len(items)}


def load_cache() -> Dict[str, Any]:
	"""ìºì‹œ íŒŒì¼ì„ ì½ì–´ ë°˜í™˜í•œë‹¤. ì—†ìœ¼ë©´ ë¹ˆ êµ¬ì¡° ë°˜í™˜."""
	try:
		with open(CACHE_FILE, "r", encoding="utf-8") as f:
			data = json.load(f)
			if not isinstance(data, dict):
				return {"updated_at": None, "items": []}
			data.setdefault("updated_at", None)
			data.setdefault("items", [])
			return data
	except FileNotFoundError:
		return {"updated_at": None, "items": []}
	except Exception:
		# ì†ìƒ ì‹œ ì´ˆê¸°í™”
		return {"updated_at": None, "items": []}


def save_cache(items: List[Dict[str, Any]]) -> Dict[str, Any]:
	updated_at = datetime.now(timezone.utc).astimezone().isoformat()
	data = {"updated_at": updated_at, "items": items}
	with open(CACHE_FILE, "w", encoding="utf-8") as f:
		json.dump(data, f, ensure_ascii=False)
	return data


def refresh_cache() -> Dict[str, Any]:
	items, stats = fetch_internal_items()
	data = save_cache(items)
	data["stats"] = stats
	return data


def fetch_workload_schedule(company: str = None, business_name: str = None) -> Dict[str, Any]:
	"""ìµœê·¼ 3ì£¼ê°„ì˜ ì‘ì—…ëŸ‰ ìŠ¤ì¼€ì¤„ì„ ì£¼ì°¨ë³„ë¡œ ë°˜í™˜ (ìºì‹œ ìš°ì„ )
	
	Args:
		company: íšŒì‚¬ëª… í•„í„° (ì œì´íˆ¬ë©, ì¼ë¥˜ê¸°íš)
		business_name: ìƒí˜¸ëª… í•„í„° (íŠ¹ì • ì—…ì²´ë§Œ ì¡°íšŒ)
		
	Returns:
		{
			"weeks": [
				{
					"start_date": "09/12",
					"end_date": "09/18",
					"bizname": "ì—…ì²´ëª…",
					"items": [
						{"name": "ì¼ë¥˜ ì €ì¥", "workload": "300"},
						{"name": "ì¼ë¥˜ ì˜ìˆ˜ì¦B", "workload": "10"}
					]
				}
			],
			"from_cache": bool
		}
	"""
	import logging
	logger = logging.getLogger(__name__)
	
	# ìºì‹œì—ì„œ ë¨¼ì € ì¡°íšŒ
	try:
		from workload_cache import WorkloadCache
		cache = WorkloadCache()
		
		if cache.is_cache_valid():
			# ì—…ì²´ë³„ ì¡°íšŒ
			if business_name:
				cached_data = cache.get_business_workload(company, business_name)
				if cached_data:
					logger.info(f"âœ… Using cached workload data for business: {business_name}")
					result = cached_data.copy()
					result["from_cache"] = True
					return result
				else:
					logger.warning(f"âš ï¸ No cached data for business: {business_name}")
			# íšŒì‚¬ ì „ì²´ ì¡°íšŒ
			else:
				cached_data = cache.get_company_workload(company)
				if cached_data:
					logger.info(f"âœ… Using cached workload data for company: {company}")
					result = cached_data.copy()
					result["from_cache"] = True
					return result
		else:
			logger.info(f"Cache invalid or expired")
	except Exception as e:
		logger.warning(f"Failed to load from cache: {e}")
	
	# ìºì‹œ ì—†ìœ¼ë©´ ì§ì ‘ ì¡°íšŒ
	logger.info(f"Fetching workload directly for {company}/{business_name or 'all'}")
	result = fetch_workload_schedule_direct(company, business_name)
	result["from_cache"] = False
	return result


def fetch_workload_schedule_direct(company: str = None, business_name: str = None) -> Dict[str, Any]:
	"""ìµœê·¼ 3ì£¼ê°„ì˜ ì‘ì—…ëŸ‰ ìŠ¤ì¼€ì¤„ì„ ì£¼ì°¨ë³„ë¡œ ë°˜í™˜ (ì§ì ‘ ì¡°íšŒ)
	
	Args:
		company: íšŒì‚¬ëª… í•„í„° (ì œì´íˆ¬ë©, ì¼ë¥˜ê¸°íš)
		business_name: ìƒí˜¸ëª… í•„í„° (íŠ¹ì • ì—…ì²´ë§Œ ì¡°íšŒ)
		
	Returns:
		{
			"weeks": [
				{
					"start_date": "09/12",
					"end_date": "09/18",
					"bizname": "ì—…ì²´ëª…",
					"items": [
						{"name": "ì¼ë¥˜ ì €ì¥", "workload": "300"},
						{"name": "ì¼ë¥˜ ì˜ìˆ˜ì¦B", "workload": "10"}
					]
				}
			]
		}
	"""
	from datetime import date, timedelta
	import logging
	logger = logging.getLogger(__name__)
	
	settings = load_settings()
	if not settings.spreadsheet_id:
		raise RuntimeError("SPREADSHEET_ID í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
	
	# íšŒì‚¬ í•„í„°ê°€ ìˆìœ¼ë©´ ë³´ì¥ê±´ ë°ì´í„°ì—ì„œ í•´ë‹¹ íšŒì‚¬ì˜ ìƒí˜¸ëª… ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
	company_business_names = None
	guarantee_data_map = {}  # ìƒí˜¸ëª… -> ë³´ì¥ê±´ ë°ì´í„° ë§¤í•‘
	if company:
		try:
			from guarantee_manager import GuaranteeManager
			gm = GuaranteeManager()
			items = gm.get_items({"company": company})
			company_business_names = {item.get("business_name") for item in items if item.get("business_name")}
			# ìƒí˜¸ëª…ë³„ ë°ì´í„° ë§¤í•‘ (ì‘ì—… ì‹œì‘ì¼ ì°¸ì¡°ìš©)
			for item in items:
				biz = item.get("business_name")
				if biz:
					guarantee_data_map[biz] = item
			if company_business_names:
				logger.info(f"ğŸ“‹ {company} ë³´ì¥ê±´ ìƒí˜¸ëª…: {len(company_business_names)}ê°œ")
			else:
				logger.warning(f"âš ï¸ {company} ë³´ì¥ê±´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤ (ì „ì²´ ë‚´ë¶€ ì§„í–‰ê±´ ì¡°íšŒ)")
				company_business_names = None
		except Exception as e:
			logger.warning(f"ë³´ì¥ê±´ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ (ì „ì²´ ë‚´ë¶€ ì§„í–‰ê±´ ì¡°íšŒ): {e}")
			company_business_names = None
	
	client = _get_client()
	ss = client.open_by_key(settings.spreadsheet_id)
	
	# í•œêµ­ ì‹œê°„ ê¸°ì¤€ (KST)
	kst = pytz.timezone('Asia/Seoul')
	today = datetime.now(kst).date()
	all_items = []
	
	# ë””ë²„ê¹… ì¹´ìš´í„°
	total_rows = 0
	internal_rows = 0
	filtered_by_company = 0
	no_start_date = 0
	valid_items = 0
	
	ws_list = ss.worksheets()
	logger.info(f"ğŸ“Š ì‘ì—…ëŸ‰ ì¡°íšŒ ì‹œì‘ - íšŒì‚¬: {company}, ì›Œí¬ì‹œíŠ¸ ìˆ˜: {len(ws_list)}")
	
	for ws in ws_list:
		tab_title = (ws.title or "").strip()
		header_row, headers = _find_header_row(ws, settings)
		records = _build_records(ws, header_row, headers)
		
		for row in records:
			row_norm = { _normalize_key(k): v for k, v in row.items() }
			total_rows += 1
			
			# ë‚´ë¶€ ì§„í–‰ê±´ ë˜ëŠ” í›„ë¶ˆ ê±´ í•„í„°
			is_internal = _is_internal_or_postpaid(_get_value_flexible(row_norm, settings.internal_col, "INTERNAL_COLUMN"))
			if not is_internal:
				continue
			
			internal_rows += 1
			
			# ê¸°ë³¸ ì •ë³´
			bizname = str(_get_value_flexible(row_norm, settings.bizname_col, "BIZNAME_COLUMN") or "").strip()
			if not bizname:
				continue
			
			# ìƒí˜¸ëª… í•„í„° (íŠ¹ì • ì—…ì²´ë§Œ ì¡°íšŒ)
			if business_name:
				if bizname != business_name:
					continue
			
			agency_raw = str(_get_value_flexible(row_norm, settings.agency_col, "AGENCY_COLUMN") or "").strip()
			remain = _parse_int_maybe(_get_value_flexible(row_norm, settings.remaining_days_col, "REMAINING_DAYS_COLUMN"))
			workload = str(_get_value_flexible(row_norm, settings.daily_workload_col, "DAILY_WORKLOAD_COLUMN") or "").strip()
			product = str(_get_value_flexible(row_norm, settings.product_col, "PRODUCT_COLUMN") or "").strip()
			product_name = str(_get_value_flexible(row_norm, settings.product_name_col, "PRODUCT_NAME_COLUMN") or "").strip()
			
			# íšŒì‚¬ í•„í„° (ìƒí˜¸ëª… ê¸°ì¤€ìœ¼ë¡œ ë§¤ì¹­) - ìƒí˜¸ëª… í•„í„°ê°€ ì—†ì„ ë•Œë§Œ ì ìš©
			if not business_name:
				if company and company_business_names is not None:
					# ë³´ì¥ê±´ ìƒí˜¸ëª… ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” ê²ƒë§Œ í¬í•¨
					if bizname not in company_business_names:
						filtered_by_company += 1
						continue
				elif company and company_business_names is None:
					# ë³´ì¥ê±´ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ëŒ€í–‰ì‚¬ëª…ìœ¼ë¡œ í´ë°±
					if agency_raw != company:
						filtered_by_company += 1
						continue
			
			# ë§ˆê°ì¼ ê³„ì‚° (í•„ìˆ˜)
			if remain is None:
				continue
			end_date = today + timedelta(days=remain)
			
			# ì‘ì—… ì‹œì‘ì¼ íŒŒì‹± (ì—¬ëŸ¬ ì»¬ëŸ¼ëª… ì‹œë„)
			start_date_str = None
			for possible_col in ["ì‘ì—… ì‹œì‘ì¼", "ì‘ì—…ì‹œì‘ì¼", "ì‹œì‘ì¼", "ì„¸íŒ…ì¼"]:
				start_val = _get_value_flexible(row_norm, possible_col, "")
				if start_val:
					start_date_str = str(start_val).strip()
					break
			
			# ì‘ì—… ì‹œì‘ì¼ íŒŒì‹±
			start_date = None
			has_real_start_date = False
			if start_date_str:
				# ë‚ ì§œ íŒŒì‹± (YYYY-MM-DD ë˜ëŠ” YYYY.MM.DD í˜•ì‹)
				try:
					import re
					# YYYY-MM-DD ë˜ëŠ” YYYY.MM.DD í˜•ì‹
					match = re.match(r"(\d{4})[.-](\d{1,2})[.-](\d{1,2})", start_date_str)
					if match:
						year, month, day = match.groups()
						start_date = date(int(year), int(month), int(day))
						has_real_start_date = True
				except:
					pass
			
			# ì‘ì—… ì‹œì‘ì¼ì´ ì—†ìœ¼ë©´ ë³´ì¥ê±´ ë°ì´í„°ì—ì„œ ê°€ì ¸ì˜¤ê¸°
			if not has_real_start_date and bizname in guarantee_data_map:
				guarantee_item = guarantee_data_map[bizname]
				work_start = guarantee_item.get("work_start_date")
				if work_start:
					try:
						import re
						match = re.match(r"(\d{4})[.-](\d{1,2})[.-](\d{1,2})", work_start)
						if match:
							year, month, day = match.groups()
							start_date = date(int(year), int(month), int(day))
							has_real_start_date = True
							logger.debug(f"ë³´ì¥ê±´ ë°ì´í„°ì—ì„œ ì‘ì—… ì‹œì‘ì¼ ê°€ì ¸ì˜´: {bizname} -> {work_start}")
					except:
						pass
			
			# ê·¸ë˜ë„ ì‘ì—… ì‹œì‘ì¼ì´ ì—†ìœ¼ë©´ ì—­ì‚° (ë§ˆê°ì¼ - 14ì¼)
			if not has_real_start_date:
				no_start_date += 1
				# ë§ˆê°ì¼ì—ì„œ 2ì£¼ ì „ì„ ì‹œì‘ì¼ë¡œ ì¶”ì •
				start_date = end_date - timedelta(days=14)
			
			# ì‘ì—…ëª… ìƒì„± ('ì˜ìˆ˜ì¦ë¦¬ë·°' íƒ­ì€ í•­ëª© ì»¬ëŸ¼ ì‚¬ìš©)
			is_review_tab = _collapse_spaces(tab_title) == _collapse_spaces("ì˜ìˆ˜ì¦ë¦¬ë·°")
			if is_review_tab:
				# ì˜ìˆ˜ì¦ë¦¬ë·° íƒ­ì€ 'í•­ëª©' ì»¬ëŸ¼ ì½ê¸°
				item_col_value = None
				for possible_col in ["í•­ëª©", "í•­ëª©ëª…"]:
					val = _get_value_flexible(row_norm, possible_col, "")
					if val:
						item_col_value = str(val).strip()
						break
				task_display = item_col_value if item_col_value else _build_task_display(tab_title, product, product_name)
			else:
				task_display = _build_task_display(tab_title, product, product_name)
			
			all_items.append({
				"agency": agency_raw or "ë‚´ë¶€ ì§„í–‰",
				"bizname": bizname,
				"task_display": task_display,
				"workload": workload,
				"start_date": start_date,
				"end_date": end_date,
				"has_real_start_date": has_real_start_date,
				"tab_title": tab_title,  # ì—…ì¢… ë¶„ë¥˜ìš©
				"product": product,  # ì—…ì¢… ë¶„ë¥˜ìš©
				"product_name": product_name  # ì—…ì¢… ë¶„ë¥˜ìš©
			})
			valid_items += 1
	
	# í†µê³„ ë¡œê¹…
	logger.info(f"ğŸ“ˆ ì‘ì—…ëŸ‰ ì¡°íšŒ í†µê³„:")
	logger.info(f"  - ì „ì²´ í–‰: {total_rows}")
	logger.info(f"  - ë‚´ë¶€ ì§„í–‰ê±´: {internal_rows}")
	if company and company_business_names is not None:
		logger.info(f"  - {company} ë³´ì¥ê±´ ë§¤ì¹­ìœ¼ë¡œ ì œì™¸: {filtered_by_company}")
	else:
		logger.info(f"  - íšŒì‚¬ í•„í„°ë¡œ ì œì™¸: {filtered_by_company}")
	logger.info(f"  - ì‘ì—… ì‹œì‘ì¼ ì—†ìŒ (ì—­ì‚° ì²˜ë¦¬): {no_start_date}")
	logger.info(f"  - ìœ íš¨í•œ ì‘ì—…: {valid_items}")
	
	# ìµœê·¼ ì‹œì‘ì¼ ì°¾ê¸°
	if not all_items:
		logger.warning(f"âš ï¸ {company}ì˜ ì‘ì—…ëŸ‰ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
		return {"weeks": []}
	
	# ì‹¤ì œ ì‘ì—… ì‹œì‘ì¼ì´ ìˆëŠ” í•­ëª©ë“¤ë§Œìœ¼ë¡œ ìµœì‹  ë‚ ì§œ ê³„ì‚°
	items_with_real_start = [item for item in all_items if item.get("has_real_start_date")]
	
	# ì—…ì²´ë³„ ì¡°íšŒì‹œì—ëŠ” ì˜¤ëŠ˜ ê¸°ì¤€ 3ì£¼ë§Œ í‘œì‹œ (ë” ì—„ê²©)
	if business_name:
		three_weeks_ago = today - timedelta(days=21)
		logger.info(f"ğŸ“… ì—…ì²´ë³„ ì¡°íšŒ({business_name}): ì˜¤ëŠ˜({today})ë¶€í„° 3ì£¼ ì „({three_weeks_ago})")
		
		filtered_items = [
			item for item in all_items 
			if item["start_date"] >= three_weeks_ago
		]
		logger.info(f"ğŸ“Š í•„í„°ë§ ê²°ê³¼: {len(all_items)}ê°œ â†’ {len(filtered_items)}ê°œ (ìµœê·¼ 3ì£¼)")
	elif items_with_real_start:
		# íšŒì‚¬ ì „ì²´ ì¡°íšŒ: ìµœì‹  ì‹œì‘ì¼ ê¸°ì¤€ 3ì£¼
		latest_start = max(item["start_date"] for item in items_with_real_start)
		three_weeks_ago = latest_start - timedelta(days=21)
		logger.info(f"ğŸ“… ìµœì‹  ì‘ì—… ì‹œì‘ì¼: {latest_start}, 3ì£¼ ì „: {three_weeks_ago}")
		
		# ì‹¤ì œ ì‹œì‘ì¼ì´ ìˆëŠ” í•­ëª©ì€ 3ì£¼ í•„í„°ë§, ì—†ëŠ” í•­ëª©ì€ ëª¨ë‘ í¬í•¨
		filtered_items = [
			item for item in all_items 
			if (item.get("has_real_start_date") and item["start_date"] >= three_weeks_ago)
			or (not item.get("has_real_start_date"))  # ì‹œì‘ì¼ì´ ì—†ëŠ” í•­ëª©ì€ ëª¨ë‘ í¬í•¨
		]
	else:
		# ì‹¤ì œ ì‹œì‘ì¼ì´ ì—†ëŠ” ê²½ìš°: ëª¨ë“  í•­ëª© í¬í•¨
		logger.info(f"ğŸ“… ì‹¤ì œ ì‘ì—… ì‹œì‘ì¼ì´ ì—†ì–´ ì „ì²´ ì‘ì—… í¬í•¨")
		filtered_items = all_items
	
	# ê¸°ê°„ë³„ ê·¸ë£¹í•‘ (ê°™ì€ ì‹œì‘ì¼-ì¢…ë£Œì¼ë¼ë¦¬ ë¬¶ìŒ)
	period_groups = {}
	for item in filtered_items:
		# ê¸°ê°„ì„ í‚¤ë¡œ ê·¸ë£¹í•‘
		key = (item["start_date"], item["end_date"])
		if key not in period_groups:
			period_groups[key] = {}
		
		# ê°™ì€ ì‘ì—…ëª…ì´ë©´ ì‘ì—…ëŸ‰ í•©ì‚°
		task_name = item["task_display"]
		
		# ì‘ì—…ëŸ‰ íŒŒì‹±
		try:
			wl_num = _parse_int_maybe(item["workload"]) or 0
		except:
			wl_num = 0
		
		if task_name in period_groups[key]:
			# ê¸°ì¡´ ì‘ì—…ëŸ‰ì— í•©ì‚°
			period_groups[key][task_name] += wl_num
		else:
			# ìƒˆ ì‘ì—… ì¶”ê°€
			period_groups[key][task_name] = wl_num
	
	# ì •ë ¬ ë° í¬ë§·íŒ… (ê¸°ê°„ë³„ë¡œ ì •ë ¬)
	weeks = []
	for (start_dt, end_dt), tasks in sorted(period_groups.items(), key=lambda x: x[0][0]):  # ì‹œì‘ì¼ ê¸°ì¤€ ì •ë ¬
		items = []
		# ì‹œíŠ¸ ìˆœì„œ ìœ ì§€ë¥¼ ìœ„í•´ ì •ë ¬í•˜ì§€ ì•Šê³  ì›ë˜ ìˆœì„œëŒ€ë¡œ (ì‚½ì… ìˆœì„œ ìœ ì§€)
		for task_name, total_workload in tasks.items():
			items.append({
				"name": task_name,
				"workload": str(total_workload) if total_workload > 0 else "0"
			})
		
		weeks.append({
			"start_date": start_dt.strftime("%m/%d"),
			"end_date": end_dt.strftime("%m/%d"),
			"items": items
		})
	
	# ìµœì¢… í†µê³„
	total_workload_items = sum(len(items) for items in period_groups.values())
	logger.info(f"âœ… {company} ì‘ì—…ëŸ‰ ì¡°íšŒ ì™„ë£Œ - {len(weeks)}ê°œ ê¸°ê°„, ì´ {total_workload_items}ê°œ ì‘ì—… (í•„í„°ë§ í›„: {len(filtered_items)}ê±´)")
	
	return {"weeks": weeks}


