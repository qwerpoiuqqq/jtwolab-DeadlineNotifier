import os
import json
import argparse
from typing import Dict, List
from flask import Flask, render_template, request, jsonify, Response, stream_with_context, redirect, url_for, session
from functools import wraps
from dotenv import load_dotenv
from datetime import date, timedelta, datetime
import re
import pytz
from apscheduler.schedulers.background import BackgroundScheduler
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

from sheet_client import fetch_grouped_messages, load_settings, inspect_sheets, diagnose_matches, fetch_grouped_messages_by_date, stream_grouped_messages_by_date, mark_checked_for_agency, mark_checked_for_agencies, list_sheet_tabs, inspect_sheets_by_id, compute_settlement_rows
from internal_manager import load_cache as internal_load_cache, refresh_cache as internal_refresh_cache, fetch_workload_schedule
from guarantee_manager import GuaranteeManager
from auth import AuthManager, ROLES

try:
	from data_security import DataSecurity
	SECURITY_AVAILABLE = True
except ImportError:
	SECURITY_AVAILABLE = False


# .env ë¡œë“œ
load_dotenv()


def _strip_parentheses(text: str) -> str:
	"""ë¬¸ìì—´ì—ì„œ ì†Œê´„í˜¸ ë‚´ ë‚´ìš©ì„ ì œê±°í•œë‹¤. ì˜ˆ: 'ì‘ì—…ëª…(ë¶€ê°€)' -> 'ì‘ì—…ëª…'"""
	if not text:
		return text
	return re.sub(r"\s*\([^)]*\)", "", text).strip()


# ë¡œê·¸ì¸ í•„ìˆ˜ ë°ì½”ë ˆì´í„°
def login_required(f):
	@wraps(f)
	def decorated_function(*args, **kwargs):
		if not session.get("user"):
			# API ìš”ì²­ì¸ ê²½ìš° JSON ì‘ë‹µ
			if request.path.startswith("/api/"):
				return jsonify({"error": "unauthorized", "message": "ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤."}), 401
			# í˜ì´ì§€ ìš”ì²­ì¸ ê²½ìš° ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
			return redirect(url_for("login_page"))
		return f(*args, **kwargs)
	return decorated_function


# ê´€ë¦¬ì ê¶Œí•œ í•„ìˆ˜ ë°ì½”ë ˆì´í„°
def admin_required(f):
	@wraps(f)
	def decorated_function(*args, **kwargs):
		user = session.get("user")
		if not user:
			if request.path.startswith("/api/"):
				return jsonify({"error": "unauthorized"}), 401
			return redirect(url_for("login_page"))
		if user.get("role") != "admin":
			if request.path.startswith("/api/"):
				return jsonify({"error": "forbidden", "message": "ê´€ë¦¬ì ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤."}), 403
			return redirect(url_for("index"))
		return f(*args, **kwargs)
	return decorated_function


def create_app() -> Flask:
	app = Flask(__name__)
	
	# ì„¸ì…˜ ì•”í˜¸í™” í‚¤ ì„¤ì •
	app.secret_key = os.getenv("SECRET_KEY", "deadline-notifier-secret-key-change-me")
	app.config["SESSION_COOKIE_SECURE"] = os.getenv("FLASK_ENV") == "production"
	app.config["SESSION_COOKIE_HTTPONLY"] = True
	app.config["SESSION_COOKIE_SAMESITE"] = "Lax"
	app.config["PERMANENT_SESSION_LIFETIME"] = timedelta(days=7)
	
	# ì¸ì¦ ë§¤ë‹ˆì € ì´ˆê¸°í™”
	auth_manager = AuthManager()
	
	# ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™”
	scheduler = BackgroundScheduler(timezone=pytz.timezone('Asia/Seoul'))
	scheduler.start()
	
	# Render ë°°í¬: ì•± ì‹œì‘ ì‹œ ìë™ ì´ˆê¸°í™”
	if os.getenv("AUTO_SYNC_ON_START", "false").lower() == "true":
		logger.info("ğŸš€ Auto-sync on startup enabled (Render mode)")
		
		def init_on_startup():
			"""ì•± ì‹œì‘ ì‹œ ë°ì´í„° ì´ˆê¸°í™”"""
			import time
			time.sleep(5)  # ì•± ì™„ì „ ì‹œì‘ ëŒ€ê¸°
			
			try:
				logger.info("ğŸ“¡ Starting auto-sync from Google Sheets...")
				gm = GuaranteeManager()
				result = gm.sync_from_google_sheets()
				logger.info(f"âœ… Auto-sync completed: {result}")
			except Exception as e:
				logger.error(f"âŒ Auto-sync failed: {e}")
			
			try:
				logger.info("âš¡ Starting workload cache refresh...")
				from workload_cache import refresh_all_workload_cache
				result = refresh_all_workload_cache()
				logger.info(f"âœ… Workload cache refreshed: {result['message']}")
			except Exception as e:
				logger.error(f"âŒ Workload cache refresh failed: {e}")
		
		# ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œë¡œ ì‹¤í–‰
		import threading
		init_thread = threading.Thread(target=init_on_startup)
		init_thread.daemon = True
		init_thread.start()
	
	# ìë™ ë™ê¸°í™” íƒœìŠ¤í¬
	def sync_guarantee_data():
		"""ë³´ì¥ê±´ ë°ì´í„° ìë™ ë™ê¸°í™”"""
		try:
			logger.info(f"Starting automatic sync at {datetime.now(pytz.timezone('Asia/Seoul'))}")
			gm = GuaranteeManager()
			result = gm.sync_from_google_sheets()
			logger.info(f"Sync completed: {result}")
		except Exception as e:
			logger.error(f"Sync failed: {e}")
	
	# ì‘ì—…ëŸ‰ ìºì‹œ ìë™ ê°±ì‹  íƒœìŠ¤í¬
	def refresh_workload_cache():
		"""ì‘ì—…ëŸ‰ ìºì‹œ ìë™ ê°±ì‹ """
		try:
			logger.info(f"Starting workload cache refresh at {datetime.now(pytz.timezone('Asia/Seoul'))}")
			from workload_cache import refresh_all_workload_cache
			result = refresh_all_workload_cache()
			logger.info(f"Workload cache refresh completed: {result['message']}")
		except Exception as e:
			logger.error(f"Workload cache refresh failed: {e}")
	
	# ìŠ¤ì¼€ì¤„ëŸ¬ ì ê¸ˆ (ë™ì‹œ ì‹¤í–‰ ë°©ì§€)
	import threading
	_scheduler_lock = threading.Lock()
	_scheduler_running = {"rank_crawl": False}
	
	# ìˆœìœ„ í¬ë¡¤ë§ ìë™ ì‹¤í–‰ íƒœìŠ¤í¬ (N2 í¬í•¨, Google Sheets ì €ì¥)
	def crawl_ranks_auto():
		"""ìˆœìœ„ ìë™ í¬ë¡¤ë§ (N2 í¬í•¨)
		
		ì£¼ì˜: Renderì—ì„œ workers=1 ê¶Œì¥. ë˜ëŠ” ì™¸ë¶€ cronì´ token endpointë¥¼ í˜¸ì¶œí•˜ëŠ” ë°©ì‹ ì‚¬ìš©.
		ë™ì‹œ ì‹¤í–‰ ë°©ì§€ë¥¼ ìœ„í•´ ì ê¸ˆ ì‚¬ìš©.
		"""
		# ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
		if _scheduler_running.get("rank_crawl"):
			logger.warning("âš ï¸ Rank crawling already running, skipping...")
			return
		
		with _scheduler_lock:
			_scheduler_running["rank_crawl"] = True
		
		from scheduler_logs import log_scheduler_event
		log_scheduler_event("rank_crawl", "ìˆœìœ„ í¬ë¡¤ë§", "started", "í¬ë¡¤ë§ ì‹œì‘")
		try:
			logger.info(f"ğŸ† Starting automatic rank crawling at {datetime.now(pytz.timezone('Asia/Seoul'))}")
			from rank_crawler import crawl_ranks_for_company
			
			# ì „ì²´ íšŒì‚¬ í•œ ë²ˆì— í¬ë¡¤ë§ (None = ëª¨ë‘)
			result = crawl_ranks_for_company(None)
			logger.info(f"âœ… Rank crawling completed: {result.get('message', 'Unknown')}")
			logger.info(f"   Crawled: {result.get('crawled_count', 0)}, Failed: {result.get('failed_count', 0)}")
			log_scheduler_event("rank_crawl", "ìˆœìœ„ í¬ë¡¤ë§", "success", 
				f"í¬ë¡¤ë§ {result.get('crawled_count', 0)}ê±´ ì™„ë£Œ", result)
			
			# 15ì‹œ í¬ë¡¤ë§ì¸ ê²½ìš° ë³´ì¥ê±´ ì‹œíŠ¸ ìë™ ì—…ë°ì´íŠ¸
			current_hour = datetime.now(pytz.timezone('Asia/Seoul')).hour
			if current_hour >= 12:  # ì˜¤í›„ í¬ë¡¤ë§ì¸ ê²½ìš°
				# ë³´ì¥ê±´ ì‹œíŠ¸ ì—…ë°ì´íŠ¸
				try:
					log_scheduler_event("guarantee_update", "ë³´ì¥ê±´ ì‹œíŠ¸ ì—…ë°ì´íŠ¸", "started", "ì—…ë°ì´íŠ¸ ì‹œì‘")
					logger.info("ğŸ“‹ Updating guarantee sheets...")
					from rank_update_service import update_guarantee_sheets_from_snapshots
					update_result = update_guarantee_sheets_from_snapshots()
					logger.info(f"âœ… Guarantee sheets updated: {update_result}")
					log_scheduler_event("guarantee_update", "ë³´ì¥ê±´ ì‹œíŠ¸ ì—…ë°ì´íŠ¸", "success", "ì—…ë°ì´íŠ¸ ì™„ë£Œ")
				except Exception as update_error:
					logger.error(f"âŒ Guarantee sheet update failed: {update_error}")
					log_scheduler_event("guarantee_update", "ë³´ì¥ê±´ ì‹œíŠ¸ ì—…ë°ì´íŠ¸", "failed", str(update_error))
				
				# ë ˆì‹œí”¼ ë¶„ì„ ì‹¤í–‰
				try:
					log_scheduler_event("recipe_analysis", "ë ˆì‹œí”¼ ë¶„ì„", "started", "ë¶„ì„ ì‹œì‘")
					logger.info("ğŸ“Š Running recipe analysis...")
					from recipe_analyzer import get_analyzer
					analyzer = get_analyzer()
					analysis_result = analyzer.analyze_all(weeks=3)
					logger.info(f"âœ… Recipe analysis complete: {analysis_result.get('total_analyzed', 0)} businesses")
					log_scheduler_event("recipe_analysis", "ë ˆì‹œí”¼ ë¶„ì„", "success", 
						f"{analysis_result.get('total_analyzed', 0)}ê°œ ì—…ì²´ ë¶„ì„ ì™„ë£Œ")
				except Exception as analysis_error:
					logger.error(f"âŒ Recipe analysis failed: {analysis_error}")
					log_scheduler_event("recipe_analysis", "ë ˆì‹œí”¼ ë¶„ì„", "failed", str(analysis_error))
				
				# í•™ìŠµ ë°ì´í„°ì…‹ ë¹Œë“œ (í¬ë¡¤ë§ í›„ ìë™ ì‹¤í–‰)
				try:
					log_scheduler_event("training_build", "í•™ìŠµ ë°ì´í„°ì…‹ ë¹Œë“œ", "started", "ë¹Œë“œ ì‹œì‘")
					logger.info("ğŸ“ Building training dataset...")
					from training_dataset_builder import build_and_save
					build_result = build_and_save(weeks=3)
					logger.info(f"âœ… Training dataset built: {build_result.get('training_rows_count', 0)} rows")
					log_scheduler_event("training_build", "í•™ìŠµ ë°ì´í„°ì…‹ ë¹Œë“œ", "success", 
						f"{build_result.get('training_rows_count', 0)}í–‰ ìƒì„± ì™„ë£Œ")
				except Exception as build_error:
					logger.error(f"âŒ Training dataset build failed: {build_error}")
					log_scheduler_event("training_build", "í•™ìŠµ ë°ì´í„°ì…‹ ë¹Œë“œ", "failed", str(build_error))
					
		except Exception as e:
			logger.error(f"âŒ Automatic rank crawling failed: {e}")
			import traceback
			logger.error(traceback.format_exc())
			log_scheduler_event("rank_crawl", "ìˆœìœ„ í¬ë¡¤ë§", "failed", str(e))
		finally:
			# ì ê¸ˆ í•´ì œ
			with _scheduler_lock:
				_scheduler_running["rank_crawl"] = False
			logger.info("ğŸ”“ Rank crawling lock released")
	
	# ë§¤ì¼ 9ì‹œ, 16ì‹œ ìŠ¤ì¼€ì¤„ ë“±ë¡ (ë³´ì¥ê±´ ë™ê¸°í™”)
	scheduler.add_job(func=sync_guarantee_data, trigger="cron", hour=9, minute=0, id="morning_sync")
	scheduler.add_job(func=sync_guarantee_data, trigger="cron", hour=16, minute=0, id="afternoon_sync")
	
	# ë§¤ì¼ 11:20 ìŠ¤ì¼€ì¤„ ë“±ë¡ (Worklog ìºì‹œ ê°±ì‹ )
	def refresh_worklog_cache_task():
		"""Worklog ìºì‹œ ìë™ ê°±ì‹ """
		from scheduler_logs import log_scheduler_event
		log_scheduler_event("worklog_cache", "Worklog ìºì‹œ", "started", "ìºì‹œ ê°±ì‹  ì‹œì‘")
		try:
			logger.info(f"ğŸ“ Starting worklog cache refresh at {datetime.now(pytz.timezone('Asia/Seoul'))}")
			from worklog_cache import refresh_worklog_cache as _refresh_worklog
			result = _refresh_worklog()
			logger.info(f"âœ… Worklog cache refresh completed: {result.get('message')}")
			log_scheduler_event("worklog_cache", "Worklog ìºì‹œ", "success", 
				f"{result.get('records_count', 0)}ê±´ ê°±ì‹  ì™„ë£Œ", result)
		except Exception as e:
			logger.error(f"âŒ Worklog cache refresh failed: {e}")
			log_scheduler_event("worklog_cache", "Worklog ìºì‹œ", "failed", str(e))
	
	scheduler.add_job(func=refresh_worklog_cache_task, trigger="cron", hour=3, minute=30, id="worklog_cache_refresh_task")
	
	# ë§¤ì¼ 03:00 ìŠ¤ì¼€ì¤„ ë“±ë¡ (ì‘ì—…ëŸ‰ ìºì‹œ ê°±ì‹  - ìƒˆë²½ ì‹œê°„ëŒ€ë¡œ ë³€ê²½)
	scheduler.add_job(func=refresh_workload_cache, trigger="cron", hour=3, minute=0, id="workload_cache_refresh")
	
	# ë§¤ì¼ 15:10 ìŠ¤ì¼€ì¤„ ë“±ë¡ (ìˆœìœ„ í¬ë¡¤ë§ - 1ì¼ 1íšŒ)
	# ì£¼ì˜: Render workers=1ì´ ì•„ë‹ˆë©´ ì™¸ë¶€ cron ì‚¬ìš© ê¶Œì¥
	if os.getenv("USE_INTERNAL_SCHEDULER", "true").lower() == "true":
		scheduler.add_job(func=crawl_ranks_auto, trigger="cron", hour=15, minute=10, id="daily_rank_crawl")
		logger.info("ğŸ“… Internal scheduler enabled:")
		logger.info("   - 03:00 ì‘ì—…ëŸ‰ ìºì‹œ ê°±ì‹ ")
		logger.info("   - 03:30 Worklog ìºì‹œ ê°±ì‹ ")
		logger.info("   - 15:10 ìˆœìœ„ í¬ë¡¤ë§")
	else:
		logger.info("ğŸ“… Internal scheduler disabled. Use /api/cron/crawl-ranks with CRON_TOKEN")

	# --- ì¸ì¦ ê´€ë ¨ ë¼ìš°íŠ¸ ---
	@app.route("/login", methods=["GET"])
	def login_page():
		"""ë¡œê·¸ì¸ í˜ì´ì§€"""
		# ì´ë¯¸ ë¡œê·¸ì¸ëœ ê²½ìš° ë©”ì¸ìœ¼ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
		if session.get("user"):
			return redirect(url_for("index"))
		return render_template("login.html")
	
	@app.route("/api/auth/login", methods=["POST"])
	def api_auth_login():
		"""ë¡œê·¸ì¸ API"""
		try:
			data = request.get_json(force=True) or {}
		except Exception:
			return jsonify({"error": "invalid_json"}), 400
		
		username = str(data.get("username", "")).strip()
		password = str(data.get("password", ""))
		
		if not username or not password:
			return jsonify({"error": "ì•„ì´ë””ì™€ ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."}), 400
		
		user = auth_manager.authenticate(username, password)
		if user:
			session.permanent = True
			session["user"] = user
			logger.info(f"User logged in: {username}")
			return jsonify({"ok": True, "user": user, "redirect": "/"}), 200
		
		return jsonify({"error": "ì•„ì´ë”” ë˜ëŠ” ë¹„ë°€ë²ˆí˜¸ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."}), 401
	
	@app.route("/api/auth/logout", methods=["POST"])
	def api_auth_logout():
		"""ë¡œê·¸ì•„ì›ƒ API"""
		user = session.get("user")
		if user:
			logger.info(f"User logged out: {user.get('username')}")
		session.clear()
		return jsonify({"ok": True}), 200
	
	@app.route("/api/auth/me", methods=["GET"])
	@login_required
	def api_auth_me():
		"""í˜„ì¬ ë¡œê·¸ì¸ ì‚¬ìš©ì ì •ë³´"""
		return jsonify({"user": session.get("user")}), 200
	
	# --- ê³„ì • ê´€ë¦¬ API (ê´€ë¦¬ì ì „ìš©) ---
	@app.route("/api/admin/users", methods=["GET"])
	@admin_required
	def api_admin_users():
		"""ì‚¬ìš©ì ëª©ë¡ ì¡°íšŒ"""
		include_inactive = request.args.get("include_inactive", "").lower() == "true"
		users = auth_manager.get_all_users(include_inactive)
		return jsonify({"users": users, "count": len(users)}), 200
	
	@app.route("/api/admin/users", methods=["POST"])
	@admin_required
	def api_admin_create_user():
		"""ì‚¬ìš©ì ìƒì„±"""
		try:
			data = request.get_json(force=True) or {}
		except Exception:
			return jsonify({"error": "invalid_json"}), 400
		
		username = str(data.get("username", "")).strip()
		password = str(data.get("password", ""))
		role = str(data.get("role", "user")).strip()
		name = str(data.get("name", "")).strip()
		
		if not username or not password:
			return jsonify({"error": "ì•„ì´ë””ì™€ ë¹„ë°€ë²ˆí˜¸ëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤."}), 400
		
		current_user = session.get("user", {})
		user = auth_manager.create_user(username, password, role, name, current_user.get("id"))
		
		if user:
			return jsonify({"ok": True, "user": user}), 201
		return jsonify({"error": "ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì•„ì´ë””ì…ë‹ˆë‹¤."}), 400
	
	@app.route("/api/admin/users/<user_id>", methods=["PUT"])
	@admin_required
	def api_admin_update_user(user_id):
		"""ì‚¬ìš©ì ìˆ˜ì •"""
		try:
			data = request.get_json(force=True) or {}
		except Exception:
			return jsonify({"error": "invalid_json"}), 400
		
		current_user = session.get("user", {})
		user = auth_manager.update_user(user_id, data, current_user.get("id"))
		
		if user:
			return jsonify({"ok": True, "user": user}), 200
		return jsonify({"error": "ì‚¬ìš©ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}), 404
	
	@app.route("/api/admin/users/<user_id>", methods=["DELETE"])
	@admin_required
	def api_admin_delete_user(user_id):
		"""ì‚¬ìš©ì ì‚­ì œ"""
		current_user = session.get("user", {})
		
		# ìê¸° ìì‹ ì€ ì‚­ì œ ë¶ˆê°€
		if user_id == current_user.get("id"):
			return jsonify({"error": "ìì‹ ì˜ ê³„ì •ì€ ì‚­ì œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}), 400
		
		if auth_manager.delete_user(user_id, current_user.get("id")):
			return jsonify({"ok": True}), 200
		return jsonify({"error": "ì‚­ì œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}), 400
	
	@app.route("/api/admin/users/<user_id>/password", methods=["PUT"])
	@admin_required
	def api_admin_change_password(user_id):
		"""ë¹„ë°€ë²ˆí˜¸ ë³€ê²½ (ê´€ë¦¬ììš©)"""
		try:
			data = request.get_json(force=True) or {}
		except Exception:
			return jsonify({"error": "invalid_json"}), 400
		
		new_password = str(data.get("password", ""))
		if not new_password:
			return jsonify({"error": "ìƒˆ ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."}), 400
		
		if auth_manager.change_password(user_id, new_password):
			return jsonify({"ok": True}), 200
		return jsonify({"error": "ì‚¬ìš©ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}), 404
	
	@app.route("/api/admin/roles", methods=["GET"])
	@login_required
	def api_admin_roles():
		"""ì—­í•  ëª©ë¡ ì¡°íšŒ"""
		return jsonify({"roles": ROLES}), 200

	@app.route("/", methods=["GET"])  # ë©”ì¸ í˜ì´ì§€: í¼ + ê²°ê³¼
	@login_required
	def index():
		settings = load_settings()
		days_param = request.args.get("days", "").strip()
		base_date_str = request.args.get("base_date", "").strip()
		filter_mode = request.args.get("filter_mode", "agency").strip().lower()  # 'agency' | 'internal'
		did_fetch = request.args.get("submit", "") == "1"

		# ê¸°ì¤€ì¼ ì²˜ë¦¬ (ê¸°ë³¸: ì˜¤ëŠ˜)
		if base_date_str:
			try:
				base_dt = date.fromisoformat(base_date_str)
			except ValueError:
				base_dt = date.today()
		else:
			base_dt = date.today()

		ordered_days: List[int] = []
		day_to_date: Dict[int, str] = {}
		day_to_date_label: Dict[int, str] = {}
		grouped_by_date: Dict[str, Dict[int, Dict[str, List[str]]]] = {}
		agency_to_message: Dict[str, str] = {}
		agency_to_message_workload: Dict[str, str] = {}
		agency_to_date_line: Dict[str, str] = {}
		error = None
		suggested_prefix = ""

		if did_fetch:
			selected_days = _parse_days(days_param)
			ordered_days = sorted(selected_days)

			# ë‚ ì§œ ë§¤í•‘ ìƒì„± (YYYY-MM-DD ë° ìš”ì¼ í¬í•¨ ë¼ë²¨)
			weekday_kr = ["ì›”", "í™”", "ìˆ˜", "ëª©", "ê¸ˆ", "í† ", "ì¼"]
			for d in ordered_days:
				cur = base_dt + timedelta(days=d)
				day_to_date[d] = cur.isoformat()
				day_to_date_label[d] = f"{cur.isoformat()}({weekday_kr[cur.weekday()]})"

			try:
				# ë‚ ì§œë³„ ê·¸ë£¹í•‘ (í•„í„° ëª¨ë“œ ì ìš©)
				grouped_by_date = fetch_grouped_messages_by_date(selected_days=selected_days, settings=settings, filter_mode=filter_mode)
			except Exception as e:
				grouped_by_date = {}
				error = str(e)
			else:
				error = None

			# ë³µë¶™ í¬ë§· 2ì¢… ìƒì„±
			# 1) ê¸°ë³¸: ë‚ ì§œ(ìš”ì¼) â†’ <ì‘ì—…ëª…> â†’ ìƒí˜¸ëª…
			# 2) ì‘ì—…ëŸ‰ í¬í•¨: ë‚ ì§œ(ìš”ì¼) â†’ <ì‘ì—…ëª…> â†’ ìƒí˜¸ëª… : ì¼ì‘ì—…ëŸ‰
			name_wl_re = re.compile(r"^(.+?)\s*\(ì¼ì‘ì—…ëŸ‰\s+(.*?)\)$")
			for category, by_agency in grouped_by_date.items():
				for agency, by_day in by_agency.items():
					parts_base: List[str] = []
					parts_wl: List[str] = []
					for d in sorted(by_day.keys()):
						# ë‚ ì§œ í—¤ë”
						date_label = day_to_date_label.get(d, f"+{d}")
						parts_base.append(date_label)
						parts_wl.append(date_label)
						# ì‘ì—…ëª…ê³¼ ìƒí˜¸ë“¤
						for task, names in by_day[d].items():
							if not names:
								continue
							display_task = _strip_parentheses(task)
							parts_base.append(f"<{display_task}>")
							parts_wl.append(f"<{display_task}>")
							for name in names:
								name_str = str(name).strip()
								m = name_wl_re.match(name_str)
								if m:
									base_name = m.group(1).strip()
									workload_val = m.group(2).strip()
									parts_base.append(base_name)
									parts_wl.append(f"{base_name} : {workload_val}")
								else:
									# ì‘ì—…ëŸ‰ ì •ë³´ê°€ ì—†ìœ¼ë©´ ë™ì¼í•˜ê²Œ í‘œê¸°
									parts_base.append(name_str)
									parts_wl.append(name_str)
							# ì‘ì—… ë¸”ë¡ ì‚¬ì´: 1ì¤„ ê³µë°±
							parts_base.append("")
							parts_wl.append("")
						# ë‚ ì§œ ë¸”ë¡ ì‚¬ì´: ì¶”ê°€ë¡œ 1ì¤„ ë” ê³µë°±(= ì´ 2ì¤„)
						parts_base.append("")
						parts_wl.append("")
					agency_to_message[agency] = "\n".join(parts_base).rstrip()
					agency_to_message_workload[agency] = "\n".join(parts_wl).rstrip()

			# ëŒ€í–‰ì‚¬ë³„ ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ë§ˆê°ì¼ ë²”ìœ„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‚ ì§œ ë¬¸êµ¬(ìš”ì¼ í¬í•¨) ìƒì„±
			weekday_kr = ["ì›”", "í™”", "ìˆ˜", "ëª©", "ê¸ˆ", "í† ", "ì¼"]
			def fmt_mmdd_w(dt: date) -> str:
				return f"{dt.month:02d}/{dt.day:02d}({weekday_kr[dt.weekday()]})"
			for category, by_agency in grouped_by_date.items():
				for agency, by_day in by_agency.items():
					present_days = sorted(list(by_day.keys()))
					if not present_days:
						continue
					start_dt = base_dt + timedelta(days=present_days[0])
					end_dt = base_dt + timedelta(days=present_days[-1])
					if start_dt == end_dt:
						line = f"{fmt_mmdd_w(start_dt)} ë§ˆê°ê±´ ì•ˆë‚´ë“œë¦½ë‹ˆë‹¤."
					else:
						line = f"{fmt_mmdd_w(start_dt)} ~ {fmt_mmdd_w(end_dt)} ë§ˆê°ê±´ ì•ˆë‚´ë“œë¦½ë‹ˆë‹¤."
					agency_to_date_line[agency] = line

			# ì„ íƒëœ ë‚ ì§œ ë²”ìœ„ ê¸°ë°˜ ì¶”ì²œ ì²« ë©˜íŠ¸ ìƒì„± (ì¸ì‚¬ + ë‚ ì§œ ë¬¸êµ¬, MM/DD í¬ë§·)
			if ordered_days:
				all_dates = [base_dt + timedelta(days=d) for d in ordered_days]
				start_dt = min(all_dates)
				end_dt = max(all_dates)
				greeting = "ëŒ€í‘œë‹˜ ì•ˆë…•í•˜ì„¸ìš”~"
				def mmdd(dt: date) -> str:
					return f"{dt.month:02d}/{dt.day:02d}"
				if start_dt == end_dt:
					line = f"{mmdd(start_dt)} ë§ˆê°ê±´ ì•ˆë‚´ë“œë¦½ë‹ˆë‹¤."
				else:
					line = f"{mmdd(start_dt)} ~ {mmdd(end_dt)} ë§ˆê°ê±´ ì•ˆë‚´ë“œë¦½ë‹ˆë‹¤."
				suggested_prefix = greeting + "\n" + line

		# ë§ˆê°ì¼ë³„ í†µê³„ ê³„ì‚° (0~5ì¼)
		deadline_stats: Dict[int, List[str]] = {i: [] for i in range(6)}
		total_agency_count = 0
		if did_fetch and grouped_by_date:
			for category, by_agency in grouped_by_date.items():
				total_agency_count += len(by_agency)
				for agency, by_day in by_agency.items():
					for day in by_day.keys():
						if 0 <= day <= 5 and agency not in deadline_stats[day]:
							deadline_stats[day].append(agency)

		return render_template(
			"index.html",
			error=error,
			total_agency_count=total_agency_count,
			did_fetch=did_fetch,
			days_param=days_param,
			base_date_str=base_date_str or base_dt.isoformat(),
			day_to_date=day_to_date,
			day_to_date_label=day_to_date_label,
			ordered_days=ordered_days,
			grouped=grouped_by_date,
			agency_to_message=agency_to_message,
			agency_to_message_workload=agency_to_message_workload,
			agency_to_date_line=agency_to_date_line,
			deadline_stats=deadline_stats,
			settings=settings,
			filter_mode=filter_mode,
			suggested_prefix=suggested_prefix,
		)

	@app.route("/manage", methods=["GET"])  # ì›”ë³´ì¥ ê´€ë¦¬ ëŒ€ì‹œë³´ë“œ
	@login_required
	def manage():
		return render_template("manage.html")

	@app.route("/settlement", methods=["GET"])  # ê²°ì¬ì„  Â· ì •ì‚° í˜ì´ì§€ (UI ìŠ¤ì¼ˆë ˆí†¤)
	@login_required
	def settlement():
		from flask import send_file
		# í…œí”Œë¦¿ ì—”ì§„ ê²½ìœ  ëŒ€ì‹  íŒŒì¼ì„ ì§ì ‘ ì„œë¹™í•˜ì—¬, í…œí”Œë¦¿ ë¡œë”/ìºì‹œ ì´ìŠˆë¥¼ ìš°íšŒí•œë‹¤.
		return send_file(os.path.join(app.root_path, "templates", "settlement.html"), mimetype="text/html; charset=utf-8")

	# --- ê²°ì¬ì„  ë³´ì¡° APIë“¤ ---
	@app.route("/api/settlement/tabs", methods=["GET"])  # ì‹œíŠ¸ íƒ­ ì œëª© ëª©ë¡ (ê²°ì¬ì„  ì „ìš© ì‹œíŠ¸)
	def api_settlement_tabs():
		try:
			settlement_ssid = os.getenv("SETTLEMENT_SPREADSHEET_ID", "").strip()
			tabs = list_sheet_tabs(settlement_ssid or None)
		except Exception as e:
			return jsonify({"error": str(e)}), 500
		return jsonify({"tabs": tabs}), 200

	@app.route("/api/settlement/pricebook", methods=["GET", "POST"])  # ë‹¨ê°€/ê³„ì¢Œ ì €ì¥ì†Œ - íŒŒì¼ ê¸°ë°˜(ë¡œì»¬)
	def api_settlement_pricebook():
		storage_path = os.getenv("PRICEBOOK_PATH", os.path.join(os.getcwd(), "pricebook.json"))
		if request.method == "GET":
			try:
				if os.path.exists(storage_path):
					with open(storage_path, "r", encoding="utf-8") as f:
						data = json.load(f)
				else:
					data = []
			except Exception as e:
				return jsonify({"error": str(e)}), 500
			return jsonify({"items": data}), 200
		# POST: ì €ì¥
		try:
			payload = request.get_json(force=True, silent=False) or {}
		except Exception:
			return jsonify({"error": "invalid_json"}), 400
		items = payload.get("items")
		if not isinstance(items, list):
			return jsonify({"error": "invalid_items"}), 400
		try:
			with open(storage_path, "w", encoding="utf-8") as f:
				json.dump(items, f, ensure_ascii=False, indent=2)
		except Exception as e:
			return jsonify({"error": str(e)}), 500
		return jsonify({"ok": True}), 200

	@app.route("/api/settlement/pricebook/upload", methods=["POST"])  # XLSX ì—…ë¡œë“œ â†’ í•­ëª© íŒŒì‹± ë°˜í™˜
	def api_settlement_pricebook_upload():
		from io import BytesIO
		from openpyxl import load_workbook
		f = request.files.get("file")
		if not f:
			return jsonify({"error": "missing_file"}), 400
		try:
			buf = BytesIO(f.read())
			wb = load_workbook(buf, data_only=True)
			ws = wb.active
		except Exception as e:
			return jsonify({"error": f"xlsx_load_failed: {e}"}), 400
		# í—¤ë” ë§¤í•‘: ê±°ë˜ì²˜, ìƒí’ˆëª…, ìœ í˜•, ë‹¨ê°€, ê³„ì¢Œ, ì˜ˆê¸ˆì£¼
		items = []
		try:
			headers = []
			for cell in ws[1]:
				headers.append(str(cell.value or "").strip())
			def idx(name: str) -> int | None:
				try:
					return headers.index(name)
				except ValueError:
					return None
			i_client = idx("ê±°ë˜ì²˜"); i_product = idx("ìƒí’ˆëª…"); i_type = idx("ìœ í˜•"); i_price = idx("ë‹¨ê°€"); i_account = idx("ê³„ì¢Œ"); i_bank = idx("ì€í–‰"); i_holder = idx("ì˜ˆê¸ˆì£¼")
			if i_client is None or i_product is None or i_price is None:
				return jsonify({"error": "missing_required_headers"}), 400
			for r in ws.iter_rows(min_row=2):
				def get(i):
					if i is None: return ""
					v = r[i].value if i < len(r) else ""
					return str(v).strip() if v is not None else ""
				client = get(i_client); product = get(i_product)
				if not client and not product:
					continue
				type_s = get(i_type)
				type_s = "ê³µí†µ" if type_s not in ("ì €ì¥", "íŠ¸ë˜í”½") else type_s
				price_s = get(i_price)
				try:
					price = float(str(price_s).replace(",", "")) if price_s else 0.0
				except Exception:
					price = 0.0
				account = get(i_account); bank = get(i_bank); holder = get(i_holder)
				items.append({"client": client, "product": product, "type": type_s, "price": price, "account": account, "bank": bank, "holder": holder})
		except Exception as e:
			return jsonify({"error": f"parse_failed: {e}"}), 400
		return jsonify({"items": items, "count": len(items)}), 200

	@app.route("/api/settlement/pricebook/template", methods=["GET"])  # ëŒ€ëŸ‰ë“±ë¡ XLSX í…œí”Œë¦¿ ë‹¤ìš´ë¡œë“œ
	def api_settlement_pricebook_template():
		from io import BytesIO
		from openpyxl import Workbook
		wb = Workbook()
		ws = wb.active
		ws.title = "pricebook"
		ws.append(["ê±°ë˜ì²˜", "ìƒí’ˆëª…", "ìœ í˜•", "ë‹¨ê°€", "ê³„ì¢Œ", "ì€í–‰", "ì˜ˆê¸ˆì£¼"])
		ws.append(["ì¼ë¥˜ê¸°íš", "í˜¸ì˜¬ìŠ¤", "ì €ì¥", 32, "123-45-67890", "êµ­ë¯¼", "ë¥˜ì¤€í˜¸"])  # ìƒ˜í”Œ
		buf = BytesIO()
		wb.save(buf)
		buf.seek(0)
		return app.response_class(buf.getvalue(), mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet", headers={"Content-Disposition": "attachment; filename=pricebook_template.xlsx"})

	@app.route("/api/settlement/inspect", methods=["GET"])  # ê²°ì¬ì„  ì‹œíŠ¸ í—¤ë” ì ê²€
	def api_settlement_inspect():
		try:
			ssid = os.getenv("SETTLEMENT_SPREADSHEET_ID", "").strip()
			info = inspect_sheets_by_id(ssid)
		except Exception as e:
			return jsonify({"error": str(e)}), 500
		return jsonify({"tabs": info}), 200

	@app.route("/api/settlement/compute", methods=["POST"])  # ê²°ì¬ì„  ì§‘ê³„ ê³„ì‚°
	def api_settlement_compute():
		try:
			payload = request.get_json(force=True, silent=False) or {}
		except Exception:
			return jsonify({"error": "invalid_json"}), 400
		ssid = os.getenv("SETTLEMENT_SPREADSHEET_ID", "").strip()
		selected_tabs = payload.get("tabs") or []
		if not isinstance(selected_tabs, list):
			return jsonify({"error": "invalid_tabs"}), 400
		# ë‹¨ê°€ ë¡œë“œ
		storage_path = os.getenv("PRICEBOOK_PATH", os.path.join(os.getcwd(), "pricebook.json"))
		try:
			if os.path.exists(storage_path):
				with open(storage_path, "r", encoding="utf-8") as f:
					pricebook = json.load(f)
			else:
				pricebook = []
		except Exception:
			pricebook = []
		try:
			result = compute_settlement_rows(ssid, selected_tabs, pricebook)
		except Exception as e:
			return jsonify({"error": str(e)}), 500
		return jsonify(result), 200

	@app.route("/api/settlement/extra", methods=["GET", "POST"])  # ìˆ˜ê¸° ì¶”ê°€ ì§€ì¶œ ì €ì¥ì†Œ
	def api_settlement_extra():
		storage_path = os.getenv("EXTRA_EXPENSES_PATH", os.path.join(os.getcwd(), "extra_expenses.json"))
		if request.method == "GET":
			try:
				if os.path.exists(storage_path):
					with open(storage_path, "r", encoding="utf-8") as f:
						data = json.load(f)
				else:
					data = []
			except Exception as e:
				return jsonify({"error": str(e)}), 500
			return jsonify({"items": data}), 200
		# POST ì €ì¥ (ì „ì²´ ì¹˜í™˜ ë°©ì‹)
		try:
			payload = request.get_json(force=True, silent=False) or {}
		except Exception:
			return jsonify({"error": "invalid_json"}), 400
		items = payload.get("items")
		if not isinstance(items, list):
			return jsonify({"error": "invalid_items"}), 400
		try:
			with open(storage_path, "w", encoding="utf-8") as f:
				json.dump(items, f, ensure_ascii=False, indent=2)
		except Exception as e:
			return jsonify({"error": str(e)}), 500
		return jsonify({"ok": True}), 200

	@app.route("/api/settlement/cream2-accounts", methods=["GET", "POST"])  # í¬ë¦¼2 ë°°í¬ ê³„ì • ê´€ë¦¬
	def api_settlement_cream2_accounts():
		# Render Disk ê²½ë¡œ ìš°ì„  ì‚¬ìš© (ì„œë²„ ì¬ì‹œì‘ ì‹œì—ë„ ë°ì´í„° ìœ ì§€)
		disk_path = "/var/data"
		default_path = os.path.join(disk_path, "cream2_accounts.json") if os.path.isdir(disk_path) else os.path.join(os.getcwd(), "cream2_accounts.json")
		storage_path = os.getenv("CREAM2_ACCOUNTS_PATH", default_path)
		if request.method == "GET":
			try:
				if os.path.exists(storage_path):
					with open(storage_path, "r", encoding="utf-8") as f:
						data = json.load(f)
				else:
					data = []
			except Exception as e:
				return jsonify({"error": str(e)}), 500
			return jsonify({"items": data}), 200
		# POST ì €ì¥ (ì „ì²´ ì¹˜í™˜ ë°©ì‹)
		try:
			payload = request.get_json(force=True, silent=False) or {}
		except Exception:
			return jsonify({"error": "invalid_json"}), 400
		items = payload.get("items")
		if not isinstance(items, list):
			return jsonify({"error": "invalid_items"}), 400
		try:
			with open(storage_path, "w", encoding="utf-8") as f:
				json.dump(items, f, ensure_ascii=False, indent=2)
		except Exception as e:
			return jsonify({"error": str(e)}), 500
		return jsonify({"ok": True}), 200

	@app.route("/api/agency-pricing", methods=["GET", "POST"])  # ëŒ€í–‰ì‚¬ íŒë§¤ ë‹¨ê°€ (ìš°ë¦¬ê°€ ë°›ëŠ” ê¸ˆì•¡)
	def api_agency_pricing():
		# Render Disk ê²½ë¡œ ìš°ì„  ì‚¬ìš©
		disk_path = "/var/data"
		default_path = os.path.join(disk_path, "agency_pricing.json") if os.path.isdir(disk_path) else os.path.join(os.getcwd(), "agency_pricing.json")
		storage_path = os.getenv("AGENCY_PRICING_PATH", default_path)
		if request.method == "GET":
			try:
				if os.path.exists(storage_path):
					with open(storage_path, "r", encoding="utf-8") as f:
						data = json.load(f)
				else:
					data = []
			except Exception as e:
				return jsonify({"error": str(e)}), 500
			return jsonify({"items": data}), 200
		# POST ì €ì¥
		try:
			payload = request.get_json(force=True, silent=False) or {}
		except Exception:
			return jsonify({"error": "invalid_json"}), 400
		items = payload.get("items")
		if not isinstance(items, list):
			return jsonify({"error": "invalid_items"}), 400
		try:
			with open(storage_path, "w", encoding="utf-8") as f:
				json.dump(items, f, ensure_ascii=False, indent=2)
		except Exception as e:
			return jsonify({"error": str(e)}), 500
		return jsonify({"ok": True}), 200

	@app.route("/debug/headers")
	def debug_headers():
		try:
			info = inspect_sheets()
		except Exception as e:
			return jsonify({"error": str(e)}), 500
		return jsonify(info)

	@app.route("/debug/matches")
	def debug_matches():
		days_param = request.args.get("days", "0").strip()
		selected_days = _parse_days(days_param)
		try:
			report = diagnose_matches(selected_days)
		except Exception as e:
			return jsonify({"error": str(e)}), 500
		return jsonify(report)

	@app.route("/api/fetch-stream")
	def fetch_stream():
		"""SSE: ì§„í–‰ë¥ ê³¼ ìµœì¢… ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•œë‹¤."""
		settings = load_settings()
		days_param = request.args.get("days", "").strip()
		filter_mode = request.args.get("filter_mode", "agency").strip().lower()
		selected_days = _parse_days(days_param)

		def event_stream():
			try:
				for evt in stream_grouped_messages_by_date(selected_days, settings, filter_mode):
					yield f"data: {json.dumps(evt, ensure_ascii=False)}\n\n"
			except Exception as e:
				payload = {"type": "error", "message": str(e)}
				yield f"data: {json.dumps(payload, ensure_ascii=False)}\n\n"

		return Response(stream_with_context(event_stream()), mimetype="text/event-stream")

	@app.route("/api/internal/items", methods=["GET"])  # ìºì‹œëœ ë‚´ë¶€ ì§„í–‰ê±´ ëª©ë¡ ë°˜í™˜
	def api_internal_items():
		try:
			cache = internal_load_cache()
		except Exception as e:
			return jsonify({"error": str(e)}), 500
		return jsonify({
			"updated_at": cache.get("updated_at"),
			"items": cache.get("items", []),
		}), 200

	@app.route("/api/internal/refresh", methods=["POST"])  # ìˆ˜ë™ ë¶ˆëŸ¬ì˜¤ê¸°
	def api_internal_refresh():
		try:
			data = internal_refresh_cache()
		except Exception as e:
			return jsonify({"error": str(e)}), 500
		return jsonify(data), 200
	
	@app.route("/api/workload/schedule", methods=["GET"])  # ì‘ì—…ëŸ‰ ìŠ¤ì¼€ì¤„ ì¡°íšŒ
	def api_workload_schedule():
		"""ìµœê·¼ 3ì£¼ê°„ ì‘ì—…ëŸ‰ ìŠ¤ì¼€ì¤„ ì¡°íšŒ (ìºì‹œ ìš°ì„ )"""
		company = request.args.get("company")
		business_name = request.args.get("business_name")  # ì—…ì²´ í•„í„° ì¶”ê°€
		
		try:
			schedule = fetch_workload_schedule(company, business_name)
			return jsonify(schedule), 200
		except Exception as e:
			logger.error(f"Workload schedule error: {e}")
			import traceback
			logger.error(traceback.format_exc())
			return jsonify({"error": str(e)}), 500
	
	@app.route("/api/workload/cache/refresh", methods=["POST"])  # ì‘ì—…ëŸ‰ ìºì‹œ ìˆ˜ë™ ê°±ì‹ 
	def api_workload_cache_refresh():
		"""ì‘ì—…ëŸ‰ ìºì‹œ ìˆ˜ë™ ê°±ì‹ """
		try:
			from workload_cache import refresh_all_workload_cache
			result = refresh_all_workload_cache()
			return jsonify(result), 200
		except Exception as e:
			logger.error(f"Workload cache refresh error: {e}")
			import traceback
			logger.error(traceback.format_exc())
			return jsonify({"error": str(e)}), 500
	
	@app.route("/api/workload/cache/status", methods=["GET"])  # ì‘ì—…ëŸ‰ ìºì‹œ ìƒíƒœ ì¡°íšŒ
	def api_workload_cache_status():
		"""ì‘ì—…ëŸ‰ ìºì‹œ ìƒíƒœ ì¡°íšŒ"""
		try:
			from workload_cache import WorkloadCache
			cache = WorkloadCache()
			status = cache.get_cache_status()
			return jsonify(status), 200
		except Exception as e:
			logger.error(f"Workload cache status error: {e}")
			return jsonify({"error": str(e)}), 500
	
	@app.route("/api/workload/businesses", methods=["GET"])  # ì—…ì²´ë³„ ì‘ì—…ëŸ‰ ì¼ê´„ ì¡°íšŒ
	def api_workload_businesses():
		"""íŠ¹ì • íšŒì‚¬ì˜ ëª¨ë“  ì—…ì²´ë³„ ì‘ì—…ëŸ‰ ë°ì´í„° ì¡°íšŒ (ìºì‹œ ìš°ì„ )"""
		company = request.args.get("company")
		
		if not company:
			return jsonify({"error": "company parameter required"}), 400
		
		try:
			from workload_cache import WorkloadCache
			cache = WorkloadCache()
			
			# ìºì‹œì—ì„œ ì—…ì²´ë³„ ë°ì´í„° ì¡°íšŒ
			businesses_data = cache.get_all_businesses_workload(company)
			
			if businesses_data:
				logger.info(f"Loaded {len(businesses_data)} businesses from cache for {company}")
				return jsonify({
					"businesses": businesses_data,
					"from_cache": True,
					"count": len(businesses_data)
				}), 200
			else:
				logger.warning(f"No cached data for {company}, cache may be expired")
				return jsonify({
					"businesses": {},
					"from_cache": False,
					"count": 0,
					"message": "ìºì‹œê°€ ì—†ê±°ë‚˜ ë§Œë£Œë˜ì—ˆìŠµë‹ˆë‹¤. 'ì‘ì—…ëŸ‰ ê°±ì‹ ' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”."
				}), 200
		except Exception as e:
			logger.error(f"Business workload error: {e}")
			import traceback
			logger.error(traceback.format_exc())
			return jsonify({"error": str(e)}), 500

	@app.route("/api/mark-done", methods=["POST"])
	def mark_done():
		"""íŠ¹ì • ëŒ€í–‰ì‚¬ ì¹´ë“œì˜ ëª¨ë“  í•´ë‹¹ í–‰ì„ 'ë§ˆê° ì•ˆë‚´ ì²´í¬'ë¡œ í‘œì‹œí•œë‹¤."""
		try:
			data = request.get_json(force=True, silent=False) or {}
		except Exception:
			return jsonify({"error": "invalid_json"}), 400

		agency_label = str(data.get("agency") or "").strip()
		days_param = str(data.get("days") or "").strip()
		filter_mode = str(data.get("filter_mode") or "agency").strip().lower()
		if not agency_label:
			return jsonify({"error": "missing_agency"}), 400

		selected_days = _parse_days(days_param)
		try:
			result = mark_checked_for_agency(selected_days=selected_days, agency_label=agency_label, filter_mode=filter_mode)
		except Exception as e:
			return jsonify({"error": str(e)}), 500
		return jsonify(result)

	@app.route("/api/mark-done-bulk", methods=["POST"])
	def mark_done_bulk():
		"""ì—¬ëŸ¬ ëŒ€í–‰ì‚¬ ì¹´ë“œë¥¼ í•œ ë²ˆì— ì²´í¬ ì²˜ë¦¬í•œë‹¤."""
		try:
			data = request.get_json(force=True, silent=False) or {}
		except Exception:
			return jsonify({"error": "invalid_json"}), 400

		agency_labels = data.get("agencies") or []
		if not isinstance(agency_labels, list):
			return jsonify({"error": "invalid_agencies"}), 400
		agency_labels = [str(a or "").strip() for a in agency_labels if str(a or "").strip()]
		if not agency_labels:
			return jsonify({"error": "empty_agencies"}), 400

		days_param = str(data.get("days") or "").strip()
		filter_mode = str(data.get("filter_mode") or "agency").strip().lower()
		selected_days = _parse_days(days_param)
		try:
			result = mark_checked_for_agencies(selected_days=selected_days, agency_labels=agency_labels, filter_mode=filter_mode)
		except Exception as e:
			return jsonify({"error": str(e)}), 500
		return jsonify(result)

	# --- ì›”ë³´ì¥ ê´€ë¦¬ API ---
	@app.route("/api/guarantee/items", methods=["GET", "POST"])
	def api_guarantee_items():
		"""ë³´ì¥ê±´ ëª©ë¡ ì¡°íšŒ ë° ìƒì„±"""
		gm = GuaranteeManager()
		
		if request.method == "GET":
			# í•„í„° íŒŒë¼ë¯¸í„°
			filters = {}
			if request.args.get("company"):
				filters["company"] = request.args.get("company")
			if request.args.get("status"):
				filters["status"] = request.args.get("status")
			if request.args.get("product"):
				filters["product"] = request.args.get("product")
			if request.args.get("active_only"):
				filters["active_only"] = True
			
			logger.info(f"Getting items with filters: {filters}")
			items = gm.get_items(filters)
			
			# ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìë™ ë™ê¸°í™” ì‹œë„ (ì„œë²„ ì¬ì‹œì‘ í›„ ì²« ìš”ì²­)
			if len(gm.get_items()) == 0:
				logger.info("ğŸ“¦ No local data found. Auto-syncing from Google Sheets...")
				try:
					sync_result = gm.sync_from_google_sheets()
					logger.info(f"âœ… Auto-sync completed: Added {sync_result.get('added', 0)} items")
					# ë‹¤ì‹œ ë°ì´í„° ì¡°íšŒ
					items = gm.get_items(filters)
				except Exception as sync_err:
					logger.error(f"âŒ Auto-sync failed: {sync_err}")
			
			logger.info(f"Found {len(items)} items")
			
			# ë””ë²„ê¹…: ì²˜ìŒ ëª‡ ê°œ ì•„ì´í…œ ë¡œê·¸
			if items:
				logger.info(f"Sample item: {items[0] if items else 'No items'}")
			
			return jsonify({"items": items, "count": len(items)}), 200
		
		# POST: ìƒˆ ë³´ì¥ê±´ ìƒì„±
		try:
			data = request.get_json(force=True)
		except Exception:
			return jsonify({"error": "invalid_json"}), 400
		
		if not data.get("business_name"):
			return jsonify({"error": "business_name_required"}), 400
		
		item = gm.create_item(data)
		return jsonify(item), 201

	@app.route("/api/guarantee/items/<item_id>", methods=["GET", "PUT", "DELETE"])
	def api_guarantee_item(item_id):
		"""íŠ¹ì • ë³´ì¥ê±´ ì¡°íšŒ/ìˆ˜ì •/ì‚­ì œ"""
		gm = GuaranteeManager()
		
		if request.method == "GET":
			item = gm.get_item(item_id)
			if not item:
				return jsonify({"error": "not_found"}), 404
			return jsonify(item), 200
		
		elif request.method == "PUT":
			try:
				data = request.get_json(force=True)
			except Exception:
				return jsonify({"error": "invalid_json"}), 400
			
			item = gm.update_item(item_id, data)
			if not item:
				return jsonify({"error": "not_found"}), 404
			return jsonify(item), 200
		
		elif request.method == "DELETE":
			if gm.delete_item(item_id):
				return jsonify({"ok": True}), 200
			return jsonify({"error": "not_found"}), 404

	@app.route("/api/guarantee/statistics", methods=["GET"])
	def api_guarantee_stats():
		"""í†µê³„ ì¡°íšŒ"""
		gm = GuaranteeManager()
		return jsonify(gm.get_statistics()), 200

	@app.route("/api/guarantee/search", methods=["GET"])
	def api_guarantee_search():
		"""ê²€ìƒ‰"""
		query = request.args.get("q", "").strip()
		if not query:
			return jsonify({"items": []}), 200
		
		gm = GuaranteeManager()
		items = gm.search(query)
		return jsonify({"items": items, "count": len(items)}), 200

	@app.route("/api/guarantee/daily-rank", methods=["POST"])
	def api_guarantee_daily_rank():
		"""ì¼ì°¨ë³„ ìˆœìœ„ ì—…ë°ì´íŠ¸"""
		try:
			data = request.get_json(force=True)
		except Exception:
			return jsonify({"error": "invalid_json"}), 400
		
		item_id = data.get("item_id")
		day = data.get("day")
		rank = data.get("rank")
		
		if not all([item_id, day is not None, rank is not None]):
			return jsonify({"error": "missing_params"}), 400
		
		gm = GuaranteeManager()
		item = gm.update_daily_rank(item_id, day, rank)
		if not item:
			return jsonify({"error": "not_found"}), 404
		return jsonify(item), 200

	@app.route("/api/guarantee/sync", methods=["POST"])
	def api_guarantee_sync():
		"""ìˆ˜ë™ ë™ê¸°í™” - ë‹¨ê³„ë³„ ì§„í–‰ (UIì—ì„œ ì§„í–‰ ìƒíƒœ í‘œì‹œìš©)
		
		í”Œë¡œìš°:
		1. ì œì´íˆ¬ë© ì—…ì²´ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
		2. ì¼ë¥˜ê¸°íš ì—…ì²´ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
		3. ì˜¤ëŠ˜ ìˆœìœ„ ë°ì´í„° í™•ì¸ (rank_snapshots)
		4. ì‹œê°„ ì²´í¬ í›„ í¬ë¡¤ë§ (00:00~15:10 ì‚¬ì´ë©´ ìŠ¤í‚µ)
		5. ì‹œíŠ¸ì— ìˆœìœ„ ê¸°ì…
		6. ì‘ì—…ëŸ‰ ë°ì´í„° ê°±ì‹ 
		"""
		import pytz
		from datetime import datetime
		
		kst = pytz.timezone('Asia/Seoul')
		now = datetime.now(kst)
		current_hour = now.hour
		current_minute = now.minute
		today_str = now.strftime("%Y-%m-%d")
		
		# ê²°ê³¼ ì €ì¥ìš©
		steps = {}
		
		try:
			# ============ STEP 1 & 2: ì—…ì²´ ì •ë³´ ê°€ì ¸ì˜¤ê¸° ============
			gm = GuaranteeManager()
			logger.info("ğŸ“¡ Starting sync: fetching company data...")
			
			sync_result = gm.sync_from_google_sheets()
			
			# íšŒì‚¬ë³„ ì¹´ìš´íŠ¸
			jtwolab_items = len(gm.get_items({"company": "ì œì´íˆ¬ë©"}))
			ilryu_items = len(gm.get_items({"company": "ì¼ë¥˜ê¸°íš"}))
			total_items = jtwolab_items + ilryu_items
			
			steps["jtwolab_sync"] = {
				"status": "success",
				"count": jtwolab_items,
				"message": f"ì œì´íˆ¬ë© {jtwolab_items}ê±´"
			}
			steps["ilryu_sync"] = {
				"status": "success", 
				"count": ilryu_items,
				"message": f"ì¼ë¥˜ê¸°íš {ilryu_items}ê±´"
			}
			
			logger.info(f"âœ… Company data fetched: ì œì´íˆ¬ë© {jtwolab_items}, ì¼ë¥˜ê¸°íš {ilryu_items}")
			
			# ============ STEP 3: ì˜¤ëŠ˜ ìˆœìœ„ ë°ì´í„° í™•ì¸ ============
			has_today_rank = False
			try:
				from rank_snapshot_manager import RankSnapshotManager
				rsm = RankSnapshotManager()
				today_snapshots = rsm.get_history(date_from=today_str, date_to=today_str, days=1)
				has_today_rank = bool(today_snapshots and len(today_snapshots) > 0)
				
				steps["rank_check"] = {
					"status": "success",
					"has_data": has_today_rank,
					"count": len(today_snapshots) if today_snapshots else 0,
					"message": f"ì˜¤ëŠ˜ ìˆœìœ„ {'ìˆìŒ' if has_today_rank else 'ì—†ìŒ'}"
				}
				logger.info(f"ğŸ” Rank check: {'ë°ì´í„° ìˆìŒ' if has_today_rank else 'ë°ì´í„° ì—†ìŒ'} ({len(today_snapshots) if today_snapshots else 0}ê±´)")
			except Exception as e:
				steps["rank_check"] = {"status": "error", "message": str(e)}
				logger.warning(f"Rank check failed: {e}")
			
			# ============ STEP 4: ì‹œê°„ ì²´í¬ í›„ í¬ë¡¤ë§ ============
			# 00:00~15:09 ì‚¬ì´ë©´ í¬ë¡¤ë§ ìŠ¤í‚µ
			is_crawl_time_window = (current_hour >= 0 and current_hour < 15) or (current_hour == 15 and current_minute < 10)
			
			rank_crawled = False
			sheet_updated = False
			
			if has_today_rank:
				# ì´ë¯¸ ì˜¤ëŠ˜ ë°ì´í„°ê°€ ìˆìœ¼ë©´ í¬ë¡¤ë§ ìŠ¤í‚µ
				steps["rank_crawl"] = {
					"status": "skipped",
					"reason": "already_exists",
					"message": "ì˜¤ëŠ˜ ë°ì´í„° ì´ë¯¸ ì¡´ì¬"
				}
				logger.info("ğŸ”„ Today's rank data exists, skipping crawl but will try sheet update")
				
				# ì‹œíŠ¸ ì—…ë°ì´íŠ¸ëŠ” ì‹œë„ (ì‹œíŠ¸ì— ì•„ì§ ì•ˆ ê¸°ì…ëì„ ìˆ˜ ìˆìŒ)
				try:
					from rank_update_service import update_guarantee_sheets_from_snapshots
					from scheduler_logs import log_scheduler_event
					log_scheduler_event("guarantee_update", "ë³´ì¥ê±´ ì‹œíŠ¸ ì—…ë°ì´íŠ¸", "started", "ê¸°ì¡´ ë°ì´í„°ë¡œ ì‹œíŠ¸ ì—…ë°ì´íŠ¸")
					logger.info("ğŸ“ Updating guarantee sheets with existing data...")
					
					update_result = update_guarantee_sheets_from_snapshots()
					total_updated = update_result.get('total_updated', 0)
					
					steps["sheet_update"] = {
						"status": "success",
						"count": total_updated,
						"message": f"{total_updated}ê±´ ì‹œíŠ¸ ê¸°ì…"
					}
					sheet_updated = True
					log_scheduler_event("guarantee_update", "ë³´ì¥ê±´ ì‹œíŠ¸ ì—…ë°ì´íŠ¸", "success", f"{total_updated}ê±´ ì—…ë°ì´íŠ¸")
					logger.info(f"âœ… Sheet update completed: {total_updated}ê±´")
				except Exception as ue:
					steps["sheet_update"] = {"status": "error", "message": str(ue)}
					log_scheduler_event("guarantee_update", "ë³´ì¥ê±´ ì‹œíŠ¸ ì—…ë°ì´íŠ¸", "failed", str(ue))
					logger.error(f"âŒ Sheet update failed: {ue}")
					
			elif is_crawl_time_window:
				# 00:00~15:09 ì‚¬ì´ë©´ ìŠ¤í‚µ
				steps["rank_crawl"] = {
					"status": "skipped",
					"reason": "time_window",
					"message": f"00:00~15:10 ì‚¬ì´ ({now.strftime('%H:%M')})"
				}
				steps["sheet_update"] = {
					"status": "skipped",
					"reason": "time_window",
					"message": "í¬ë¡¤ë§ ìŠ¤í‚µë¨"
				}
				logger.info(f"â° Crawl skipped: time window (current: {now.strftime('%H:%M')})")
			else:
				# í¬ë¡¤ë§ ì‹¤í–‰
				try:
					from rank_crawler import crawl_ranks_for_company
					from scheduler_logs import log_scheduler_event
					
					log_scheduler_event("rank_crawl", "ìˆœìœ„ í¬ë¡¤ë§ (ë™ê¸°í™”)", "started", "ë™ê¸°í™” ë²„íŠ¼ìœ¼ë¡œ ì‹¤í–‰")
					logger.info("ğŸ† Starting rank crawl...")
					
					crawl_result = crawl_ranks_for_company(None)
					crawled_count = crawl_result.get('crawled_count', 0)
					
					steps["rank_crawl"] = {
						"status": "success",
						"count": crawled_count,
						"message": f"{crawled_count}ê±´ í¬ë¡¤ë§ ì™„ë£Œ"
					}
					rank_crawled = True
					log_scheduler_event("rank_crawl", "ìˆœìœ„ í¬ë¡¤ë§ (ë™ê¸°í™”)", "success", f"{crawled_count}ê±´ ì™„ë£Œ")
					logger.info(f"âœ… Rank crawl completed: {crawled_count}ê±´")
					
					# ============ STEP 5: ì‹œíŠ¸ì— ìˆœìœ„ ê¸°ì… ============
					try:
						from rank_update_service import update_guarantee_sheets_from_snapshots
						log_scheduler_event("guarantee_update", "ë³´ì¥ê±´ ì‹œíŠ¸ ì—…ë°ì´íŠ¸", "started", "ë™ê¸°í™” í›„ ì‹¤í–‰")
						logger.info("ğŸ“ Updating guarantee sheets...")
						
						update_result = update_guarantee_sheets_from_snapshots()
						total_updated = update_result.get('total_updated', 0)
						
						steps["sheet_update"] = {
							"status": "success",
							"count": total_updated,
							"message": f"{total_updated}ê±´ ì‹œíŠ¸ ê¸°ì…"
						}
						sheet_updated = True
						log_scheduler_event("guarantee_update", "ë³´ì¥ê±´ ì‹œíŠ¸ ì—…ë°ì´íŠ¸", "success", f"{total_updated}ê±´ ì—…ë°ì´íŠ¸")
						logger.info(f"âœ… Sheet update completed: {total_updated}ê±´")
					except Exception as ue:
						steps["sheet_update"] = {"status": "error", "message": str(ue)}
						log_scheduler_event("guarantee_update", "ë³´ì¥ê±´ ì‹œíŠ¸ ì—…ë°ì´íŠ¸", "failed", str(ue))
						logger.error(f"âŒ Sheet update failed: {ue}")
						
				except Exception as ce:
					steps["rank_crawl"] = {"status": "error", "message": str(ce)}
					steps["sheet_update"] = {"status": "skipped", "reason": "crawl_failed"}
					from scheduler_logs import log_scheduler_event
					log_scheduler_event("rank_crawl", "ìˆœìœ„ í¬ë¡¤ë§ (ë™ê¸°í™”)", "failed", str(ce))
					logger.error(f"âŒ Rank crawl failed: {ce}")
			
			# ============ STEP 6: ì‘ì—…ëŸ‰ ë°ì´í„° ê°±ì‹  ============
			workload_refreshed = False
			try:
				from workload_cache import WorkloadCache, refresh_all_workload_cache
				wc = WorkloadCache()
				
				if not wc.is_cache_valid():
					logger.info("âš¡ Refreshing workload cache...")
					wresult = refresh_all_workload_cache()
					workload_refreshed = True
					steps["workload_refresh"] = {
						"status": "success",
						"message": "ì‘ì—…ëŸ‰ ìºì‹œ ê°±ì‹  ì™„ë£Œ"
					}
					logger.info(f"âœ… Workload cache refreshed")
				else:
					steps["workload_refresh"] = {
						"status": "skipped",
						"reason": "cache_valid",
						"message": "ìºì‹œ ìœ íš¨ (ê°±ì‹  ë¶ˆí•„ìš”)"
					}
			except Exception as we:
				steps["workload_refresh"] = {"status": "error", "message": str(we)}
				logger.warning(f"Workload refresh failed: {we}")
			
			# ============ ìµœì¢… ê²°ê³¼ ============
			last_sync = gm.get_last_sync_time()
			
			# ë©”ì‹œì§€ ìƒì„±
			message_parts = [f"ë™ê¸°í™” ì™„ë£Œ (ì´ {total_items}ê±´)"]
			if rank_crawled:
				message_parts.append("ìˆœìœ„ í¬ë¡¤ë§ë¨")
			if sheet_updated:
				message_parts.append("ì‹œíŠ¸ ê¸°ì…ë¨")
			if workload_refreshed:
				message_parts.append("ì‘ì—…ëŸ‰ ê°±ì‹ ë¨")
			
			return jsonify({
				"ok": True,
				"steps": steps,
				"result": sync_result,
				"last_sync": last_sync,
				"total_items": total_items,
				"jtwolab_count": jtwolab_items,
				"ilryu_count": ilryu_items,
				"rank_crawled": rank_crawled,
				"sheet_updated": sheet_updated,
				"workload_refreshed": workload_refreshed,
				"message": " Â· ".join(message_parts)
			}), 200
			
		except Exception as e:
			logger.error(f"Manual sync failed: {str(e)}")
			import traceback
			logger.error(f"Traceback: {traceback.format_exc()}")
			return jsonify({
				"ok": False,
				"error": str(e),
				"steps": steps,
				"detail": "ì„œë²„ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”"
			}), 500


	@app.route("/api/guarantee/sync-status", methods=["GET"])
	def api_guarantee_sync_status():
		"""ë™ê¸°í™” ìƒíƒœ í™•ì¸"""
		gm = GuaranteeManager()
		last_sync = gm.get_last_sync_time()
		
		# ë‹¤ìŒ ë™ê¸°í™” ì‹œê°„ ê³„ì‚°
		now = datetime.now(pytz.timezone('Asia/Seoul'))
		current_hour = now.hour
		
		if current_hour < 9:
			next_sync = now.replace(hour=9, minute=0, second=0, microsecond=0)
		elif current_hour < 16:
			next_sync = now.replace(hour=16, minute=0, second=0, microsecond=0)
		else:
			# ë‹¤ìŒë‚  9ì‹œ
			next_sync = (now + timedelta(days=1)).replace(hour=9, minute=0, second=0, microsecond=0)
		
		return jsonify({
			"last_sync": last_sync,
			"next_sync": next_sync.isoformat(),
			"next_sync_kst": next_sync.strftime("%Y-%m-%d %H:%M KST")
		}), 200

	@app.route("/api/guarantee/exposure-status", methods=["GET"])
	def api_exposure_status():
		"""ì‹¤ì‹œê°„ ë…¸ì¶œ í˜„í™© ì¡°íšŒ"""
		company = request.args.get("company")  # ì œì´íˆ¬ë©, ì¼ë¥˜ê¸°íš
		
		try:
			gm = GuaranteeManager()
			status = gm.get_exposure_status(company)
			
			# í¬ë¡¤ë§ ìˆœìœ„ ë°ì´í„° ë³‘í•©
			try:
				from rank_crawler import get_latest_ranks
				latest_ranks = get_latest_ranks(company)
				
				# ìƒí˜¸ëª… -> ìˆœìœ„ ë§¤í•‘
				rank_map = {}
				for rank_data in latest_ranks:
					rank_map[rank_data["business_name"]] = {
						"rank": rank_data["rank"],
						"keyword": rank_data["keyword"],
						"checked_at": rank_data["checked_at"]
					}
				
				# exposure_detailsì— í¬ë¡¤ë§ ìˆœìœ„ ì¶”ê°€
				for detail in status.get("exposure_details", []):
					biz_name = detail.get("business_name")
					if biz_name in rank_map:
						detail["crawled_rank"] = rank_map[biz_name]["rank"]
						detail["crawled_at"] = rank_map[biz_name]["checked_at"]
			except Exception as e:
				logger.warning(f"Failed to merge crawled ranks: {e}")
			
			return jsonify(status), 200
		except Exception as e:
			logger.error(f"Exposure status error: {e}")
			return jsonify({"error": str(e)}), 500
	
	@app.route("/api/guarantee/deadline-status", methods=["GET"])
	def api_deadline_status():
		"""ë§ˆê° ì„ë°• í˜„í™© ì¡°íšŒ"""
		company = request.args.get("company")  # ì œì´íˆ¬ë©, ì¼ë¥˜ê¸°íš
		
		try:
			gm = GuaranteeManager()
			status = gm.get_deadline_status(company)
			return jsonify(status), 200
		except Exception as e:
			logger.error(f"Deadline status error: {e}")
			return jsonify({"error": str(e)}), 500
	
	@app.route("/api/recipe-analysis", methods=["GET"])
	def api_recipe_analysis():
		"""ìµœì  ë ˆì‹œí”¼ ë¶„ì„ ê²°ê³¼ ì¡°íšŒ"""
		weeks = request.args.get("weeks", 3, type=int)
		weeks = min(max(weeks, 1), 3)  # 1~3ì£¼ ì œí•œ
		
		try:
			from recipe_analyzer import get_analyzer
			analyzer = get_analyzer()
			result = analyzer.analyze_all(weeks=weeks)
			return jsonify(result), 200
		except Exception as e:
			logger.error(f"Recipe analysis error: {e}")
			import traceback
			logger.error(traceback.format_exc())
			return jsonify({"error": str(e)}), 500
	
	@app.route("/api/business-dashboard/<business_name>", methods=["GET"])
	def api_business_dashboard(business_name):
		"""ì—…ì²´ë³„ ëŒ€ì‹œë³´ë“œ ë°ì´í„°"""
		try:
			from recipe_analyzer import get_analyzer
			analyzer = get_analyzer()
			result = analyzer.get_business_dashboard(business_name)
			
			if result:
				return jsonify(result), 200
			else:
				return jsonify({"error": "Business not found"}), 404
		except Exception as e:
			logger.error(f"Business dashboard error: {e}")
			return jsonify({"error": str(e)}), 500
	
	@app.route("/api/guarantee/security-status", methods=["GET"])
	def api_security_status():
		"""ë°ì´í„° ë³´ì•ˆ ìƒíƒœ í™•ì¸ (ê´€ë¦¬ììš©)"""
		status = {
			"encryption_enabled": SECURITY_AVAILABLE,
			"data_info": {}
		}
		
		if SECURITY_AVAILABLE:
			try:
				security = DataSecurity()
				status["data_info"] = security.get_data_info()
				status["encryption_key_source"] = "environment" if os.getenv("DATA_ENCRYPTION_KEY") else "file"
			except Exception as e:
				status["error"] = str(e)
		else:
			status["warning"] = "Encryption module not available"
		
		# ê¸°ë³¸ ë°ì´í„° íŒŒì¼ ì¡´ì¬ ì—¬ë¶€
		status["plain_files"] = {
			"guarantee_data.json": os.path.exists("guarantee_data.json"),
			"pricebook.json": os.path.exists("pricebook.json"),
			"internal_cache.json": os.path.exists("internal_cache.json")
		}
		
		return jsonify(status), 200
	
	@app.route("/api/guarantee/export", methods=["GET"])
	def api_guarantee_export():
		"""ë³´ì¥ê±´ ë°ì´í„° ë‚´ë³´ë‚´ê¸° (JSON)"""
		try:
			gm = GuaranteeManager()
			items = gm.get_items()
			
			# ë¯¼ê° ì •ë³´ ì œê±° ì˜µì…˜
			remove_sensitive = request.args.get("remove_sensitive", "false").lower() == "true"
			if remove_sensitive:
				for item in items:
					item.pop("place_account", None)
					item.pop("url", None)
			
			return jsonify({
				"exported_at": datetime.now().isoformat(),
				"count": len(items),
				"items": items
			}), 200
		except Exception as e:
			return jsonify({"error": str(e)}), 500
	
	@app.route("/api/guarantee/crawl-ranks", methods=["POST"])
	def api_crawl_ranks():
		"""ì• ë“œë¡œê·¸ì—ì„œ ìˆœìœ„ í¬ë¡¤ë§ ì‹¤í–‰ (N2 í¬í•¨, Google Sheets ì €ì¥)"""
		company = request.args.get("company")  # ì œì´íˆ¬ë©, ì¼ë¥˜ê¸°íš (Noneì´ë©´ ì „ì²´)
		
		try:
			from rank_crawler import crawl_ranks_for_company
			result = crawl_ranks_for_company(company)
			
			if result.get("success"):
				# ë³´ì¥ê±´ ì‹œíŠ¸ ìë™ ì—…ë°ì´íŠ¸ (ì„±ê³µ ì‹œ)
				try:
					from rank_update_service import update_guarantee_sheets_from_snapshots
					update_result = update_guarantee_sheets_from_snapshots()
					result["sheet_update"] = update_result
					logger.info(f"âœ… Guarantee sheets updated manually: {update_result}")
				except Exception as update_error:
					logger.error(f"âŒ Guarantee sheet update failed: {update_error}")
					result["sheet_update_error"] = str(update_error)
					
				return jsonify(result), 200
			else:
				return jsonify(result), 500
		except ValueError as ve:
			# í™˜ê²½ë³€ìˆ˜ ì˜¤ë¥˜ (ADLOG_ID/PASSWORD ë¯¸ì„¤ì •)
			logger.error(f"Configuration error: {ve}")
			return jsonify({"success": False, "error": str(ve), "message": "í™˜ê²½ë³€ìˆ˜ ì˜¤ë¥˜"}), 500
		except Exception as e:
			logger.error(f"Rank crawling error: {e}")
			import traceback
			logger.error(traceback.format_exc())
			return jsonify({"success": False, "error": str(e), "message": "í¬ë¡¤ë§ ì‹¤íŒ¨"}), 500
	
	@app.route("/api/cron/crawl-ranks", methods=["POST"])
	def api_cron_crawl_ranks():
		"""ì™¸ë¶€ cron ì„œë¹„ìŠ¤ìš© ìˆœìœ„ í¬ë¡¤ë§ endpoint (token ì¸ì¦)
		
		Render ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€ë¥¼ ìœ„í•´ ì™¸ë¶€ cron ì„œë¹„ìŠ¤(cron-job.org ë“±)ë¥¼ ì‚¬ìš©í•  ë•Œ:
		- CRON_TOKEN í™˜ê²½ë³€ìˆ˜ ì„¤ì • í•„ìˆ˜
		- Header: Authorization: Bearer <token> ë˜ëŠ” Query: ?token=<token>
		"""
		# í† í° ì¶”ì¶œ
		auth_header = request.headers.get("Authorization", "")
		if auth_header.startswith("Bearer "):
			token = auth_header[7:]
		else:
			token = request.args.get("token") or request.form.get("token")
		
		# í† í° ê²€ì¦
		expected_token = os.getenv("CRON_TOKEN")
		if not expected_token:
			logger.warning("CRON_TOKEN not configured")
			return jsonify({"success": False, "message": "CRON_TOKEN ë¯¸ì„¤ì •"}), 500
		
		if not token or token != expected_token:
			logger.warning(f"Invalid cron token attempt")
			return jsonify({"success": False, "message": "Invalid token"}), 401
		
		# í¬ë¡¤ë§ ì‹¤í–‰
		try:
			from rank_crawler import crawl_ranks_for_company
			result = crawl_ranks_for_company(None)  # ì „ì²´ íšŒì‚¬
			
			if result.get("success"):
				logger.info(f"âœ… Cron crawl completed: {result.get('message')}")
				return jsonify(result), 200
			else:
				return jsonify(result), 500
		except Exception as e:
			logger.error(f"Cron crawl error: {e}")
			return jsonify({"success": False, "error": str(e)}), 500
	
	@app.route("/api/guarantee/latest-ranks", methods=["GET"])
	def api_latest_ranks():
		"""ìµœì‹  ìˆœìœ„ ë°ì´í„° ì¡°íšŒ"""
		company = request.args.get("company")
		
		try:
			from rank_crawler import get_latest_ranks
			ranks = get_latest_ranks(company)
			return jsonify({"ranks": ranks, "count": len(ranks)}), 200
		except Exception as e:
			logger.error(f"Latest ranks error: {e}")
			return jsonify({"error": str(e)}), 500
	
	@app.route("/api/guarantee/rank-history/<business_name>", methods=["GET"])
	def api_rank_history(business_name):
		"""ì—…ì²´ë³„ ìˆœìœ„ íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
		limit = int(request.args.get("limit", 30))
		
		try:
			from rank_crawler import get_rank_history
			history = get_rank_history(business_name, limit)
			return jsonify({"history": history, "count": len(history)}), 200
		except Exception as e:
			logger.error(f"Rank history error: {e}")
			return jsonify({"error": str(e)}), 500
	
	@app.route("/api/guarantee/ranks/export", methods=["GET"])
	def api_ranks_export():
		"""ìˆœìœ„ ë°ì´í„° JSONìœ¼ë¡œ ë‚´ë³´ë‚´ê¸° (ë°±ì—…ìš©)"""
		try:
			from db_backup import export_rank_history_to_json
			data = export_rank_history_to_json()
			return jsonify(data), 200
		except Exception as e:
			logger.error(f"Rank export error: {e}")
			return jsonify({"error": str(e)}), 500
	
	@app.route("/api/guarantee/ranks/import", methods=["POST"])
	def api_ranks_import():
		"""ìˆœìœ„ ë°ì´í„° JSONì—ì„œ ê°€ì ¸ì˜¤ê¸° (ë³µì›ìš©)"""
		try:
			data = request.get_json(force=True)
			from db_backup import import_rank_history_from_json
			success = import_rank_history_from_json(data)
			
			if success:
				return jsonify({"ok": True, "message": "ìˆœìœ„ ë°ì´í„° ë³µì› ì™„ë£Œ"}), 200
			else:
				return jsonify({"error": "ë³µì› ì‹¤íŒ¨"}), 500
		except Exception as e:
			logger.error(f"Rank import error: {e}")
			return jsonify({"error": str(e)}), 500

	# --- Worklog Cache API ---
	@app.route("/api/worklog/cache/refresh", methods=["POST"])
	@login_required
	def api_worklog_cache_refresh():
		"""Worklog ìºì‹œ ê°±ì‹ """
		try:
			from worklog_cache import refresh_worklog_cache
			result = refresh_worklog_cache()
			return jsonify(result), 200 if result.get("success") else 500
		except Exception as e:
			logger.error(f"Worklog cache refresh error: {e}")
			return jsonify({"error": str(e)}), 500
	
	@app.route("/api/worklog/cache/status", methods=["GET"])
	@login_required
	def api_worklog_cache_status():
		"""Worklog ìºì‹œ ìƒíƒœ ì¡°íšŒ"""
		try:
			from worklog_cache import get_worklog_cache_status
			status = get_worklog_cache_status()
			return jsonify(status), 200
		except Exception as e:
			logger.error(f"Worklog cache status error: {e}")
			return jsonify({"error": str(e)}), 500
	
	# --- Training Dataset API ---
	@app.route("/api/training/build", methods=["POST"])
	@login_required
	def api_training_build():
		"""í•™ìŠµ ë°ì´í„°ì…‹ ë¹Œë“œ"""
		weeks = request.args.get("weeks", 3, type=int)
		weeks = min(max(weeks, 1), 8)  # 1~8ì£¼ ì œí•œ
		
		try:
			from training_dataset_builder import build_and_save
			result = build_and_save(weeks=weeks)
			return jsonify(result), 200 if result.get("success") else 500
		except Exception as e:
			logger.error(f"Training build error: {e}")
			import traceback
			logger.error(traceback.format_exc())
			return jsonify({"error": str(e)}), 500
	
	@app.route("/api/recipe/top", methods=["GET"])
	@login_required
	def api_recipe_top():
		"""ìƒìœ„ ë ˆì‹œí”¼ ì¡°íšŒ"""
		weeks = request.args.get("weeks", 3, type=int)
		weeks = min(max(weeks, 1), 8)
		
		try:
			from training_dataset_builder import get_top_recipes
			recipes = get_top_recipes(weeks=weeks)
			return jsonify({"recipes": recipes, "count": len(recipes)}), 200
		except Exception as e:
			logger.error(f"Recipe top error: {e}")
			return jsonify({"error": str(e)}), 500

	# --- Scheduler Logs API ---
	@app.route("/api/scheduler/logs", methods=["GET"])
	@login_required
	def api_scheduler_logs():
		"""ìŠ¤ì¼€ì¤„ëŸ¬ ë¡œê·¸ ì¡°íšŒ"""
		job_id = request.args.get("job_id")
		status = request.args.get("status")
		limit = request.args.get("limit", 50, type=int)
		limit = min(max(limit, 1), 100)
		
		try:
			from scheduler_logs import get_scheduler_logs
			logs = get_scheduler_logs(job_id=job_id, status=status, limit=limit)
			return jsonify({"logs": logs, "count": len(logs)}), 200
		except Exception as e:
			logger.error(f"Scheduler logs error: {e}")
			return jsonify({"error": str(e)}), 500
	
	@app.route("/api/scheduler/summary", methods=["GET"])
	@login_required
	def api_scheduler_summary():
		"""ìŠ¤ì¼€ì¤„ëŸ¬ ìš”ì•½ ì¡°íšŒ"""
		try:
			from scheduler_logs import get_scheduler_summary
			summary = get_scheduler_summary()
			return jsonify(summary), 200
		except Exception as e:
			logger.error(f"Scheduler summary error: {e}")
			return jsonify({"error": str(e)}), 500

	# Shutdown scheduler when app closes
	import atexit
	atexit.register(lambda: scheduler.shutdown())

	return app

app = create_app()


def _parse_days(days_param: str) -> List[int]:
	if not days_param:
		return []
	parts = [p.strip() for p in days_param.split(",") if p.strip()]
	selected: List[int] = []
	for p in parts:
		try:
			selected.append(int(p))
		except ValueError:
			continue
	return selected


if __name__ == "__main__":
	parser = argparse.ArgumentParser()
	parser.add_argument("--prod", action="store_true", help="Run with waitress server")
	args = parser.parse_args()

	host = os.getenv("FLASK_HOST", "0.0.0.0")
	port = int(os.getenv("FLASK_PORT", "8080"))
	debug = os.getenv("FLASK_DEBUG", "false").lower() == "true"

	if args.prod:
		from waitress import serve

		serve(app, host=host, port=port)
	else:
		app.run(host=host, port=port, debug=debug)
