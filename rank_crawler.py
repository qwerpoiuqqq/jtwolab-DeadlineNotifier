"""
ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ìˆœìœ„ í¬ë¡¤ë§ ëª¨ë“ˆ
ì• ë“œë¡œê·¸ ì‚¬ì´íŠ¸ì—ì„œ ìˆœìœ„/ì €ì¥/ë¦¬ë·°/N2 ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ Google Sheets ì›ì¥ì— ì €ì¥

í•µì‹¬ ì„¤ê³„:
1. row_textì— ì—¬ëŸ¬ ë‚ ì§œ ë°ì´í„°ê°€ ìˆìœ¼ë¯€ë¡œ 'ìµœì‹  ë‚ ì§œ ë¸”ë¡'ë§Œ íŒŒì‹±
2. dateëŠ” í™”ë©´ì—ì„œ íŒŒì‹±í•œ ë‚ ì§œ, collected_atëŠ” ì‹¤ì œ ìˆ˜ì§‘ ì‹œê°
3. í¬ë¡¤ë§ ì „ 'ë¦¬ë·°ìˆ˜ë³´ê¸°/ì—…ì²´ì ìˆ˜ë³´ê¸°' ì²´í¬ë°•ìŠ¤ ìƒíƒœ í™•ì¸
4. ADLOG_ID/PASSWORD ê¸°ë³¸ê°’ ì œê±°, env ì—†ìœ¼ë©´ ì—ëŸ¬
5. Render ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€: cron token endpoint ë°©ì‹ ê¶Œì¥
"""
import os
import re
import sqlite3
import logging
import time
from datetime import datetime
from typing import List, Dict, Optional, Tuple
import pytz
from playwright.sync_api import sync_playwright, Page, Browser

logger = logging.getLogger(__name__)

# ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • (ë ˆê±°ì‹œ í˜¸í™˜ìš©)
DB_PATH = os.getenv("RANK_DB_PATH", "rank_history.db")

# í•œêµ­ ì‹œê°„ëŒ€
KST = pytz.timezone('Asia/Seoul')


# =============================================================================
# í™˜ê²½ë³€ìˆ˜ í•„ìˆ˜ ì²´í¬
# =============================================================================

def get_adlog_credentials() -> Tuple[str, str]:
    """ì• ë“œë¡œê·¸ ë¡œê·¸ì¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ê¸°ë³¸ê°’ ì—†ìŒ, env í•„ìˆ˜)
    
    Returns:
        (adlog_id, adlog_password)
        
    Raises:
        ValueError: í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš°
    """
    adlog_id = os.getenv("ADLOG_ID")
    adlog_password = os.getenv("ADLOG_PASSWORD")
    
    if not adlog_id:
        raise ValueError("ADLOG_ID í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
    if not adlog_password:
        raise ValueError("ADLOG_PASSWORD í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
    
    return adlog_id, adlog_password


# =============================================================================
# ë‚ ì§œ ë¸”ë¡ íŒŒì‹± ìœ í‹¸ë¦¬í‹°
# =============================================================================

def parse_date_marker(text: str) -> Optional[str]:
    """ë‚ ì§œ ë§ˆì»¤ íŒŒì‹± (ì˜ˆ: '12-28(í† )' -> '2025-12-28')
    
    Args:
        text: ë‚ ì§œ ë§ˆì»¤ í…ìŠ¤íŠ¸
        
    Returns:
        YYYY-MM-DD í˜•ì‹ ë‚ ì§œ ë˜ëŠ” None
    """
    # íŒ¨í„´: MM-DD(ìš”ì¼) ë˜ëŠ” MM-DD ë˜ëŠ” M-DD
    match = re.search(r'(\d{1,2})-(\d{1,2})(?:\([^)]+\))?', text)
    if match:
        month = int(match.group(1))
        day = int(match.group(2))
        
        # ìœ íš¨ì„± ê²€ì‚¬: ì›”ì€ 1-12, ì¼ì€ 1-31
        if not (1 <= month <= 12 and 1 <= day <= 31):
            return None
        
        # í˜„ì¬ ì—°ë„ ê¸°ì¤€ (12ì›” ë°ì´í„°ê°€ 1ì›”ì— ì¡°íšŒë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì²˜ë¦¬)
        now = datetime.now(KST)
        year = now.year
        
        # í˜„ì¬ 1-2ì›”ì¸ë° íŒŒì‹±í•œ ì›”ì´ 11-12ì›”ì´ë©´ ì‘ë…„
        if now.month <= 2 and month >= 11:
            year -= 1
        # í˜„ì¬ 11-12ì›”ì¸ë° íŒŒì‹±í•œ ì›”ì´ 1-2ì›”ì´ë©´ ë‚´ë…„
        elif now.month >= 11 and month <= 2:
            year += 1
        
        return f"{year}-{month:02d}-{day:02d}"
    
    return None


def find_date_blocks(row_text: str) -> List[Tuple[str, int, int]]:
    """í–‰ í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ ë¸”ë¡ ìœ„ì¹˜ ì°¾ê¸°
    
    Args:
        row_text: ì „ì²´ í–‰ í…ìŠ¤íŠ¸
        
    Returns:
        [(ë‚ ì§œ, ì‹œì‘ìœ„ì¹˜, ëìœ„ì¹˜), ...] - ìµœì‹  ë‚ ì§œìˆœ
    """
    # ë‚ ì§œ ë§ˆì»¤ íŒ¨í„´: MM-DD(ìš”ì¼) í˜•ì‹
    pattern = r'(\d{1,2}-\d{1,2})\s*(?:\([^)]+\))?'
    
    blocks = []
    for match in re.finditer(pattern, row_text):
        date_str = parse_date_marker(match.group(0))
        if date_str:
            blocks.append((date_str, match.start(), match.end()))
    
    # ê°€ì¥ ìµœì‹  ë‚ ì§œê°€ ë¨¼ì € ì˜¤ë„ë¡ ì •ë ¬ (ë‚ ì§œ ì—­ìˆœ)
    blocks.sort(key=lambda x: x[0], reverse=True)
    
    return blocks


def extract_latest_date_block(row_text: str) -> Tuple[Optional[str], str]:
    """ìµœì‹  ë‚ ì§œ ë¸”ë¡ë§Œ ì¶”ì¶œ
    
    í–‰ í…ìŠ¤íŠ¸ì—ì„œ ì—¬ëŸ¬ ë‚ ì§œì˜ rank/N2ê°€ ë°˜ë³µë˜ë¯€ë¡œ,
    'ì²« ë‚ ì§œ ë§ˆì»¤ ~ ë‘ ë²ˆì§¸ ë‚ ì§œ ë§ˆì»¤ ì „' ì˜ì—­ë§Œ ì¶”ì¶œ
    
    Args:
        row_text: ì „ì²´ í–‰ í…ìŠ¤íŠ¸
        
    Returns:
        (íŒŒì‹±ëœ ë‚ ì§œ, ìµœì‹  ë¸”ë¡ í…ìŠ¤íŠ¸)
    """
    # ë‚ ì§œ ë§ˆì»¤ íŒ¨í„´: MM-DD(ìš”ì¼) í˜•ì‹
    pattern = r'\d{1,2}-\d{1,2}\s*(?:\([^)]+\))?'
    
    matches = list(re.finditer(pattern, row_text))
    
    if not matches:
        # ë‚ ì§œ ë§ˆì»¤ê°€ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ ë°˜í™˜
        return None, row_text
    
    # ì²« ë²ˆì§¸ ë‚ ì§œ ë§ˆì»¤
    first_match = matches[0]
    first_date = parse_date_marker(first_match.group(0))
    
    if len(matches) >= 2:
        # ë‘ ë²ˆì§¸ ë‚ ì§œ ë§ˆì»¤ ì´ì „ê¹Œì§€ë§Œ ì¶”ì¶œ
        second_match = matches[1]
        block_text = row_text[first_match.start():second_match.start()]
    else:
        # ë‚ ì§œ ë§ˆì»¤ê°€ í•˜ë‚˜ë©´ ê·¸ ì´í›„ ì „ì²´
        block_text = row_text[first_match.start():]
    
    return first_date, block_text


def extract_data_from_block(block_text: str) -> Dict:
    """ë‚ ì§œ ë¸”ë¡ì—ì„œ ìˆœìœ„/ë¸”/ë°©/N2 ì¶”ì¶œ
    
    Args:
        block_text: ë‚ ì§œ ë¸”ë¡ í…ìŠ¤íŠ¸
        
    Returns:
        {rank, saves, blog_reviews, visitor_reviews, n2_score}
    """
    data = {
        "rank": None,
        "saves": None,
        "blog_reviews": None,
        "visitor_reviews": None,
        "n2_score": None,
    }
    
    # N2 ì¶”ì¶œ: "N2 0.439588" íŒ¨í„´ (ì£¼í™©ìƒ‰ í…ìŠ¤íŠ¸)
    n2_match = re.search(r'N2\s*([0-9.]+)', block_text, re.IGNORECASE)
    if n2_match:
        try:
            data["n2_score"] = float(n2_match.group(1))
        except ValueError:
            pass
    
    # ìˆœìœ„ ì¶”ì¶œ: "3ìœ„" ë˜ëŠ” ë‹¨ë… ìˆ«ì
    # ë¸”ë¡ ì•ë¶€ë¶„ì—ì„œ ì²« ë²ˆì§¸ ìˆœìœ„ ì¶”ì¶œ
    rank_match = re.search(r'(\d+)\s*ìœ„', block_text)
    if rank_match:
        data["rank"] = int(rank_match.group(1))
    
    # ì €ì¥ ìˆ˜: "ì € X,XXX" íŒ¨í„´
    saves_match = re.search(r'ì €\s*([0-9,]+)', block_text)
    if saves_match:
        try:
            data["saves"] = int(saves_match.group(1).replace(',', ''))
        except ValueError:
            pass
    
    # ë¸”ë¡œê·¸ ë¦¬ë·°: "ë¸” XXX" íŒ¨í„´
    blog_match = re.search(r'ë¸”\s*([0-9,]+)', block_text)
    if blog_match:
        try:
            data["blog_reviews"] = int(blog_match.group(1).replace(',', ''))
        except ValueError:
            pass
    
    # ë°©ë¬¸ì ë¦¬ë·°: "ë°© XXX" íŒ¨í„´
    visitor_match = re.search(r'ë°©\s*([0-9,]+)', block_text)
    if visitor_match:
        try:
            data["visitor_reviews"] = int(visitor_match.group(1).replace(',', ''))
        except ValueError:
            pass
    
    return data


# =============================================================================
# RankDatabase (ë ˆê±°ì‹œ í˜¸í™˜ - SQLite)
# =============================================================================

class RankDatabase:
    """ìˆœìœ„ ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ í´ë˜ìŠ¤ (ë ˆê±°ì‹œ í˜¸í™˜ìš©)"""
    
    def __init__(self, db_path: str = None):
        self.db_path = db_path or DB_PATH
        self._init_db()
    
    def _init_db(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS rank_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                company TEXT NOT NULL,
                business_name TEXT NOT NULL,
                keyword TEXT NOT NULL,
                rank INTEGER,
                checked_at DATETIME NOT NULL,
                source TEXT DEFAULT 'adlog',
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # ì¸ë±ìŠ¤ ìƒì„±
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_business_name 
            ON rank_history(business_name)
        """)
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_checked_at 
            ON rank_history(checked_at DESC)
        """)
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_company_business 
            ON rank_history(company, business_name, checked_at DESC)
        """)
        
        conn.commit()
        conn.close()
        logger.info(f"Database initialized: {self.db_path}")
    
    def save_rank(self, company: str, business_name: str, keyword: str, 
                  rank: Optional[int], checked_at: datetime = None) -> bool:
        """ìˆœìœ„ ë°ì´í„° ì €ì¥"""
        if checked_at is None:
            checked_at = datetime.now(KST)
        
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute("""
                INSERT INTO rank_history 
                (company, business_name, keyword, rank, checked_at, source)
                VALUES (?, ?, ?, ?, ?, 'adlog')
            """, (company, business_name, keyword, rank, checked_at.isoformat()))
            
            conn.commit()
            conn.close()
            return True
        except Exception as e:
            logger.error(f"Failed to save rank: {e}")
            return False
    
    def get_latest_ranks(self, company: str = None) -> List[Dict]:
        """ìµœì‹  ìˆœìœ„ ë°ì´í„° ì¡°íšŒ"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        if company:
            query = """
                SELECT business_name, keyword, rank, checked_at
                FROM rank_history
                WHERE company = ? 
                  AND id IN (
                    SELECT MAX(id) FROM rank_history
                    WHERE company = ? GROUP BY business_name
                  )
                ORDER BY business_name
            """
            cursor.execute(query, (company, company))
        else:
            query = """
                SELECT business_name, keyword, rank, checked_at, company
                FROM rank_history
                WHERE id IN (
                    SELECT MAX(id) FROM rank_history
                    GROUP BY company, business_name
                )
                ORDER BY company, business_name
            """
            cursor.execute(query)
        
        rows = cursor.fetchall()
        conn.close()
        
        results = []
        for row in rows:
            if company:
                business_name, keyword, rank, checked_at = row
                comp = company
            else:
                business_name, keyword, rank, checked_at, comp = row
            
            results.append({
                "company": comp,
                "business_name": business_name,
                "keyword": keyword,
                "rank": rank,
                "checked_at": checked_at
            })
        
        return results
    
    def get_rank_history(self, business_name: str, limit: int = 30) -> List[Dict]:
        """ì—…ì²´ë³„ ìˆœìœ„ íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT keyword, rank, checked_at, company
            FROM rank_history
            WHERE business_name = ?
            ORDER BY checked_at DESC
            LIMIT ?
        """, (business_name, limit))
        
        rows = cursor.fetchall()
        conn.close()
        
        return [
            {"keyword": kw, "rank": r, "checked_at": ca, "company": co}
            for kw, r, ca, co in rows
        ]


# =============================================================================
# AdlogCrawler - N2 ì¶”ì¶œ + Google Sheets ì €ì¥
# =============================================================================

class AdlogCrawler:
    """ì• ë“œë¡œê·¸ ìˆœìœ„ í¬ë¡¤ëŸ¬ (N2 ì¶”ì¶œ + Google Sheets ì €ì¥)
    
    í•µì‹¬ ê¸°ëŠ¥:
    - ì²´í¬ë°•ìŠ¤ ìƒíƒœ ë§ì¶¤ (ë¦¬ë·°ìˆ˜ë³´ê¸°/ì—…ì²´ì ìˆ˜ë³´ê¸°)
    - ìµœì‹  ë‚ ì§œ ë¸”ë¡ë§Œ íŒŒì‹±
    - Google Sheets ì›ì¥ ì €ì¥
    """
    
    def __init__(self):
        # í™˜ê²½ë³€ìˆ˜ í•„ìˆ˜ ì²´í¬ (ê¸°ë³¸ê°’ ì—†ìŒ)
        self.adlog_id, self.adlog_password = get_adlog_credentials()
        
        self.adlog_url = os.getenv("ADLOG_URL", 
            "https://www.adlog.kr/adlog/naver_place_rank_check.php?sca=&sfl=api_memo&stx=%EC%9B%94%EB%B3%B4%EC%9E%A5&page_rows=100")
        
        # Headless ëª¨ë“œ ì„¤ì •
        self.headless = os.getenv("HEADLESS", "true").lower() == "true"
        
        # ìŠ¤í¬ë¦°ìƒ· ì €ì¥ ì„¤ì •
        self.save_screenshots = os.getenv("SAVE_SCREENSHOTS", "false").lower() == "true"
        self.screenshot_path = os.getenv("SCREENSHOT_PATH", "/tmp/rank_crawler_screenshots")
        
        # ì €ì¥ì†Œ ì„ íƒ
        self.storage_mode = os.getenv("RANK_STORAGE_MODE", "sheets").lower()
        
        # ë ˆê±°ì‹œ SQLite
        self.db = RankDatabase()
        
        # Google Sheets ë§¤ë‹ˆì € (lazy init)
        self._snapshot_manager = None
    
    @property
    def snapshot_manager(self):
        """RankSnapshotManager lazy initialization"""
        if self._snapshot_manager is None:
            try:
                from rank_snapshot_manager import RankSnapshotManager
                self._snapshot_manager = RankSnapshotManager()
            except ImportError as e:
                logger.warning(f"RankSnapshotManager not available: {e}")
                self._snapshot_manager = None
        return self._snapshot_manager
    
    def _get_monitoring_targets(self, company: str = None) -> List[Dict]:
        """ëª¨ë‹ˆí„°ë§ ëŒ€ìƒ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ì§„í–‰ì¤‘/í›„ë¶ˆ ìƒíƒœë§Œ)"""
        from guarantee_manager import GuaranteeManager
        gm = GuaranteeManager()
        
        # ì „ì²´ ì•„ì´í…œ ê°€ì ¸ì˜¤ê¸°
        items = gm.get_items()
        
        # ì§„í–‰ì¤‘/í›„ë¶ˆ í•„í„°
        active_statuses = ['ì§„í–‰ì¤‘', 'í›„ë¶ˆ']
        items = [item for item in items 
                 if item.get("status") in active_statuses]
        
        # íšŒì‚¬ í•„í„°
        if company:
            items = [item for item in items 
                     if item.get("company") == company]
        
        # í•„ìˆ˜ í•„ë“œê°€ ìˆëŠ” í•­ëª©ë§Œ ë°˜í™˜
        targets = []
        for item in items:
            if item.get("business_name") and item.get("main_keyword"):
                targets.append({
                    "business_name": item.get("business_name"),
                    "keyword": item.get("main_keyword"),
                    "place_url": item.get("url", ""),
                    "company": item.get("company", ""),
                    "agency": item.get("agency", ""),
                })
        
        return targets
    
    def _ensure_checkboxes(self, page: Page) -> None:
        """ì²´í¬ë°•ìŠ¤ ìƒíƒœ í™•ì¸ ë° ì„¤ì •
        
        'ë¦¬ë·°ìˆ˜ ë³´ê¸°', 'ì—…ì²´ì ìˆ˜ ë³´ê¸°' ì²´í¬ë°•ìŠ¤ë¥¼ ì›í•˜ëŠ” ìƒíƒœë¡œ ë§ì¶¤
        """
        try:
            # ì²´í¬ë°•ìŠ¤ ì…€ë ‰í„° (ì• ë“œë¡œê·¸ í˜ì´ì§€ êµ¬ì¡°ì— ë§ê²Œ ì¡°ì • í•„ìš”)
            checkboxes = {
                "ë¦¬ë·°ìˆ˜ë³´": "input[name='show_review']",  # ì˜ˆì‹œ
                "ì—…ì²´ì ìˆ˜ë³´": "input[name='show_score']",  # ì˜ˆì‹œ
            }
            
            # ë” ì¼ë°˜ì ì¸ ì ‘ê·¼: ì²´í¬ë°•ìŠ¤ ë ˆì´ë¸”ë¡œ ì°¾ê¸°
            # ì‹¤ì œ ì…€ë ‰í„°ëŠ” í˜ì´ì§€ êµ¬ì¡°ì— ë”°ë¼ ì¡°ì • í•„ìš”
            
            # PCë¦¬ë·°ë³´ ì²´í¬ë°•ìŠ¤
            pc_review = page.locator('text=PCë¦¬ë·°ë³´').first
            if pc_review.count() > 0:
                checkbox = pc_review.locator('xpath=preceding-sibling::input[1]')
                if checkbox.count() > 0 and not checkbox.is_checked():
                    checkbox.check()
                    logger.info("Checked 'PCë¦¬ë·°ë³´' checkbox")
            
            # ë¦¬ë·°ìˆ˜ë³´ ì²´í¬ë°•ìŠ¤  
            review_count = page.locator('text=ë¦¬ë·°ìˆ˜ë³´').first
            if review_count.count() > 0:
                checkbox = review_count.locator('xpath=preceding-sibling::input[1]')
                if checkbox.count() > 0 and not checkbox.is_checked():
                    checkbox.check()
                    logger.info("Checked 'ë¦¬ë·°ìˆ˜ë³´' checkbox")
            
            # ì—…ì²´ì ìˆ˜ë³´ ì²´í¬ë°•ìŠ¤
            score = page.locator('text=ì—…ì²´ì ìˆ˜ë³´').first
            if score.count() > 0:
                checkbox = score.locator('xpath=preceding-sibling::input[1]')
                if checkbox.count() > 0 and not checkbox.is_checked():
                    checkbox.check()
                    logger.info("Checked 'ì—…ì²´ì ìˆ˜ë³´' checkbox")
            
            # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨ (ì²´í¬ë°•ìŠ¤ ë³€ê²½ ì ìš©)
            page.wait_for_timeout(500)
            
        except Exception as e:
            logger.warning(f"Checkbox handling warning: {e}")
    
    def _parse_row(self, row_element, targets_map: Dict, url_map: Dict = None) -> Optional[Dict]:
        """í–‰ ë°ì´í„° íŒŒì‹± (ìµœì‹  ë‚ ì§œ ë¸”ë¡ë§Œ)
        
        ì• ë“œë¡œê·¸ í…Œì´ë¸” êµ¬ì¡°:
        - [0] ì²´í¬ë°•ìŠ¤
        - [1] ê·¸ë£¹ëª…
        - [2] ê²€ìƒ‰ í‚¤ì›Œë“œ ID (ìˆ«ì)
        - [3] í”Œë ˆì´ìŠ¤ URL/ìƒí˜¸ëª… (ë§í¬ í…ìŠ¤íŠ¸ê°€ ìƒí˜¸ëª…)
        - [4~] ë‚ ì§œë³„ ìˆœìœ„ ë°ì´í„°
        
        Args:
            row_element: Playwright row element
            targets_map: {ìƒí˜¸ëª…: target_info} ë§¤í•‘
            url_map: {place_id: target_info} ë§¤í•‘ (ì¶”ê°€ ë§¤ì¹­ìš©)
            
        Returns:
            íŒŒì‹±ëœ ë°ì´í„° ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” None
        """
        try:
            # í–‰ ì „ì²´ í…ìŠ¤íŠ¸
            row_text = row_element.inner_text()
            
            # ì…€ ì¶”ì¶œ
            cells = row_element.locator('td').all()
            if len(cells) < 5:
                return None
            
            # === ìƒí˜¸ëª… ì¶”ì¶œ ===
            # ë°©ë²• 1: í”Œë ˆì´ìŠ¤ URL ì…€(3ë²ˆì§¸)ì—ì„œ ë§í¬ í…ìŠ¤íŠ¸ ì°¾ê¸°
            business_name = ""
            place_url_from_page = ""
            keyword_from_page = ""
            
            # ì…€ 3 ë˜ëŠ” 4ì—ì„œ place.naver.com ë§í¬ ì°¾ê¸°
            for cell_idx in [3, 4, 2]:
                if cell_idx >= len(cells):
                    continue
                try:
                    cell = cells[cell_idx]
                    links = cell.locator('a').all()
                    for link in links:
                        href = link.get_attribute('href') or ""
                        if 'place.naver.com' in href or 'm.place.naver.com' in href:
                            place_url_from_page = href
                            # ë§í¬ í…ìŠ¤íŠ¸ê°€ ìƒí˜¸ëª… (ë‹¨, URLì´ ì•„ë‹Œ ê²½ìš°)
                            link_text = link.inner_text().strip()
                            if link_text and 'place.naver.com' not in link_text and 'http' not in link_text:
                                business_name = link_text
                            break
                except:
                    continue
                if place_url_from_page:
                    break
            
            # ë°©ë²• 2: ìƒí˜¸ëª…ì´ ì—†ìœ¼ë©´ í–‰ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ ì‹œë„
            if not business_name:
                # í–‰ í…ìŠ¤íŠ¸ì—ì„œ ìƒí˜¸ëª… íŒ¨í„´ ì°¾ê¸°
                # ë³´í†µ "ìƒí˜¸ëª…" í˜•íƒœë¡œ ë‚˜íƒ€ë‚¨
                import re
                # place URL ì•ì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                if 'ì œì´íˆ¬ë©' in row_text or 'ì¼ë¥˜ê¸°íš' in row_text:
                    # ê·¸ë£¹ëª… ì´í›„, URL ì´ì „ í…ìŠ¤íŠ¸
                    parts = row_text.split('\n')
                    for part in parts:
                        part = part.strip()
                        # URLì´ë‚˜ ë‚ ì§œê°€ ì•„ë‹Œ 2ê¸€ì ì´ìƒ í…ìŠ¤íŠ¸
                        if len(part) >= 2 and 'place' not in part and 'http' not in part:
                            if not re.match(r'^[\d\-\.\s\(\)]+$', part):  # ìˆ«ì/ë‚ ì§œê°€ ì•„ë‹˜
                                if 'ë“±ë¡ì' not in part and 'ë©”ëª¨' not in part:
                                    business_name = part
                                    break
            
            # === í‚¤ì›Œë“œ ì¶”ì¶œ ===
            # ì…€ ë‚´ìš©ì—ì„œ í‚¤ì›Œë“œ íŒíŠ¸ ì°¾ê¸°
            try:
                # ê²€ìƒ‰ í‚¤ì›Œë“œ ì…€ (ë³´í†µ 2ë²ˆì§¸ ë˜ëŠ” ê·¸ ê·¼ì²˜)
                for cell_idx in [2, 1]:
                    if cell_idx >= len(cells):
                        continue
                    cell_text = cells[cell_idx].inner_text().strip()
                    # ìˆ«ìë§Œ ìˆëŠ” ê²½ìš° ë¬´ì‹œ
                    if cell_text and not cell_text.isdigit():
                        keyword_from_page = cell_text
                        break
            except:
                pass
            
            # === place_id ì¶”ì¶œ ===
            place_id = ""
            if place_url_from_page:
                import re
                match = re.search(r'/(\d{5,})', place_url_from_page)
                if match:
                    place_id = match.group(1)
            
            # === íƒ€ê²Ÿ ë§¤ì¹­ ===
            target = None
            matched_by = ""
            
            # 1ë‹¨ê³„: place_idë¡œ URL ë§¤ì¹­ (ê°€ì¥ ì •í™•)
            if url_map and place_id:
                if place_id in url_map:
                    target = url_map[place_id]
                    matched_by = f"place_id:{place_id}"
            
            # 2ë‹¨ê³„: ìƒí˜¸ëª… ì •í™• ë§¤ì¹­
            if not target and business_name:
                target = targets_map.get(business_name)
                if target:
                    matched_by = f"exact:{business_name}"
            
            # 3ë‹¨ê³„: ìƒí˜¸ëª… ë¶€ë¶„ ë§¤ì¹­
            if not target and business_name:
                for target_name, target_info in targets_map.items():
                    # ì• ë“œë¡œê·¸ ìƒí˜¸ëª…ì´ íƒ€ê²Ÿì— í¬í•¨
                    if business_name in target_name:
                        target = target_info
                        matched_by = f"partial:{business_name} in {target_name}"
                        break
                    # íƒ€ê²Ÿì´ ì• ë“œë¡œê·¸ ìƒí˜¸ëª…ì— í¬í•¨
                    if target_name in business_name:
                        target = target_info
                        matched_by = f"partial:{target_name} in {business_name}"
                        break
            
            # 4ë‹¨ê³„: í‚¤ì›Œë“œ ë§¤ì¹­ (ë§ˆì§€ë§‰ ìˆ˜ë‹¨)
            if not target and keyword_from_page:
                for target_name, target_info in targets_map.items():
                    if target_info.get("keyword") == keyword_from_page:
                        target = target_info
                        matched_by = f"keyword:{keyword_from_page}"
                        break
            
            if not target:
                return None
            
            # === ìµœì‹  ë‚ ì§œ ì…€ì—ì„œ ë°ì´í„° ì¶”ì¶œ ===
            # ì• ë“œë¡œê·¸ í…Œì´ë¸” êµ¬ì¡°: [ì²´í¬ë°•ìŠ¤][ê·¸ë£¹][í‚¤ì›Œë“œID][URL][ë‚ ì§œ1][ë‚ ì§œ2]...
            # ë‚ ì§œ ì…€ì€ ì¸ë±ìŠ¤ 4ë¶€í„° ì‹œì‘ (ì²« ë²ˆì§¸ ë‚ ì§œê°€ ê°€ì¥ ìµœê·¼)
            
            parsed_date = None
            block_data = {"rank": None, "saves": None, "blog_reviews": None, "visitor_reviews": None, "n2_score": None}
            
            # ë‚ ì§œ ì…€ íƒìƒ‰ (ì¸ë±ìŠ¤ 4ë¶€í„°)
            for cell_idx in range(4, min(len(cells), 20)):  # ìµœëŒ€ 20ê°œ ì…€ê¹Œì§€
                try:
                    cell = cells[cell_idx]
                    cell_text = cell.inner_text().strip()
                    
                    if not cell_text:
                        continue
                    
                    # ë‚ ì§œ ë§ˆì»¤ í™•ì¸ (ì˜ˆ: "12-28(í† )")
                    date_match = re.search(r'(\d{1,2})-(\d{1,2})\s*(?:\([^)]+\))?', cell_text)
                    if date_match:
                        month = int(date_match.group(1))
                        day = int(date_match.group(2))
                        
                        # ìœ íš¨í•œ ë‚ ì§œì¸ì§€ í™•ì¸
                        if 1 <= month <= 12 and 1 <= day <= 31:
                            # ì—°ë„ ê³„ì‚°
                            now = datetime.now(KST)
                            year = now.year
                            if now.month <= 2 and month >= 11:
                                year -= 1
                            elif now.month >= 11 and month <= 2:
                                year += 1
                            parsed_date = f"{year}-{month:02d}-{day:02d}"
                            
                            # ì´ ì…€ì—ì„œ ìˆœìœ„ ë°ì´í„° ì¶”ì¶œ
                            # ìˆœìœ„: "3ìœ„" íŒ¨í„´
                            rank_match = re.search(r'(\d+)\s*ìœ„', cell_text)
                            if rank_match:
                                block_data["rank"] = int(rank_match.group(1))
                            
                            # N2 ì ìˆ˜: "N2 0.439" íŒ¨í„´
                            n2_match = re.search(r'N2\s*([0-9.]+)', cell_text, re.IGNORECASE)
                            if n2_match:
                                try:
                                    block_data["n2_score"] = float(n2_match.group(1))
                                except ValueError:
                                    pass
                            
                            # ì €ì¥ ìˆ˜: "ì € 2,419" ë˜ëŠ” "ì €ì¥ 2,419" íŒ¨í„´
                            saves_match = re.search(r'ì €[ì¥]?\s*([0-9,]+)', cell_text)
                            if saves_match:
                                try:
                                    block_data["saves"] = int(saves_match.group(1).replace(',', ''))
                                except ValueError:
                                    pass
                            
                            # ë¸”ë¡œê·¸ ë¦¬ë·°: "ë¸” 243" íŒ¨í„´
                            blog_match = re.search(r'ë¸”\s*([0-9,]+)', cell_text)
                            if blog_match:
                                try:
                                    block_data["blog_reviews"] = int(blog_match.group(1).replace(',', ''))
                                except ValueError:
                                    pass
                            
                            # ë°©ë¬¸ì ë¦¬ë·°: "ë°© 1,189" íŒ¨í„´
                            visitor_match = re.search(r'ë°©\s*([0-9,]+)', cell_text)
                            if visitor_match:
                                try:
                                    block_data["visitor_reviews"] = int(visitor_match.group(1).replace(',', ''))
                                except ValueError:
                                    pass
                            
                            # ì²« ë²ˆì§¸ ìœ íš¨í•œ ë‚ ì§œ ì…€ì„ ì°¾ìœ¼ë©´ ì¢…ë£Œ
                            break
                            
                except Exception as cell_error:
                    logger.debug(f"Cell {cell_idx} parse error: {cell_error}")
                    continue
            
            # ë‚ ì§œê°€ ì—†ìœ¼ë©´ ì˜¤ëŠ˜ë¡œ ì„¤ì •
            if not parsed_date:
                parsed_date = datetime.now(KST).strftime("%Y-%m-%d")
            
            # ë””ë²„ê·¸ ë¡œê·¸
            if matched_by:
                logger.debug(f"Matched by {matched_by}: date={parsed_date}, rank={block_data.get('rank')}, n2={block_data.get('n2_score')}")
            
            # ê²°ê³¼ ì¡°í•© (ìƒí˜¸ëª…ì€ íƒ€ê²Ÿì—ì„œ ê°€ì ¸ì˜¤ê¸°)
            return {
                "date": parsed_date,
                "client_name": target.get("business_name") or business_name,  # íƒ€ê²Ÿ ìƒí˜¸ëª… ìš°ì„ 
                "keyword": target.get("keyword", "") or keyword_from_page,
                "place_url": target.get("place_url", "") or place_url_from_page,
                "group": target.get("company", ""),
                "rank": block_data.get("rank"),
                "saves": block_data.get("saves"),
                "blog_reviews": block_data.get("blog_reviews"),
                "visitor_reviews": block_data.get("visitor_reviews"),
                "n2_score": block_data.get("n2_score"),
            }
            
        except Exception as e:
            logger.warning(f"Row parsing error: {e}")
            import traceback
            logger.debug(traceback.format_exc())
            return None
    
    def crawl_ranks(self, company: str = None) -> Dict:
        """ì• ë“œë¡œê·¸ì—ì„œ ìˆœìœ„ ë°ì´í„° í¬ë¡¤ë§
        
        Args:
            company: íŠ¹ì • íšŒì‚¬ë§Œ í•„í„°ë§ (Noneì´ë©´ ì „ì²´)
            
        Returns:
            í¬ë¡¤ë§ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        start_time = time.time()
        logger.info(f"Starting rank crawling from Adlog for company: {company or 'ALL'}")
        logger.info(f"Storage mode: {self.storage_mode}, Headless: {self.headless}")
        
        crawled_data = []
        failed_details = []
        crawled_count = 0
        failed_count = 0
        
        # í˜„ì¬ ì‹œê°„ëŒ€
        now = datetime.now(KST)
        time_slot = "09:00" if now.hour < 12 else "15:00"
        collected_at = now.isoformat()
        
        try:
            # Playwright ë¸Œë¼ìš°ì € ì„¤ì¹˜ í™•ì¸
            try:
                import subprocess
                logger.info("Checking Playwright browser installation...")
                result = subprocess.run(
                    ['playwright', 'install', 'chromium'],
                    capture_output=True, text=True, timeout=120
                )
                if result.returncode == 0:
                    logger.info("Chromium browser ready")
            except Exception as install_error:
                logger.warning(f"Browser install check failed: {install_error}")
            
            with sync_playwright() as p:
                logger.info("Launching Chromium browser...")
                
                browser = p.chromium.launch(
                    headless=self.headless,
                    args=[
                        '--no-sandbox',
                        '--disable-setuid-sandbox',
                        '--disable-dev-shm-usage',
                        '--disable-gpu',
                        '--disable-software-rasterizer',
                        '--disable-extensions',
                        '--single-process',
                        '--no-zygote'
                    ]
                )
                logger.info("âœ… Browser launched")
                
                context = browser.new_context(
                    viewport={'width': 1280, 'height': 720},
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                )
                page = context.new_page()
                
                # ë¡œê·¸ì¸
                login_url = "https://www.adlog.kr/bbs/login.php?url=%2Fadlog%2Fnaver_place_rank_check.php%3Fsca%3D%26sfl%3Dapi_memo%26stx%3D%25EC%259B%2594%25EB%25B3%25B4%25EC%259E%25A5%26page_rows%3D100"
                logger.info("Navigating to login page...")
                
                page.goto(login_url, wait_until="domcontentloaded", timeout=60000)
                logger.info("âœ… Login page loaded")
                
                # ë¡œê·¸ì¸ ì •ë³´ ì…ë ¥
                page.fill('input[name="mb_id"]', self.adlog_id)
                page.fill('input[name="mb_password"]', self.adlog_password)
                logger.info("âœ… Credentials filled")
                
                # ë¡œê·¸ì¸ ë²„íŠ¼ í´ë¦­
                page.click('button[type="submit"], input[type="submit"]')
                logger.info("âœ… Login button clicked")
                
                # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
                page.wait_for_load_state("domcontentloaded", timeout=60000)
                page.wait_for_timeout(2000)
                logger.info("âœ… Login completed")
                
                # ì²´í¬ë°•ìŠ¤ ìƒíƒœ í™•ì¸/ì„¤ì •
                self._ensure_checkboxes(page)
                
                # í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ
                logger.info("Looking for table data...")
                
                # ëª¨ë“  í–‰ ê°€ì ¸ì˜¤ê¸°
                all_rows = page.locator('table tbody tr').all()
                logger.info(f"âœ… Found {len(all_rows)} total rows")
                
                # ëª¨ë‹ˆí„°ë§ ëŒ€ìƒ ë¡œë“œ
                targets = self._get_monitoring_targets(company)
                logger.info(f"Loaded {len(targets)} monitoring targets")
                
                # ë””ë²„ê·¸: ì²˜ìŒ 5ê°œ íƒ€ê²Ÿ ì¶œë ¥
                for i, t in enumerate(targets[:5]):
                    logger.info(f"  Target {i+1}: {t.get('business_name')} / {t.get('keyword')}")
                
                # íƒ€ê²Ÿ â†’ ìƒí˜¸ëª… ë§µ (ë¹ ë¥¸ ë§¤ì¹­ìš©)
                targets_map = {t["business_name"]: t for t in targets}
                
                # URL ë§µ (place_url ê¸°ë°˜ ë§¤ì¹­ìš©)
                url_map = {}
                for t in targets:
                    url = t.get("place_url", "")
                    if url:
                        import re
                        match = re.search(r'/(\d{5,})', url)
                        if match:
                            url_map[match.group(1)] = t
                
                logger.info(f"Created {len(targets_map)} name mappings, {len(url_map)} URL mappings")
                
                # ìŠ¤ëƒ…ìƒ· ì €ì¥ìš© ë ˆì½”ë“œ
                snapshot_records = []
                
                # ê¸°ë³¸ì •ë³´ í–‰ + ìˆœìœ„ í–‰(tr.tr2)ì„ ìŒìœ¼ë¡œ ì²˜ë¦¬
                # ê¸°ë³¸ì •ë³´ í–‰ì—ì„œ ìƒí˜¸ëª…/URL ì¶”ì¶œ, ìˆœìœ„ í–‰ì—ì„œ ìˆœìœ„/N2 ì¶”ì¶œ
                current_info = None  # í˜„ì¬ ê¸°ë³¸ì •ë³´ í–‰ ë°ì´í„°
                processed_count = 0
                
                for idx, row in enumerate(all_rows):
                    if idx % 20 == 0:
                        logger.info(f"Processing row {idx+1}/{len(all_rows)}...")
                    
                    try:
                        # í˜„ì¬ í–‰ì´ ìˆœìœ„ í–‰(tr.tr2)ì¸ì§€ í™•ì¸
                        row_class = row.get_attribute('class') or ""
                        is_rank_row = 'tr2' in row_class
                        
                        if not is_rank_row:
                            # === ê¸°ë³¸ì •ë³´ í–‰: ìƒí˜¸ëª…/URL ì¶”ì¶œ ===
                            business_name = ""
                            place_url = ""
                            place_id = ""
                            
                            # ë§í¬ì—ì„œ place.naver.com URLê³¼ ìƒí˜¸ëª… ì°¾ê¸°
                            links = row.locator('a').all()
                            for link in links:
                                href = link.get_attribute('href') or ""
                                if 'place.naver.com' in href:
                                    place_url = href
                                    link_text = link.inner_text().strip()
                                    if link_text and 'http' not in link_text and len(link_text) > 1:
                                        business_name = link_text
                                    # place_id ì¶”ì¶œ
                                    id_match = re.search(r'/(\d{5,})', href)
                                    if id_match:
                                        place_id = id_match.group(1)
                                    break
                            
                            # í˜„ì¬ ê¸°ë³¸ì •ë³´ ì €ì¥ (ë‹¤ìŒ ìˆœìœ„ í–‰ì—ì„œ ì‚¬ìš©)
                            current_info = {
                                "business_name": business_name,
                                "place_url": place_url,
                                "place_id": place_id,
                            }
                            
                            # ë””ë²„ê·¸ (ì²˜ìŒ 3ê°œ)
                            if processed_count < 3 and business_name:
                                logger.info(f"  Info row: business_name='{business_name}', place_id='{place_id}'")
                            
                        else:
                            # === ìˆœìœ„ í–‰(tr.tr2): ìˆœìœ„/N2 ì¶”ì¶œ ===
                            if not current_info:
                                continue
                            
                            # stat_div ì°¾ê¸°
                            stat_divs = row.locator('div.stat_div').all()
                            if not stat_divs:
                                stat_divs = row.locator('[class*="stat"]').all()
                            
                            if not stat_divs:
                                continue
                            
                            # ì²« ë²ˆì§¸ stat_div (ìµœì‹  ë‚ ì§œ)
                            first_stat_div = stat_divs[0]
                            stat_text = first_stat_div.inner_text().strip()
                            
                            # ë””ë²„ê·¸: ì²˜ìŒ 3ê°œ stat_text ì¶œë ¥
                            if processed_count < 3:
                                logger.info(f"  Rank row stat_text: {stat_text[:80]}...")
                            
                            # === ìˆœìœ„ ë°ì´í„° íŒŒì‹± ===
                            # ë‚ ì§œ ì¶”ì¶œ: "12-28(ì¼)" í˜•ì‹
                            parsed_date = None
                            date_match = re.search(r'(\d{1,2})-(\d{1,2})', stat_text)
                            if date_match:
                                month, day = int(date_match.group(1)), int(date_match.group(2))
                                if 1 <= month <= 12 and 1 <= day <= 31:
                                    year = now.year
                                    if now.month <= 2 and month >= 11:
                                        year -= 1
                                    elif now.month >= 11 and month <= 2:
                                        year += 1
                                    parsed_date = f"{year}-{month:02d}-{day:02d}"
                            
                            # ìˆœìœ„ ì¶”ì¶œ: "12ìœ„" í˜•ì‹
                            rank_val = None
                            rank_match = re.search(r'(\d+)\s*ìœ„', stat_text)
                            if rank_match:
                                rank_val = int(rank_match.group(1))
                            
                            # ì €ì¥ìˆ˜ ì¶”ì¶œ: fc_grn í´ë˜ìŠ¤ ë˜ëŠ” í…ìŠ¤íŠ¸ì—ì„œ
                            saves_val = None
                            try:
                                saves_elem = first_stat_div.locator('b.fc_grn').first
                                if saves_elem.count() > 0:
                                    saves_text = saves_elem.inner_text().strip()
                                    saves_match = re.search(r'([0-9,]+)', saves_text)
                                    if saves_match:
                                        saves_val = int(saves_match.group(1).replace(',', ''))
                            except:
                                pass
                            
                            # ë¸”ë¡œê·¸ ë¦¬ë·°: "ë¸” 393ê°œ" í˜•ì‹
                            blog_val = None
                            blog_match = re.search(r'ë¸”\s*([0-9,]+)\s*ê°œ?', stat_text)
                            if blog_match:
                                blog_val = int(blog_match.group(1).replace(',', ''))
                            
                            # ë°©ë¬¸ì ë¦¬ë·°: "ë°© 287ê°œ" í˜•ì‹
                            visitor_val = None
                            visitor_match = re.search(r'ë°©\s*([0-9,]+)\s*ê°œ?', stat_text)
                            if visitor_match:
                                visitor_val = int(visitor_match.group(1).replace(',', ''))
                            
                            # N2 ì ìˆ˜: "N2 0.439588" í˜•ì‹
                            n2_val = None
                            n2_match = re.search(r'N2\s*([0-9.]+)', stat_text, re.IGNORECASE)
                            if n2_match:
                                try:
                                    n2_val = float(n2_match.group(1))
                                except ValueError:
                                    pass
                            
                            # === íƒ€ê²Ÿ ë§¤ì¹­ ===
                            business_name = current_info["business_name"]
                            place_url = current_info["place_url"]
                            place_id = current_info["place_id"]
                            
                            target = None
                            matched_by = ""
                            
                            # 1ë‹¨ê³„: place_idë¡œ URL ë§¤ì¹­ (ê°€ì¥ ì •í™•)
                            if url_map and place_id:
                                if place_id in url_map:
                                    target = url_map[place_id]
                                    matched_by = f"place_id:{place_id}"
                            
                            # 2ë‹¨ê³„: ìƒí˜¸ëª… ì •í™• ë§¤ì¹­
                            if not target and business_name:
                                target = targets_map.get(business_name)
                                if target:
                                    matched_by = f"exact:{business_name}"
                            
                            # 3ë‹¨ê³„: ìƒí˜¸ëª… ë¶€ë¶„ ë§¤ì¹­
                            if not target and business_name:
                                for target_name, target_info in targets_map.items():
                                    if business_name in target_name or target_name in business_name:
                                        target = target_info
                                        matched_by = f"partial:{business_name}"
                                        break
                            
                            if not target:
                                if processed_count < 3:
                                    logger.warning(f"  No match: business='{business_name}', place_id='{place_id}'")
                                current_info = None  # ë¦¬ì…‹
                                continue
                            
                            # ë‚ ì§œê°€ ì—†ìœ¼ë©´ ì˜¤ëŠ˜
                            if not parsed_date:
                                parsed_date = now.strftime("%Y-%m-%d")
                            
                            # ê²°ê³¼ ìƒì„±
                            parsed = {
                                "date": parsed_date,
                                "time_slot": time_slot,
                                "collected_at": collected_at,
                                "agency": target.get("agency", ""),  # ëŒ€í–‰ì‚¬ëª…
                                "client_name": target.get("business_name") or business_name,
                                "keyword": target.get("keyword", ""),
                                "place_url": target.get("place_url", "") or place_url,
                                "group": target.get("company", ""),
                                "rank": rank_val,
                                "saves": saves_val,
                                "blog_reviews": blog_val,
                                "visitor_reviews": visitor_val,
                                "n2_score": n2_val,
                            }
                            
                            # ë””ë²„ê·¸ ë¡œê·¸ (ì²˜ìŒ 5ê°œ)
                            if processed_count < 5:
                                logger.info(
                                    f"  âœ… [{processed_count+1}] {parsed['client_name'][:15]} / {parsed['keyword']} â†’ "
                                    f"ìˆœìœ„:{rank_val}, N2:{n2_val}, ì €ì¥:{saves_val}"
                                )
                            
                            # Google Sheets ì €ì¥ìš©
                            if self.storage_mode in ["sheets", "both"] and self.snapshot_manager:
                                snapshot_records.append(parsed)
                            
                            # SQLite ì €ì¥ (ë ˆê±°ì‹œ)
                            if self.storage_mode in ["sqlite", "both"]:
                                self.db.save_rank(
                                    parsed.get("group", ""),
                                    parsed.get("client_name", ""),
                                    parsed.get("keyword", ""),
                                    parsed.get("rank"),
                                    now
                                )
                            
                            crawled_data.append(parsed)
                            crawled_count += 1
                            processed_count += 1
                            
                            # í˜„ì¬ ì •ë³´ ë¦¬ì…‹ (ë‹¤ìŒ ê¸°ë³¸ì •ë³´ í–‰ê¹Œì§€)
                            current_info = None
                            
                    except Exception as e:
                        logger.warning(f"Row {idx} error: {e}")
                        failed_details.append({"row": idx, "reason": str(e)})
                        failed_count += 1
                
                # Google Sheets ë°°ì¹˜ ì €ì¥
                if snapshot_records and self.snapshot_manager:
                    logger.info(f"Saving {len(snapshot_records)} records to Google Sheets...")
                    try:
                        upsert_result = self.snapshot_manager.upsert_bulk(snapshot_records)
                        logger.info(f"âœ… Sheets save result: {upsert_result}")
                    except Exception as sheets_error:
                        logger.error(f"Sheets save error: {sheets_error}")
                
                browser.close()
                logger.info("âœ… Browser closed")
        
        except ValueError as ve:
            # í™˜ê²½ë³€ìˆ˜ ì˜¤ë¥˜ (ADLOG_ID/PASSWORD ë¯¸ì„¤ì •)
            logger.error(f"âŒ Configuration error: {ve}")
            return {
                "success": False,
                "crawled_count": 0,
                "failed_count": 0,
                "message": str(ve),
                "data": [],
                "failed_details": [{"error": str(ve)}],
                "elapsed_seconds": 0
            }
        
        except Exception as e:
            logger.error(f"âŒ Crawling failed: {e}")
            import traceback
            logger.error(traceback.format_exc())
            
            elapsed = time.time() - start_time
            
            if self.snapshot_manager:
                try:
                    self.snapshot_manager.log_execution({
                        "success_count": 0,
                        "failed_count": 1,
                        "elapsed_seconds": round(elapsed, 2),
                        "message": f"í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}",
                        "failed_details": [{"error": str(e)}]
                    })
                except:
                    pass
            
            return {
                "success": False,
                "crawled_count": 0,
                "failed_count": 0,
                "message": f"í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}",
                "data": [],
                "failed_details": [{"error": str(e)}],
                "elapsed_seconds": round(time.time() - start_time, 2)
            }
        
        elapsed = time.time() - start_time
        message = f"ìˆœìœ„ í¬ë¡¤ë§ ì™„ë£Œ - ì„±ê³µ: {crawled_count}ê±´, ì‹¤íŒ¨: {failed_count}ê±´ ({elapsed:.1f}ì´ˆ)"
        logger.info(f"ğŸ‰ {message}")
        
        # ì‹¤í–‰ ë¡œê·¸ ê¸°ë¡
        if self.snapshot_manager:
            try:
                self.snapshot_manager.log_execution({
                    "success_count": crawled_count,
                    "failed_count": failed_count,
                    "elapsed_seconds": round(elapsed, 2),
                    "message": message,
                    "failed_details": failed_details[:10]
                })
            except Exception as log_error:
                logger.warning(f"Log error: {log_error}")
        
        return {
            "success": True,
            "crawled_count": crawled_count,
            "failed_count": failed_count,
            "message": message,
            "data": crawled_data,
            "failed_details": failed_details[:20],
            "elapsed_seconds": round(elapsed, 2),
            "checked_at": collected_at
        }
    
    def _save_screenshot(self, page: Page, name: str) -> None:
        """ì—ëŸ¬ ìŠ¤í¬ë¦°ìƒ· ì €ì¥"""
        try:
            os.makedirs(self.screenshot_path, exist_ok=True)
            timestamp = datetime.now(KST).strftime("%Y%m%d_%H%M%S")
            filepath = os.path.join(self.screenshot_path, f"{name}_{timestamp}.png")
            page.screenshot(path=filepath)
            logger.info(f"Screenshot saved: {filepath}")
        except Exception as e:
            logger.warning(f"Screenshot error: {e}")


# =============================================================================
# ì™¸ë¶€ í˜¸ì¶œìš© í•¨ìˆ˜
# =============================================================================

def crawl_ranks_for_company(company: str = None) -> Dict:
    """íŠ¹ì • íšŒì‚¬ì˜ ìˆœìœ„ í¬ë¡¤ë§ (ì™¸ë¶€ í˜¸ì¶œìš©)"""
    crawler = AdlogCrawler()
    return crawler.crawl_ranks(company)


def get_latest_ranks(company: str = None) -> List[Dict]:
    """ìµœì‹  ìˆœìœ„ ë°ì´í„° ì¡°íšŒ (SQLite ê¸°ë°˜)"""
    db = RankDatabase()
    return db.get_latest_ranks(company)


def get_latest_ranks_from_sheets(company: str = None) -> List[Dict]:
    """ìµœì‹  ìˆœìœ„ ë°ì´í„° ì¡°íšŒ (Google Sheets ê¸°ë°˜)"""
    try:
        from rank_snapshot_manager import RankSnapshotManager
        manager = RankSnapshotManager()
        history = manager.get_history(days=1)
        
        if company:
            history = [h for h in history if h.get("group") == company]
        
        return history
    except Exception as e:
        logger.error(f"Sheets rank error: {e}")
        return []


def get_rank_history(business_name: str, limit: int = 30) -> List[Dict]:
    """ì—…ì²´ë³„ ìˆœìœ„ íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
    db = RankDatabase()
    return db.get_rank_history(business_name, limit)


def get_current_rank_for_business(company: str, business_name: str) -> Optional[Dict]:
    """íŠ¹ì • ì—…ì²´ì˜ í˜„ì¬ ìˆœìœ„ ì¡°íšŒ"""
    # Sheetsì—ì„œ ì¡°íšŒ ì‹œë„
    try:
        from rank_snapshot_manager import RankSnapshotManager
        manager = RankSnapshotManager()
        history = manager.get_history(days=1)
        
        for record in history:
            if record.get("client_name") == business_name:
                return {
                    "rank": int(record.get("rank")) if record.get("rank") else None,
                    "keyword": record.get("keyword"),
                    "checked_at": record.get("collected_at"),
                    "n2_score": float(record.get("n2_score")) if record.get("n2_score") else None,
                    "saves": int(record.get("saves")) if record.get("saves") else None,
                }
    except Exception as e:
        logger.warning(f"Sheets lookup failed: {e}")
    
    # SQLite í´ë°±
    db = RankDatabase()
    latest_ranks = db.get_latest_ranks(company)
    
    for rank_data in latest_ranks:
        if rank_data["business_name"] == business_name:
            return {
                "rank": rank_data["rank"],
                "keyword": rank_data["keyword"],
                "checked_at": rank_data["checked_at"]
            }
    
    return None


# =============================================================================
# Cron Token Endpointìš© í•¨ìˆ˜ (Render ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€)
# =============================================================================

def crawl_ranks_with_token(token: str, company: str = None) -> Dict:
    """í† í° ì¸ì¦ í›„ í¬ë¡¤ë§ ì‹¤í–‰ (ì™¸ë¶€ cron ì„œë¹„ìŠ¤ìš©)
    
    Args:
        token: CRON_TOKEN í™˜ê²½ë³€ìˆ˜ì™€ ë¹„êµí•  í† í°
        company: íšŒì‚¬ í•„í„°
        
    Returns:
        í¬ë¡¤ë§ ê²°ê³¼ ë˜ëŠ” ì¸ì¦ ì˜¤ë¥˜
    """
    expected_token = os.getenv("CRON_TOKEN")
    
    if not expected_token:
        return {
            "success": False,
            "message": "CRON_TOKENì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        }
    
    if token != expected_token:
        return {
            "success": False,
            "message": "Invalid token"
        }
    
    return crawl_ranks_for_company(company)
