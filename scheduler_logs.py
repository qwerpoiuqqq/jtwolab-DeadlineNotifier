"""
ìŠ¤ì¼€ì¤„ëŸ¬ ë¡œê·¸ ê´€ë¦¬ ëª¨ë“ˆ
ëª¨ë“  ìŠ¤ì¼€ì¤„ëŸ¬ ì‘ì—…ì˜ ì‹¤í–‰ ë¡œê·¸ë¥¼ ì €ì¥í•˜ê³  ì¡°íšŒ

í•µì‹¬ ê¸°ëŠ¥:
- ë©”ëª¨ë¦¬ + íŒŒì¼ ê¸°ë°˜ ë¡œê·¸ ì €ì¥ (ìµœê·¼ 100ê°œ)
- ì‘ì—…ë³„ ì‹¤í–‰ ìƒíƒœ, ì‹œê°„, ê²°ê³¼ ê¸°ë¡
- API ì—”ë“œí¬ì¸íŠ¸ë¥¼ í†µí•œ ì¡°íšŒ
"""
import os
import json
import logging
from datetime import datetime
from typing import Dict, List, Optional, Any
from collections import deque
import pytz
import threading

logger = logging.getLogger(__name__)

# ê¸°ë³¸ ì„¤ì •
KST = pytz.timezone('Asia/Seoul')
MAX_LOG_ENTRIES = 100

# Render Disk ê²½ë¡œ ìš°ì„  ì‚¬ìš©
DISK_PATH = "/var/data"
if os.path.isdir(DISK_PATH):
    DEFAULT_LOG_PATH = os.path.join(DISK_PATH, "scheduler_logs.json")
else:
    DEFAULT_LOG_PATH = os.path.join(os.getcwd(), "scheduler_logs.json")


class SchedulerLogManager:
    """ìŠ¤ì¼€ì¤„ëŸ¬ ë¡œê·¸ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
        
        self.log_file = os.getenv("SCHEDULER_LOG_FILE", DEFAULT_LOG_PATH)
        self.logs = deque(maxlen=MAX_LOG_ENTRIES)
        self._load_logs()
        self._initialized = True
    
    def _load_logs(self):
        """íŒŒì¼ì—ì„œ ë¡œê·¸ ë¡œë“œ"""
        try:
            if os.path.exists(self.log_file):
                with open(self.log_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    logs = data.get("logs", [])
                    for log in logs[-MAX_LOG_ENTRIES:]:
                        self.logs.append(log)
                logger.info(f"Loaded {len(self.logs)} scheduler logs")
        except Exception as e:
            logger.warning(f"Failed to load scheduler logs: {e}")
    
    def _save_logs(self):
        """íŒŒì¼ì— ë¡œê·¸ ì €ì¥"""
        try:
            dir_path = os.path.dirname(self.log_file)
            if dir_path and not os.path.exists(dir_path):
                os.makedirs(dir_path, exist_ok=True)
            
            with open(self.log_file, "w", encoding="utf-8") as f:
                json.dump({
                    "updated_at": datetime.now(KST).isoformat(),
                    "logs": list(self.logs)
                }, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.warning(f"Failed to save scheduler logs: {e}")
    
    def add_log(
        self,
        job_id: str,
        job_name: str,
        status: str,  # "started", "success", "failed"
        message: str = "",
        details: Dict = None
    ) -> Dict:
        """ë¡œê·¸ ì¶”ê°€
        
        Args:
            job_id: ì‘ì—… ID (ì˜ˆ: "daily_rank_crawl")
            job_name: ì‘ì—… ì´ë¦„ (ì˜ˆ: "ìˆœìœ„ í¬ë¡¤ë§")
            status: ìƒíƒœ ("started", "success", "failed")
            message: ê²°ê³¼ ë©”ì‹œì§€
            details: ì¶”ê°€ ìƒì„¸ ì •ë³´
        """
        now = datetime.now(KST)
        
        log_entry = {
            "id": f"{job_id}_{now.strftime('%Y%m%d_%H%M%S')}",
            "job_id": job_id,
            "job_name": job_name,
            "status": status,
            "message": message,
            "details": details or {},
            "timestamp": now.isoformat(),
            "date": now.strftime("%Y-%m-%d"),
            "time": now.strftime("%H:%M:%S"),
        }
        
        self.logs.append(log_entry)
        self._save_logs()
        
        # ì½˜ì†” ë¡œê·¸ë„ ì¶œë ¥
        status_emoji = {"started": "ğŸš€", "success": "âœ…", "failed": "âŒ"}.get(status, "ğŸ“")
        logger.info(f"{status_emoji} [{job_name}] {status}: {message}")
        
        return log_entry
    
    def get_logs(
        self,
        job_id: str = None,
        status: str = None,
        limit: int = 50,
        date_from: str = None
    ) -> List[Dict]:
        """ë¡œê·¸ ì¡°íšŒ
        
        Args:
            job_id: íŠ¹ì • ì‘ì—…ë§Œ í•„í„°ë§
            status: íŠ¹ì • ìƒíƒœë§Œ í•„í„°ë§
            limit: ìµœëŒ€ ê°œìˆ˜
            date_from: ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)
        """
        results = []
        
        for log in reversed(list(self.logs)):
            if job_id and log.get("job_id") != job_id:
                continue
            if status and log.get("status") != status:
                continue
            if date_from and log.get("date", "") < date_from:
                continue
            
            results.append(log)
            if len(results) >= limit:
                break
        
        return results
    
    def get_latest_by_job(self) -> Dict[str, Dict]:
        """ê° ì‘ì—…ë³„ ìµœì‹  ë¡œê·¸ ì¡°íšŒ"""
        latest = {}
        
        for log in reversed(list(self.logs)):
            job_id = log.get("job_id")
            if job_id and job_id not in latest:
                latest[job_id] = log
        
        return latest
    
    def get_summary(self) -> Dict:
        """ë¡œê·¸ ìš”ì•½ í†µê³„"""
        total = len(self.logs)
        success = sum(1 for l in self.logs if l.get("status") == "success")
        failed = sum(1 for l in self.logs if l.get("status") == "failed")
        
        # ì˜¤ëŠ˜ ë¡œê·¸
        today = datetime.now(KST).strftime("%Y-%m-%d")
        today_logs = [l for l in self.logs if l.get("date") == today]
        
        return {
            "total_logs": total,
            "success_count": success,
            "failed_count": failed,
            "today_count": len(today_logs),
            "latest_by_job": self.get_latest_by_job()
        }
    
    def clear_old_logs(self, days: int = 7):
        """ì˜¤ë˜ëœ ë¡œê·¸ ì •ë¦¬"""
        cutoff = (datetime.now(KST) - timedelta(days=days)).strftime("%Y-%m-%d")
        old_count = len(self.logs)
        
        self.logs = deque(
            (l for l in self.logs if l.get("date", "") >= cutoff),
            maxlen=MAX_LOG_ENTRIES
        )
        
        removed = old_count - len(self.logs)
        if removed > 0:
            self._save_logs()
            logger.info(f"Removed {removed} old scheduler logs")


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_log_manager = None


def get_scheduler_log_manager() -> SchedulerLogManager:
    """ì‹±ê¸€í„´ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _log_manager
    if _log_manager is None:
        _log_manager = SchedulerLogManager()
    return _log_manager


def log_scheduler_event(
    job_id: str,
    job_name: str,
    status: str,
    message: str = "",
    details: Dict = None
) -> Dict:
    """ìŠ¤ì¼€ì¤„ëŸ¬ ì´ë²¤íŠ¸ ë¡œê¹… (ì™¸ë¶€ í˜¸ì¶œìš©)"""
    manager = get_scheduler_log_manager()
    return manager.add_log(job_id, job_name, status, message, details)


def get_scheduler_logs(
    job_id: str = None,
    status: str = None,
    limit: int = 50
) -> List[Dict]:
    """ìŠ¤ì¼€ì¤„ëŸ¬ ë¡œê·¸ ì¡°íšŒ (ì™¸ë¶€ í˜¸ì¶œìš©)"""
    manager = get_scheduler_log_manager()
    return manager.get_logs(job_id=job_id, status=status, limit=limit)


def get_scheduler_summary() -> Dict:
    """ìŠ¤ì¼€ì¤„ëŸ¬ ìš”ì•½ ì¡°íšŒ (ì™¸ë¶€ í˜¸ì¶œìš©)"""
    manager = get_scheduler_log_manager()
    return manager.get_summary()


# timedelta import ì¶”ê°€
from datetime import timedelta
