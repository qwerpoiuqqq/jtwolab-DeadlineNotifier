"""
í¬ë¡¤ë§ ì‹¤íŒ¨ ë³µêµ¬ ì„œë¹„ìŠ¤

rank_update_logsì—ì„œ ì‹¤íŒ¨í•œ ë‚ ì§œë¥¼ ì°¾ì•„ ì¬í¬ë¡¤ë§í•˜ê³ ,
ì›”ë³´ì¥ ì‹œíŠ¸ì— ëˆ„ë½ëœ ìˆœìœ„ë§Œ ì„ íƒì ìœ¼ë¡œ ì—…ë°ì´íŠ¸

í•µì‹¬ ê¸°ëŠ¥:
1. ì‹¤íŒ¨í•œ í¬ë¡¤ë§ ë‚ ì§œ ì¡°íšŒ (rank_update_logs)
2. ê³¼ê±° ë‚ ì§œ ë°ì´í„° ì¬í¬ë¡¤ë§ (Adlog)
3. ì›”ë³´ì¥ ì‹œíŠ¸ ì„ íƒì  ì—…ë°ì´íŠ¸ (ì´ë¯¸ ì±„ì›Œì§„ ì…€ ê±´ë„ˆë›°ê¸°)
"""
import os
import re
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from zoneinfo import ZoneInfo

import gspread
from google.oauth2.service_account import Credentials

logger = logging.getLogger(__name__)

KST = ZoneInfo("Asia/Seoul")

# ì‹œíŠ¸ ID
JTWOLAB_SHEET_ID = os.getenv("JTWOLAB_SHEET_ID", "1zRgtvTZ6SZF-bWiMO8qmnIhhrNVsrDxbIj3HvE8Tv3Y")
ILRYU_SHEET_ID = os.getenv("ILRYU_SHEET_ID", "1gInZEFprmb_p43SPKUZT6zjpG36AQPW9OpcsljgNtxM")


class RecoveryService:
    """í¬ë¡¤ë§ ì‹¤íŒ¨ ë³µêµ¬ ì„œë¹„ìŠ¤"""

    def __init__(self):
        self.gc = self._get_gspread_client()

    def _get_gspread_client(self) -> gspread.Client:
        """gspread í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        scopes = [
            "https://www.googleapis.com/auth/spreadsheets",
            "https://www.googleapis.com/auth/drive"
        ]

        service_account_json = os.getenv("SERVICE_ACCOUNT_JSON")
        if service_account_json:
            creds_dict = json.loads(service_account_json)
            creds = Credentials.from_service_account_info(creds_dict, scopes=scopes)
        else:
            creds_path = os.getenv("GOOGLE_APPLICATION_CREDENTIALS", "service_account.json")
            creds = Credentials.from_service_account_file(creds_path, scopes=scopes)

        return gspread.authorize(creds)

    # =========================================================================
    # 1. ì‹¤íŒ¨í•œ í¬ë¡¤ë§ ë‚ ì§œ ì¡°íšŒ
    # =========================================================================

    def get_failed_crawl_dates(self, days_back: int = 7) -> List[Dict[str, Any]]:
        """rank_update_logsì—ì„œ ì‹¤íŒ¨í•œ í¬ë¡¤ë§ ë‚ ì§œ ì¡°íšŒ

        Args:
            days_back: ì¡°íšŒí•  ê³¼ê±° ì¼ìˆ˜ (ê¸°ë³¸: 7ì¼)

        Returns:
            ì‹¤íŒ¨ ê¸°ë¡ ë¦¬ìŠ¤íŠ¸ [{date, time_slot, failed_count, message, failed_details}, ...]
        """
        try:
            from rank_snapshot_manager import RankSnapshotManager
            manager = RankSnapshotManager()
            spreadsheet = manager._get_spreadsheet()

            try:
                log_ws = spreadsheet.worksheet("rank_update_logs")
            except gspread.WorksheetNotFound:
                logger.warning("rank_update_logs íƒ­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return []

            all_values = log_ws.get_all_values()
            if len(all_values) <= 1:
                return []

            headers = all_values[0]

            # í—¤ë” ì¸ë±ìŠ¤ ì°¾ê¸°
            idx_map = {}
            for i, h in enumerate(headers):
                idx_map[h] = i

            cutoff_date = (datetime.now(KST) - timedelta(days=days_back)).strftime("%Y-%m-%d")

            failed_records = []
            for row in all_values[1:]:
                try:
                    executed_at = row[idx_map.get("executed_at", 0)] if len(row) > idx_map.get("executed_at", 0) else ""
                    failed_count_str = row[idx_map.get("failed_count", 3)] if len(row) > idx_map.get("failed_count", 3) else "0"
                    message = row[idx_map.get("message", 5)] if len(row) > idx_map.get("message", 5) else ""
                    failed_details_str = row[idx_map.get("failed_details", 6)] if len(row) > idx_map.get("failed_details", 6) else "[]"
                    time_slot = row[idx_map.get("time_slot", 1)] if len(row) > idx_map.get("time_slot", 1) else ""

                    # ë‚ ì§œ ì¶”ì¶œ (ISO í˜•ì‹ì—ì„œ)
                    if "T" in executed_at:
                        log_date = executed_at.split("T")[0]
                    else:
                        log_date = executed_at[:10] if len(executed_at) >= 10 else ""

                    # ë‚ ì§œ í•„í„°
                    if log_date < cutoff_date:
                        continue

                    # ì‹¤íŒ¨ ê±´ìˆ˜ í™•ì¸
                    failed_count = int(failed_count_str) if failed_count_str.isdigit() else 0

                    # ì‹¤íŒ¨ê°€ ìˆê±°ë‚˜ ë©”ì‹œì§€ì— 'ì‹¤íŒ¨'ê°€ í¬í•¨ëœ ê²½ìš°
                    is_failed = failed_count > 0 or "ì‹¤íŒ¨" in message or "failed" in message.lower()

                    if is_failed:
                        # failed_details JSON íŒŒì‹±
                        try:
                            failed_details = json.loads(failed_details_str) if failed_details_str else []
                        except json.JSONDecodeError:
                            failed_details = []

                        failed_records.append({
                            "date": log_date,
                            "time_slot": time_slot,
                            "executed_at": executed_at,
                            "failed_count": failed_count,
                            "message": message,
                            "failed_details": failed_details,
                        })

                except Exception as e:
                    logger.warning(f"ë¡œê·¸ í–‰ íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue

            # ë‚ ì§œ ì—­ìˆœ ì •ë ¬
            failed_records.sort(key=lambda x: x["date"], reverse=True)

            logger.info(f"ìµœê·¼ {days_back}ì¼ ì¤‘ {len(failed_records)}ê°œì˜ ì‹¤íŒ¨ ê¸°ë¡ ë°œê²¬")
            return failed_records

        except Exception as e:
            logger.error(f"ì‹¤íŒ¨ ê¸°ë¡ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []

    def get_dates_missing_in_snapshots(self, target_dates: List[str]) -> List[str]:
        """rank_snapshotsì—ì„œ ë°ì´í„°ê°€ ì—†ëŠ” ë‚ ì§œ ì°¾ê¸°

        Args:
            target_dates: í™•ì¸í•  ë‚ ì§œ ë¦¬ìŠ¤íŠ¸ ['2025-01-08', '2025-01-09', ...]

        Returns:
            ë°ì´í„°ê°€ ì—†ëŠ” ë‚ ì§œ ë¦¬ìŠ¤íŠ¸
        """
        try:
            from rank_snapshot_manager import RankSnapshotManager
            manager = RankSnapshotManager()

            # ì „ì²´ ê¸°ê°„ì˜ ë°ì´í„° ì¡°íšŒ
            if not target_dates:
                return []

            min_date = min(target_dates)
            max_date = max(target_dates)

            history = manager.get_history(date_from=min_date, date_to=max_date)

            # ë‚ ì§œë³„ ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            dates_with_data = set()
            for record in history:
                record_date = record.get("date", "")
                if record_date:
                    dates_with_data.add(record_date)

            # ë°ì´í„°ê°€ ì—†ëŠ” ë‚ ì§œ
            missing_dates = [d for d in target_dates if d not in dates_with_data]

            logger.info(f"í™•ì¸ ìš”ì²­ {len(target_dates)}ì¼ ì¤‘ {len(missing_dates)}ì¼ ë°ì´í„° ëˆ„ë½")
            return missing_dates

        except Exception as e:
            logger.error(f"ìŠ¤ëƒ…ìƒ· í™•ì¸ ì˜¤ë¥˜: {e}")
            return target_dates  # ì˜¤ë¥˜ ì‹œ ëª¨ë“  ë‚ ì§œ ë°˜í™˜

    # =========================================================================
    # 2. ê³¼ê±° ë‚ ì§œ ë°ì´í„° ì¬í¬ë¡¤ë§
    # =========================================================================

    def crawl_historical_date(self, target_date: str) -> Dict[str, Any]:
        """íŠ¹ì • ê³¼ê±° ë‚ ì§œì˜ ë°ì´í„° í¬ë¡¤ë§

        Adlog í˜ì´ì§€ì—ëŠ” ì—¬ëŸ¬ ë‚ ì§œì˜ ë°ì´í„°ê°€ í‘œì‹œë˜ë¯€ë¡œ,
        í˜„ì¬ í¬ë¡¤ë§ í›„ target_dateì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ë§Œ í•„í„°ë§

        Args:
            target_date: í¬ë¡¤ë§í•  ë‚ ì§œ (YYYY-MM-DD)

        Returns:
            í¬ë¡¤ë§ ê²°ê³¼
        """
        try:
            from rank_crawler import AdlogCrawler

            logger.info(f"ğŸ”„ {target_date} ë‚ ì§œ ë°ì´í„° ì¬í¬ë¡¤ë§ ì‹œì‘")

            crawler = AdlogCrawler()

            # ì „ì²´ í¬ë¡¤ë§ ìˆ˜í–‰ (AdlogëŠ” ì—¬ëŸ¬ ë‚ ì§œ ë°ì´í„°ë¥¼ ë™ì‹œì— ë³´ì—¬ì¤Œ)
            result = crawler.crawl_ranks(None)

            if not result.get("success"):
                return {
                    "success": False,
                    "date": target_date,
                    "message": result.get("message", "í¬ë¡¤ë§ ì‹¤íŒ¨"),
                    "data": []
                }

            # target_dateì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ë§Œ í•„í„°ë§
            all_data = result.get("data", [])
            filtered_data = [
                record for record in all_data
                if record.get("date") == target_date
            ]

            logger.info(f"âœ… {target_date}: ì´ {len(all_data)}ê±´ ì¤‘ {len(filtered_data)}ê±´ ë§¤ì¹­")

            return {
                "success": True,
                "date": target_date,
                "message": f"{len(filtered_data)}ê±´ í¬ë¡¤ë§ ì™„ë£Œ",
                "data": filtered_data,
                "total_crawled": len(all_data)
            }

        except Exception as e:
            logger.error(f"ê³¼ê±° ë‚ ì§œ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            return {
                "success": False,
                "date": target_date,
                "message": str(e),
                "data": []
            }

    # =========================================================================
    # 3. ì›”ë³´ì¥ ì‹œíŠ¸ ì„ íƒì  ì—…ë°ì´íŠ¸ (ì´ë¯¸ ì±„ì›Œì§„ ì…€ ê±´ë„ˆë›°ê¸°)
    # =========================================================================

    def update_guarantee_sheets_selective(
        self,
        rank_data: List[Dict[str, Any]],
        target_date: str
    ) -> Dict[str, Any]:
        """ì›”ë³´ì¥ ì‹œíŠ¸ì— ì„ íƒì ìœ¼ë¡œ ìˆœìœ„ ì—…ë°ì´íŠ¸

        ë‹´ë‹¹ìê°€ ì´ë¯¸ ìˆ˜ë™ìœ¼ë¡œ ì…ë ¥í•œ ê²½ìš° ê±´ë„ˆë›°ê³ ,
        ë¹„ì–´ìˆëŠ” ì…€ë§Œ ì—…ë°ì´íŠ¸

        Args:
            rank_data: ìˆœìœ„ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            target_date: ì—…ë°ì´íŠ¸í•  ë‚ ì§œ (YYYY-MM-DD)

        Returns:
            ì—…ë°ì´íŠ¸ ê²°ê³¼
        """
        results = {}

        # ë‚ ì§œ ë¬¸ìì—´ ë³€í™˜ (2025-01-08 â†’ 25. 01. 08)
        try:
            dt = datetime.strptime(target_date, "%Y-%m-%d")
            date_str = dt.strftime("%y. %m. %d")
        except ValueError:
            logger.error(f"ì˜ëª»ëœ ë‚ ì§œ í˜•ì‹: {target_date}")
            return {"success": False, "error": "Invalid date format"}

        # URL â†’ ìˆœìœ„ ë°ì´í„° ë§µ ìƒì„±
        url_to_rank = {}
        name_keyword_to_rank = {}

        for item in rank_data:
            place_url = item.get("place_url", "")
            if place_url:
                match = re.search(r'/(\d{5,})', place_url)
                if match:
                    place_id = match.group(1)
                    url_to_rank[place_id] = item

            name = item.get("client_name", "")
            keyword = item.get("keyword", "")
            if name and keyword:
                key = f"{name}|{keyword}"
                name_keyword_to_rank[key] = item

        logger.info(f"[{target_date}] {len(url_to_rank)} URL ë§¤í•‘, {len(name_keyword_to_rank)} ì´ë¦„+í‚¤ì›Œë“œ ë§¤í•‘")

        # ê° ì‹œíŠ¸ ì—…ë°ì´íŠ¸
        sheets_config = [
            ("jtwolab", JTWOLAB_SHEET_ID),
            ("ilryu", ILRYU_SHEET_ID),
        ]

        for sheet_name, sheet_id in sheets_config:
            try:
                result = self._update_sheet_selective(
                    sheet_id,
                    sheet_name,
                    url_to_rank,
                    name_keyword_to_rank,
                    date_str,
                    target_date
                )
                results[sheet_name] = result
            except Exception as e:
                logger.error(f"{sheet_name} ì‹œíŠ¸ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
                results[sheet_name] = {"success": False, "error": str(e)}

        total_updated = sum(
            r.get("updated", 0) for r in results.values() if isinstance(r, dict)
        )
        total_skipped_existing = sum(
            r.get("skipped_existing", 0) for r in results.values() if isinstance(r, dict)
        )

        return {
            "success": True,
            "date": target_date,
            "results": results,
            "total_updated": total_updated,
            "total_skipped_existing": total_skipped_existing,
        }

    def _update_sheet_selective(
        self,
        sheet_id: str,
        sheet_name: str,
        url_to_rank: Dict[str, Dict],
        name_keyword_to_rank: Dict[str, Dict],
        date_str: str,
        target_date: str
    ) -> Dict[str, Any]:
        """ë‹¨ì¼ ì‹œíŠ¸ ì„ íƒì  ì—…ë°ì´íŠ¸ (ì´ë¯¸ ì±„ì›Œì§„ ë‚ ì§œëŠ” ê±´ë„ˆë›°ê¸°)"""
        try:
            spreadsheet = self.gc.open_by_key(sheet_id)
            worksheet = spreadsheet.worksheet("ë³´ì¥ê±´")

            all_values = worksheet.get_all_values()

            # í—¤ë” í–‰ (ë³´í†µ 2í–‰)
            header_row_idx = 1
            if len(all_values) <= header_row_idx:
                return {"success": False, "error": "ì‹œíŠ¸ì— ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤"}

            headers = all_values[header_row_idx]

            # í—¤ë” ë§¤í•‘
            col_map = {}
            daily_start_idx = -1

            for idx, header in enumerate(headers):
                h = str(header).strip()

                if "ì‘ì—…" in h and "ì—¬ë¶€" in h:
                    col_map["status"] = idx
                elif "ìƒí˜¸" in h or ("í”Œë ˆì´ìŠ¤" in h and "ìë™ì™„ì„±" in h):
                    col_map["business_name"] = idx
                elif "í‚¤ì›Œë“œ" in h and "ë©”ì¸" in h:
                    col_map["keyword"] = idx
                elif "ìƒí’ˆ" in h:
                    col_map["product"] = idx
                elif "URL" in h.upper():
                    col_map["url"] = idx
                elif "ë³´ì¥" in h and "ìˆœìœ„" in h:
                    col_map["guarantee_rank"] = idx

                if daily_start_idx == -1:
                    if h == "1" or h == "1ì¼":
                        daily_start_idx = idx

            # í•„ìˆ˜ ì»¬ëŸ¼ ì²´í¬
            required_cols = ["business_name", "status", "product", "guarantee_rank"]
            missing = [c for c in required_cols if c not in col_map]
            if missing:
                return {"success": False, "error": f"í•„ìˆ˜ í—¤ë” ëˆ„ë½: {missing}"}

            if daily_start_idx == -1:
                daily_start_idx = 17

            VALID_STATUSES = ["ì§„í–‰ì¤‘", "í›„ë¶ˆ", "ë°˜ë¶ˆ"]
            MAX_DAILY_COUNT = 25

            updates = []
            matched_count = 0
            skipped_existing = 0  # ì´ë¯¸ ì±„ì›Œì§„ ì…€ (ë‹´ë‹¹ìê°€ ì…ë ¥)
            skipped_rank = 0  # ë³´ì¥ ìˆœìœ„ ì´ˆê³¼

            start_row = header_row_idx + 1
            for i, row in enumerate(all_values[start_row:]):
                row_num = start_row + i + 1

                def get_val(col_name):
                    idx = col_map.get(col_name)
                    if idx is not None and idx < len(row):
                        return row[idx].strip()
                    return ""

                # í•„í„°ë§
                status = get_val("status")
                if status not in VALID_STATUSES:
                    continue

                product = get_val("product")
                if "í”Œë ˆì´ìŠ¤" not in product:
                    continue

                # ë§¤ì¹­ ë°ì´í„° ì°¾ê¸°
                business_name = get_val("business_name")
                keyword = get_val("keyword")
                url = get_val("url")

                guarantee_rank_str = get_val("guarantee_rank")
                guarantee_rank = None
                try:
                    guarantee_rank = int(re.sub(r'[^\d]', '', guarantee_rank_str))
                except ValueError:
                    continue

                if not guarantee_rank:
                    continue

                # ë§¤ì¹­ ì‹œë„
                rank_item = None
                if url:
                    match = re.search(r'/(\d{5,})', url)
                    if match:
                        place_id = match.group(1)
                        rank_item = url_to_rank.get(place_id)

                if not rank_item and business_name and keyword:
                    key = f"{business_name}|{keyword}"
                    rank_item = name_keyword_to_rank.get(key)

                if not rank_item:
                    continue

                # ìˆœìœ„ ê°’ í™•ì¸
                raw_rank = rank_item.get("rank")
                if raw_rank is None:
                    continue

                try:
                    current_rank = int(str(raw_rank).replace('ìœ„', '').strip())
                except ValueError:
                    continue

                # ë³´ì¥ ìˆœìœ„ ì´ë‚´ í™•ì¸
                if current_rank > guarantee_rank:
                    skipped_rank += 1
                    continue

                # ====== í•µì‹¬: í•´ë‹¹ ë‚ ì§œê°€ ì´ë¯¸ ì±„ì›Œì ¸ ìˆëŠ”ì§€ í™•ì¸ ======
                target_col_idx = -1
                date_already_exists = False

                for offset in range(MAX_DAILY_COUNT):
                    check_idx = daily_start_idx + offset

                    cell_value = ""
                    if check_idx < len(row):
                        cell_value = row[check_idx].strip()

                    if not cell_value:
                        # ë¹ˆ ì…€ ë°œê²¬ - ì—¬ê¸°ì— ê¸°ì… ê°€ëŠ¥
                        if target_col_idx == -1:
                            target_col_idx = check_idx
                        continue

                    # í•´ë‹¹ ë‚ ì§œê°€ ì´ë¯¸ ê¸°ë¡ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                    # í˜•ì‹: "25. 01. 08\n3ë“±" ë˜ëŠ” "25. 01. 08 3ë“±"
                    if date_str in cell_value:
                        date_already_exists = True
                        break

                # ì´ë¯¸ í•´ë‹¹ ë‚ ì§œ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ê±´ë„ˆë›°ê¸° (ë‹´ë‹¹ìê°€ ì…ë ¥)
                if date_already_exists:
                    skipped_existing += 1
                    logger.debug(f"[{sheet_name}] {business_name}: {target_date} ì´ë¯¸ ê¸°ë¡ë¨ (ê±´ë„ˆë›°ê¸°)")
                    continue

                # ë¹ˆ ì…€ì´ ì—†ìœ¼ë©´ ë§ˆì§€ë§‰ ìœ„ì¹˜ì— ì¶”ê°€
                if target_col_idx == -1:
                    target_col_idx = daily_start_idx + MAX_DAILY_COUNT

                # ì—…ë°ì´íŠ¸ ì¶”ê°€
                cell_value = f"{date_str}\n{current_rank}ë“±"

                updates.append({
                    "row": row_num,
                    "col": target_col_idx + 1,
                    "value": cell_value,
                    "business_name": business_name,
                })
                matched_count += 1

            # ë°°ì¹˜ ì—…ë°ì´íŠ¸
            if updates:
                cells_to_update = []
                for update in updates:
                    cell = gspread.Cell(update["row"], update["col"], update["value"])
                    cells_to_update.append(cell)

                worksheet.update_cells(cells_to_update, value_input_option='USER_ENTERED')
                logger.info(f"[{sheet_name}] {len(updates)}ê°œ ì…€ ì—…ë°ì´íŠ¸ ì™„ë£Œ")

            return {
                "success": True,
                "matched": matched_count,
                "updated": len(updates),
                "skipped_existing": skipped_existing,
                "skipped_rank": skipped_rank,
            }

        except gspread.WorksheetNotFound:
            return {"success": False, "error": "ë³´ì¥ê±´ íƒ­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
        except Exception as e:
            logger.error(f"[{sheet_name}] ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {"success": False, "error": str(e)}

    # =========================================================================
    # 4. í†µí•© ë³µêµ¬ ê¸°ëŠ¥
    # =========================================================================

    def recover_failed_crawls(self, days_back: int = 7) -> Dict[str, Any]:
        """ì‹¤íŒ¨í•œ í¬ë¡¤ë§ ë³µêµ¬ (ì „ì²´ í”„ë¡œì„¸ìŠ¤)

        1. ì‹¤íŒ¨í•œ ë‚ ì§œ ì¡°íšŒ
        2. rank_snapshotsì—ì„œ ëˆ„ë½ í™•ì¸
        3. ëˆ„ë½ëœ ë‚ ì§œ ì¬í¬ë¡¤ë§
        4. ì›”ë³´ì¥ ì‹œíŠ¸ ì„ íƒì  ì—…ë°ì´íŠ¸

        Args:
            days_back: ì¡°íšŒí•  ê³¼ê±° ì¼ìˆ˜

        Returns:
            ë³µêµ¬ ê²°ê³¼
        """
        logger.info(f"ğŸ”„ í¬ë¡¤ë§ ì‹¤íŒ¨ ë³µêµ¬ ì‹œì‘ (ìµœê·¼ {days_back}ì¼)")

        result = {
            "failed_dates_found": [],
            "missing_dates": [],
            "crawl_results": [],
            "update_results": [],
            "summary": {},
        }

        # 1. ì‹¤íŒ¨í•œ ë‚ ì§œ ì¡°íšŒ
        failed_records = self.get_failed_crawl_dates(days_back)
        failed_dates = list(set([r["date"] for r in failed_records]))
        result["failed_dates_found"] = failed_dates

        if not failed_dates:
            logger.info("âœ… ì‹¤íŒ¨í•œ í¬ë¡¤ë§ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤")
            result["summary"] = {"status": "no_failures", "message": "ì‹¤íŒ¨ ê¸°ë¡ ì—†ìŒ"}
            return result

        logger.info(f"ğŸ“‹ ì‹¤íŒ¨í•œ ë‚ ì§œ: {failed_dates}")

        # 2. rank_snapshotsì—ì„œ ì‹¤ì œë¡œ ëˆ„ë½ëœ ë‚ ì§œ í™•ì¸
        missing_dates = self.get_dates_missing_in_snapshots(failed_dates)
        result["missing_dates"] = missing_dates

        if not missing_dates:
            logger.info("âœ… ëª¨ë“  ì‹¤íŒ¨ ë‚ ì§œì˜ ë°ì´í„°ê°€ ì´ë¯¸ ë³µêµ¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
            result["summary"] = {"status": "already_recovered", "message": "ì´ë¯¸ ë³µêµ¬ë¨"}
            return result

        logger.info(f"ğŸ“‹ ë°ì´í„° ëˆ„ë½ ë‚ ì§œ: {missing_dates}")

        # 3. ëˆ„ë½ëœ ë‚ ì§œ ì¬í¬ë¡¤ë§
        total_crawled = 0
        for target_date in missing_dates:
            crawl_result = self.crawl_historical_date(target_date)
            result["crawl_results"].append(crawl_result)

            if crawl_result.get("success") and crawl_result.get("data"):
                total_crawled += len(crawl_result["data"])

                # 4. ì›”ë³´ì¥ ì‹œíŠ¸ ì—…ë°ì´íŠ¸
                update_result = self.update_guarantee_sheets_selective(
                    crawl_result["data"],
                    target_date
                )
                result["update_results"].append(update_result)

        # ìš”ì•½
        total_updated = sum(
            r.get("total_updated", 0) for r in result["update_results"]
        )
        total_skipped = sum(
            r.get("total_skipped_existing", 0) for r in result["update_results"]
        )

        result["summary"] = {
            "status": "completed",
            "failed_dates_count": len(failed_dates),
            "missing_dates_count": len(missing_dates),
            "total_crawled": total_crawled,
            "total_updated": total_updated,
            "total_skipped_existing": total_skipped,
            "message": f"{len(missing_dates)}ê°œ ë‚ ì§œ ë³µêµ¬, {total_updated}ê°œ ì…€ ì—…ë°ì´íŠ¸, {total_skipped}ê°œ ê±´ë„ˆëœ€ (ì´ë¯¸ ì…ë ¥ë¨)"
        }

        logger.info(f"âœ… ë³µêµ¬ ì™„ë£Œ: {result['summary']['message']}")

        return result

    def recover_specific_date(self, target_date: str) -> Dict[str, Any]:
        """íŠ¹ì • ë‚ ì§œë§Œ ë³µêµ¬

        Args:
            target_date: ë³µêµ¬í•  ë‚ ì§œ (YYYY-MM-DD)

        Returns:
            ë³µêµ¬ ê²°ê³¼
        """
        logger.info(f"ğŸ”„ {target_date} ë‚ ì§œ ë³µêµ¬ ì‹œì‘")

        result = {
            "date": target_date,
            "crawl_result": None,
            "update_result": None,
            "summary": {},
        }

        # 1. í¬ë¡¤ë§
        crawl_result = self.crawl_historical_date(target_date)
        result["crawl_result"] = crawl_result

        if not crawl_result.get("success"):
            result["summary"] = {
                "status": "crawl_failed",
                "message": crawl_result.get("message", "í¬ë¡¤ë§ ì‹¤íŒ¨")
            }
            return result

        if not crawl_result.get("data"):
            result["summary"] = {
                "status": "no_data",
                "message": f"{target_date} ë‚ ì§œì˜ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            }
            return result

        # 2. ì‹œíŠ¸ ì—…ë°ì´íŠ¸
        update_result = self.update_guarantee_sheets_selective(
            crawl_result["data"],
            target_date
        )
        result["update_result"] = update_result

        result["summary"] = {
            "status": "completed",
            "crawled_count": len(crawl_result["data"]),
            "updated_count": update_result.get("total_updated", 0),
            "skipped_existing": update_result.get("total_skipped_existing", 0),
            "message": f"{len(crawl_result['data'])}ê±´ í¬ë¡¤ë§, {update_result.get('total_updated', 0)}ê±´ ì—…ë°ì´íŠ¸, {update_result.get('total_skipped_existing', 0)}ê±´ ê±´ë„ˆëœ€"
        }

        logger.info(f"âœ… {target_date} ë³µêµ¬ ì™„ë£Œ: {result['summary']['message']}")

        return result


# =============================================================================
# ì™¸ë¶€ í˜¸ì¶œìš© í•¨ìˆ˜
# =============================================================================

def get_failed_crawl_dates(days_back: int = 7) -> List[Dict[str, Any]]:
    """ì‹¤íŒ¨í•œ í¬ë¡¤ë§ ë‚ ì§œ ì¡°íšŒ (ì™¸ë¶€ í˜¸ì¶œìš©)"""
    service = RecoveryService()
    return service.get_failed_crawl_dates(days_back)


def recover_failed_crawls(days_back: int = 7) -> Dict[str, Any]:
    """ì‹¤íŒ¨í•œ í¬ë¡¤ë§ ë³µêµ¬ (ì™¸ë¶€ í˜¸ì¶œìš©)"""
    service = RecoveryService()
    return service.recover_failed_crawls(days_back)


def recover_specific_date(target_date: str) -> Dict[str, Any]:
    """íŠ¹ì • ë‚ ì§œ ë³µêµ¬ (ì™¸ë¶€ í˜¸ì¶œìš©)"""
    service = RecoveryService()
    return service.recover_specific_date(target_date)
