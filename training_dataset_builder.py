"""
í•™ìŠµìš© ë°ì´í„°ì…‹ ë¹Œë” ëª¨ë“ˆ
rank_snapshotsì™€ worklog_cacheë¥¼ ì¡°ì¸í•˜ì—¬ í•™ìŠµìš© ë°ì´í„°ì…‹ ìƒì„±

í•µì‹¬ ê¸°ëŠ¥:
- business_name + date ê¸°ë°˜ ì¡°ì¸
- N2 delta ê³„ì‚° (ì‘ì—… ì‹œì‘ì¼ ê¸°ì¤€ 3ì¼ì§¸, fallback: 4/2/5)
- ë ˆì‹œí”¼ í†µê³„ ìƒì„± (ë‹¨ì¼/ì¡°í•©ë³„)
"""
import os
import json
import hashlib
import logging
from datetime import datetime, date, timedelta
from typing import Dict, List, Optional, Any, Tuple
from collections import defaultdict
import pytz

logger = logging.getLogger(__name__)

# ê¸°ë³¸ ì„¤ì •
KST = pytz.timezone('Asia/Seoul')

# Render Disk ê²½ë¡œ ìš°ì„  ì‚¬ìš©
DISK_PATH = "/var/data"
if os.path.isdir(DISK_PATH):
    DEFAULT_TRAINING_PATH = os.path.join(DISK_PATH, "training_rows.json")
    DEFAULT_RECIPE_PATH = os.path.join(DISK_PATH, "recipe_stats.json")
else:
    DEFAULT_TRAINING_PATH = os.path.join(os.getcwd(), "training_rows.json")
    DEFAULT_RECIPE_PATH = os.path.join(os.getcwd(), "recipe_stats.json")


def generate_tasks_hash(tasks: List[str]) -> str:
    """ì‘ì—… ëª©ë¡ì˜ canonical í•´ì‹œ ìƒì„±"""
    sorted_tasks = sorted(set(tasks))
    canonical = "|".join(sorted_tasks)
    return hashlib.sha1(canonical.encode()).hexdigest()[:12]


def get_n2_at_day(snapshots: List[Dict], start_date: date, target_day: int) -> Optional[Dict]:
    """ì‘ì—… ì‹œì‘ì¼ ê¸°ì¤€ Nì¼ì§¸ ìŠ¤ëƒ…ìƒ· ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    
    Args:
        snapshots: í•´ë‹¹ ì—…ì²´ì˜ ìŠ¤ëƒ…ìƒ· ë¦¬ìŠ¤íŠ¸
        start_date: ì‘ì—… ì‹œì‘ì¼
        target_day: ëª©í‘œ ì¼ì°¨ (1=ì‹œì‘ì¼, 3=3ì¼ì§¸)
        
    Returns:
        í•´ë‹¹ ì¼ìì˜ ìŠ¤ëƒ…ìƒ· ë°ì´í„° ë˜ëŠ” None
    """
    target_date = start_date + timedelta(days=target_day - 1)
    target_str = target_date.isoformat()
    
    for snap in snapshots:
        if snap.get("date") == target_str:
            return snap
    
    return None


def calculate_n2_delta(
    snapshots: List[Dict], 
    start_date: date
) -> Tuple[Optional[float], int, Optional[float], Optional[float]]:
    """N2 delta ê³„ì‚° (3ì¼ì§¸ ê¸°ì¤€, fallback: 4/2/5)
    
    Returns:
        (delta, day_used, start_n2, end_n2)
    """
    # ì‹œì‘ì¼ì˜ N2
    start_snap = get_n2_at_day(snapshots, start_date, 1)
    if not start_snap:
        return None, 0, None, None
    
    start_n2 = start_snap.get("n2_score")
    if start_n2 is None:
        return None, 0, None, None
    
    # 3ì¼ì§¸ ìš°ì„ , ì—†ìœ¼ë©´ 4/2/5 ìˆœ
    for target_day in [3, 4, 2, 5]:
        end_snap = get_n2_at_day(snapshots, start_date, target_day)
        if end_snap and end_snap.get("n2_score") is not None:
            end_n2 = end_snap.get("n2_score")
            delta = round(end_n2 - start_n2, 6)
            return delta, target_day, start_n2, end_n2
    
    return None, 0, start_n2, None


class TrainingDatasetBuilder:
    """í•™ìŠµìš© ë°ì´í„°ì…‹ ë¹Œë” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.training_rows = []
        self.recipe_stats = {}
    
    def build_training_rows(self, weeks: int = 3) -> List[Dict]:
        """training_rows ìƒì„±
        
        Args:
            weeks: ë¶„ì„ ê¸°ê°„ (ì£¼)
            
        Returns:
            training_rows ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"ğŸ”„ Training rows ìƒì„± ì‹œì‘ (ìµœê·¼ {weeks}ì£¼)")
        
        try:
            from worklog_cache import get_worklog_cache
            from rank_snapshot_manager import RankSnapshotManager
            from guarantee_manager import GuaranteeManager
            
            # ìºì‹œ ë¡œë“œ
            worklog_cache = get_worklog_cache()
            if not worklog_cache.is_cache_valid():
                logger.warning("Worklog ìºì‹œê°€ ìœ íš¨í•˜ì§€ ì•ŠìŒ - ê°±ì‹  í•„ìš”")
            
            # ìŠ¤ëƒ…ìƒ· íˆìŠ¤í† ë¦¬ ë¡œë“œ
            snapshot_manager = RankSnapshotManager()
            days = weeks * 7
            all_snapshots = snapshot_manager.get_history(days=days)
            logger.info(f"ğŸ“Š ìŠ¤ëƒ…ìƒ· {len(all_snapshots)}ê°œ ë¡œë“œ")
            
            # ë³´ì¥ê±´ ë°ì´í„° ë¡œë“œ (ì‘ì—… ì‹œì‘ì¼ ì¡°íšŒìš©)
            gm = GuaranteeManager()
            guarantee_items = gm.get_items()
            guarantee_map = {item.get("business_name"): item for item in guarantee_items}
            
            # ì—…ì²´ë³„ ìŠ¤ëƒ…ìƒ· ê·¸ë£¹í•‘
            snapshots_by_biz = defaultdict(list)
            for snap in all_snapshots:
                biz = snap.get("client_name") or snap.get("business_name")
                if biz:
                    snapshots_by_biz[biz].append(snap)
            
            # ê° ìŠ¤ëƒ…ìƒ·ì— ëŒ€í•´ training row ìƒì„±
            training_rows = []
            processed_keys = set()
            
            for biz_name, biz_snapshots in snapshots_by_biz.items():
                # ë³´ì¥ê±´ì—ì„œ ì‘ì—… ì‹œì‘ì¼ ì¡°íšŒ
                guarantee = guarantee_map.get(biz_name, {})
                start_date_str = guarantee.get("start_date")
                
                if start_date_str:
                    try:
                        start_date = date.fromisoformat(start_date_str)
                    except:
                        start_date = None
                else:
                    start_date = None
                
                # ìŠ¤ëƒ…ìƒ·ì„ ë‚ ì§œìˆœ ì •ë ¬
                biz_snapshots.sort(key=lambda x: x.get("date", ""))
                
                for snap in biz_snapshots:
                    snap_date_str = snap.get("date")
                    if not snap_date_str:
                        continue
                    
                    try:
                        snap_date = date.fromisoformat(snap_date_str)
                    except:
                        continue
                    
                    # ì¤‘ë³µ í‚¤ ì²´í¬
                    unique_key = f"{biz_name}|{snap_date_str}"
                    if unique_key in processed_keys:
                        continue
                    processed_keys.add(unique_key)
                    
                    # í•´ë‹¹ ë‚ ì§œì˜ í™œì„± ì‘ì—… ì¡°íšŒ
                    active_tasks = worklog_cache.get_active_tasks_on_date(biz_name, snap_date)
                    task_names = [t.get("task_name", "") for t in active_tasks if t.get("task_name")]
                    task_totals = worklog_cache.get_task_totals_on_date(biz_name, snap_date)
                    
                    # N2 delta ê³„ì‚°
                    n2_delta, day_used, start_n2, end_n2 = None, 0, None, None
                    if start_date:
                        n2_delta, day_used, start_n2, end_n2 = calculate_n2_delta(
                            biz_snapshots, start_date
                        )
                    
                    # Training row ìƒì„±
                    row = {
                        "date": snap_date_str,
                        "time_slot": snap.get("time_slot", ""),
                        "business_name": biz_name,
                        "keyword": snap.get("keyword", ""),
                        "place_url": snap.get("place_url", ""),
                        "company": snap.get("group") or guarantee.get("company", ""),
                        "n2_score": snap.get("n2_score"),
                        "n2_delta_3d": n2_delta,
                        "delta_day_used": day_used,
                        "start_n2": start_n2,
                        "rank": snap.get("rank"),
                        "saves": snap.get("saves"),
                        "blog_reviews": snap.get("blog_reviews"),
                        "visitor_reviews": snap.get("visitor_reviews"),
                        "tasks_active": task_names,
                        "tasks_hash": generate_tasks_hash(task_names) if task_names else "",
                        "task_totals": task_totals,
                        "tasks_count": len(task_names),
                    }
                    training_rows.append(row)
            
            self.training_rows = training_rows
            logger.info(f"âœ… Training rows {len(training_rows)}ê°œ ìƒì„± ì™„ë£Œ")
            return training_rows
            
        except Exception as e:
            logger.error(f"âŒ Training rows ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return []
    
    def build_recipe_stats(self, training_rows: List[Dict] = None) -> Dict:
        """ë ˆì‹œí”¼ í†µê³„ ìƒì„±
        
        Args:
            training_rows: training rows (Noneì´ë©´ self.training_rows ì‚¬ìš©)
            
        Returns:
            ë ˆì‹œí”¼ í†µê³„
        """
        if training_rows is None:
            training_rows = self.training_rows
        
        if not training_rows:
            return {}
        
        logger.info(f"ğŸ”„ ë ˆì‹œí”¼ í†µê³„ ìƒì„± ({len(training_rows)}ê°œ í–‰)")
        
        # ë‹¨ì¼ ì‘ì—…ë³„ í†µê³„
        single_task_stats = defaultdict(lambda: {
            "deltas": [],
            "count": 0,
            "up": 0,
            "down": 0,
            "stable": 0
        })
        
        # ì¡°í•©ë³„ í†µê³„
        combo_stats = defaultdict(lambda: {
            "deltas": [],
            "count": 0,
            "up": 0,
            "down": 0,
            "stable": 0
        })
        
        for row in training_rows:
            delta = row.get("n2_delta_3d")
            tasks = row.get("tasks_active", [])
            
            if delta is None or not tasks:
                continue
            
            # íŠ¸ë Œë“œ íŒì •
            if delta > 0.005:
                trend = "up"
            elif delta < -0.005:
                trend = "down"
            else:
                trend = "stable"
            
            # ë‹¨ì¼ ì‘ì—… í†µê³„
            for task in tasks:
                single_task_stats[task]["deltas"].append(delta)
                single_task_stats[task]["count"] += 1
                single_task_stats[task][trend] += 1
            
            # ì¡°í•© í†µê³„ (2~3ê°œ ì¡°í•©)
            tasks_sorted = sorted(set(tasks))
            if len(tasks_sorted) >= 2:
                # 2ê°œ ì¡°í•©
                for i in range(len(tasks_sorted)):
                    for j in range(i + 1, len(tasks_sorted)):
                        combo = f"{tasks_sorted[i]}+{tasks_sorted[j]}"
                        combo_stats[combo]["deltas"].append(delta)
                        combo_stats[combo]["count"] += 1
                        combo_stats[combo][trend] += 1
            
            # ì „ì²´ ì¡°í•© (tasks_hash ê¸°ì¤€)
            if tasks_sorted:
                full_combo = "+".join(tasks_sorted)
                combo_stats[full_combo]["deltas"].append(delta)
                combo_stats[full_combo]["count"] += 1
                combo_stats[full_combo][trend] += 1
        
        # í†µê³„ ê³„ì‚°
        def calc_stats(stat_dict: Dict) -> List[Dict]:
            results = []
            for name, data in stat_dict.items():
                deltas = data["deltas"]
                if not deltas:
                    continue
                
                avg_delta = sum(deltas) / len(deltas)
                count = data["count"]
                up_rate = data["up"] / count if count > 0 else 0
                
                results.append({
                    "name": name,
                    "avg_delta": round(avg_delta, 6),
                    "count": count,
                    "up_count": data["up"],
                    "down_count": data["down"],
                    "stable_count": data["stable"],
                    "up_rate": round(up_rate, 4),
                })
            
            # avg_delta ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
            results.sort(key=lambda x: x["avg_delta"], reverse=True)
            return results
        
        single_results = calc_stats(single_task_stats)
        combo_results = calc_stats(combo_stats)
        
        # ìƒìœ„ 20ê°œ ë ˆì‹œí”¼
        top_recipes = combo_results[:20] if len(combo_results) >= 20 else combo_results
        
        self.recipe_stats = {
            "generated_at": datetime.now(KST).isoformat(),
            "training_rows_count": len(training_rows),
            "single_task_stats": single_results,
            "combo_stats": combo_results[:50],  # ìƒìœ„ 50ê°œë§Œ
            "top_recipes": top_recipes,
            "summary": {
                "total_single_tasks": len(single_results),
                "total_combos": len(combo_results),
                "avg_delta_all": round(
                    sum(r["avg_delta"] for r in single_results) / len(single_results), 6
                ) if single_results else 0
            }
        }
        
        logger.info(f"âœ… ë ˆì‹œí”¼ í†µê³„ ìƒì„± ì™„ë£Œ - ë‹¨ì¼:{len(single_results)}, ì¡°í•©:{len(combo_results)}")
        return self.recipe_stats
    
    def save_results(
        self, 
        training_path: str = None,
        recipe_path: str = None,
        save_to_sheets: bool = True
    ) -> Dict:
        """ê²°ê³¼ ì €ì¥ (JSON + Google Sheets)
        
        Args:
            save_to_sheets: Trueë©´ Google Sheetsì—ë„ ë°±ì—…
            
        Returns:
            ì €ì¥ ê²°ê³¼ {"success": bool, ...}
        """
        training_path = training_path or os.getenv("TRAINING_ROWS_FILE", DEFAULT_TRAINING_PATH)
        recipe_path = recipe_path or os.getenv("RECIPE_STATS_FILE", DEFAULT_RECIPE_PATH)
        
        result = {
            "success": True,
            "training_rows_count": len(self.training_rows),
            "json_saved": False,
            "sheets_saved": False
        }
        
        # 1. JSON íŒŒì¼ ì €ì¥
        try:
            for path in [training_path, recipe_path]:
                dir_path = os.path.dirname(path)
                if dir_path and not os.path.exists(dir_path):
                    os.makedirs(dir_path, exist_ok=True)
            
            with open(training_path, "w", encoding="utf-8") as f:
                json.dump({
                    "generated_at": datetime.now(KST).isoformat(),
                    "count": len(self.training_rows),
                    "rows": self.training_rows
                }, f, ensure_ascii=False, indent=2)
            
            with open(recipe_path, "w", encoding="utf-8") as f:
                json.dump(self.recipe_stats, f, ensure_ascii=False, indent=2)
            
            result["json_saved"] = True
            result["training_rows_path"] = training_path
            result["recipe_stats_path"] = recipe_path
            logger.info(f"âœ… JSON ì €ì¥ ì™„ë£Œ - {training_path}")
            
        except Exception as e:
            logger.error(f"âŒ JSON ì €ì¥ ì‹¤íŒ¨: {e}")
            result["json_error"] = str(e)
        
        # 2. Google Sheets ë°±ì—…
        if save_to_sheets and self.training_rows:
            try:
                sheets_result = self._save_to_google_sheets()
                result["sheets_saved"] = sheets_result.get("success", False)
                result["sheets_result"] = sheets_result
            except Exception as e:
                logger.error(f"âŒ Sheets ë°±ì—… ì‹¤íŒ¨: {e}")
                result["sheets_error"] = str(e)
        
        return result
    
    def _save_to_google_sheets(self) -> Dict:
        """Google Sheetsì— í•™ìŠµ ë°ì´í„° ë°±ì—…
        
        ì›”ë³´ì¥ ìˆœìœ„ DB ì‹œíŠ¸ì— training_rowsì™€ recipe_stats íƒ­ ìƒì„±/ê°±ì‹ 
        """
        import gspread
        from google.oauth2.service_account import Credentials
        import os
        import json as json_module
        
        logger.info("ğŸ“Š Google Sheets ë°±ì—… ì‹œì‘...")
        
        # ì¸ì¦ ì„¤ì •
        scopes = [
            "https://www.googleapis.com/auth/spreadsheets",
            "https://www.googleapis.com/auth/drive"
        ]
        
        creds = None
        json_str = os.getenv("SERVICE_ACCOUNT_JSON", "")
        if json_str:
            import io
            creds = Credentials.from_service_account_info(
                json_module.loads(json_str), scopes=scopes
            )
        else:
            creds_path = os.getenv("GOOGLE_APPLICATION_CREDENTIALS", "service_account.json")
            if os.path.exists(creds_path):
                creds = Credentials.from_service_account_file(creds_path, scopes=scopes)
        
        if not creds:
            return {"success": False, "error": "ì¸ì¦ ì •ë³´ ì—†ìŒ"}
        
        client = gspread.authorize(creds)
        
        # ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ì—´ê¸° (RANK_SHEET_ID ë˜ëŠ” JTWOLAB_SHEET_ID)
        sheet_id = os.getenv("RANK_SHEET_ID") or os.getenv(
            "JTWOLAB_SHEET_ID", "1zRgtvTZ6SZF-bWiMO8qmnIhhrNVsrDxbIj3HvE8Tv3Y"
        )
        ss = client.open_by_key(sheet_id)
        
        result = {"success": True, "training_rows": 0, "recipe_stats": 0}
        
        # === training_rows íƒ­ ì €ì¥ ===
        try:
            tab_name = "training_rows"
            try:
                ws = ss.worksheet(tab_name)
                ws.clear()
            except gspread.WorksheetNotFound:
                ws = ss.add_worksheet(title=tab_name, rows=1000, cols=20)
            
            # í—¤ë”
            headers = [
                "date", "time_slot", "business_name", "keyword", "company",
                "n2_score", "n2_delta_3d", "delta_day_used", "start_n2",
                "rank", "saves", "blog_reviews", "visitor_reviews",
                "tasks_active", "tasks_hash", "task_totals", "tasks_count"
            ]
            
            # ë°ì´í„° í–‰ ìƒì„±
            rows = [headers]
            for row in self.training_rows[:500]:  # ìµœëŒ€ 500í–‰
                rows.append([
                    row.get("date", ""),
                    row.get("time_slot", ""),
                    row.get("business_name", ""),
                    row.get("keyword", ""),
                    row.get("company", ""),
                    row.get("n2_score") or "",
                    row.get("n2_delta_3d") or "",
                    row.get("delta_day_used") or "",
                    row.get("start_n2") or "",
                    row.get("rank") or "",
                    row.get("saves") or "",
                    row.get("blog_reviews") or "",
                    row.get("visitor_reviews") or "",
                    "|".join(row.get("tasks_active", [])),
                    row.get("tasks_hash", ""),
                    json_module.dumps(row.get("task_totals", {}), ensure_ascii=False),
                    row.get("tasks_count") or 0
                ])
            
            ws.update(rows, value_input_option="USER_ENTERED")
            result["training_rows"] = len(rows) - 1
            logger.info(f"  âœ… training_rows íƒ­: {len(rows)-1}í–‰ ì €ì¥")
            
        except Exception as e:
            logger.error(f"  âŒ training_rows ì €ì¥ ì‹¤íŒ¨: {e}")
            result["training_rows_error"] = str(e)
        
        # === recipe_stats íƒ­ ì €ì¥ ===
        try:
            tab_name = "recipe_stats"
            try:
                ws = ss.worksheet(tab_name)
                ws.clear()
            except gspread.WorksheetNotFound:
                ws = ss.add_worksheet(title=tab_name, rows=200, cols=10)
            
            # í—¤ë”
            headers = ["recipe_name", "avg_delta", "count", "up_count", "down_count", "stable_count", "up_rate"]
            
            # top_recipes ì €ì¥
            rows = [headers]
            top_recipes = self.recipe_stats.get("top_recipes", [])
            for recipe in top_recipes:
                rows.append([
                    recipe.get("name", ""),
                    recipe.get("avg_delta", 0),
                    recipe.get("count", 0),
                    recipe.get("up_count", 0),
                    recipe.get("down_count", 0),
                    recipe.get("stable_count", 0),
                    recipe.get("up_rate", 0)
                ])
            
            ws.update(rows, value_input_option="USER_ENTERED")
            result["recipe_stats"] = len(rows) - 1
            logger.info(f"  âœ… recipe_stats íƒ­: {len(rows)-1}í–‰ ì €ì¥")
            
        except Exception as e:
            logger.error(f"  âŒ recipe_stats ì €ì¥ ì‹¤íŒ¨: {e}")
            result["recipe_stats_error"] = str(e)
        
        logger.info(f"âœ… Google Sheets ë°±ì—… ì™„ë£Œ - training:{result['training_rows']}, recipe:{result['recipe_stats']}")
        return result


def build_and_save(weeks: int = 3) -> Dict:
    """í•™ìŠµ ë°ì´í„°ì…‹ ë¹Œë“œ ë° ì €ì¥ (ì™¸ë¶€ í˜¸ì¶œìš©)
    
    Returns:
        ë¹Œë“œ ê²°ê³¼
    """
    logger.info(f"ğŸš€ í•™ìŠµ ë°ì´í„°ì…‹ ë¹Œë“œ ì‹œì‘ (weeks={weeks})")
    
    try:
        builder = TrainingDatasetBuilder()
        
        # Training rows ìƒì„±
        training_rows = builder.build_training_rows(weeks=weeks)
        
        if not training_rows:
            return {
                "success": False,
                "message": "Training rows ìƒì„± ì‹¤íŒ¨ - ë°ì´í„° ì—†ìŒ"
            }
        
        # Recipe stats ìƒì„±
        recipe_stats = builder.build_recipe_stats(training_rows)
        
        # ì €ì¥
        save_result = builder.save_results()
        
        return {
            "success": True,
            "training_rows_count": len(training_rows),
            "recipe_stats": recipe_stats.get("summary", {}),
            "top_recipes": recipe_stats.get("top_recipes", [])[:10],
            "save_result": save_result
        }
        
    except Exception as e:
        logger.error(f"âŒ ë¹Œë“œ ì‹¤íŒ¨: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return {
            "success": False,
            "error": str(e)
        }


def get_top_recipes(weeks: int = 3) -> List[Dict]:
    """ìƒìœ„ ë ˆì‹œí”¼ ì¡°íšŒ (ì™¸ë¶€ í˜¸ì¶œìš©)
    
    ìºì‹œëœ íŒŒì¼ì—ì„œ ì½ê±°ë‚˜, ì—†ìœ¼ë©´ ìƒˆë¡œ ë¹Œë“œ
    """
    recipe_path = os.getenv("RECIPE_STATS_FILE", DEFAULT_RECIPE_PATH)
    
    # ìºì‹œëœ íŒŒì¼ í™•ì¸
    if os.path.exists(recipe_path):
        try:
            with open(recipe_path, "r", encoding="utf-8") as f:
                data = json.load(f)
            
            # 24ì‹œê°„ ì´ë‚´ë©´ ìºì‹œ ì‚¬ìš©
            generated_at = data.get("generated_at")
            if generated_at:
                gen_dt = datetime.fromisoformat(generated_at)
                if gen_dt.tzinfo is None:
                    gen_dt = KST.localize(gen_dt)
                
                if datetime.now(KST) - gen_dt < timedelta(hours=24):
                    return data.get("top_recipes", [])
        except Exception as e:
            logger.warning(f"ìºì‹œ ì½ê¸° ì‹¤íŒ¨: {e}")
    
    # ìƒˆë¡œ ë¹Œë“œ
    result = build_and_save(weeks=weeks)
    return result.get("top_recipes", [])
